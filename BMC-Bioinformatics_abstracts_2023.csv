,Title,Abstract
0,"ICON-GEMs: integration of co-expression network in genome-scale metabolic models, shedding light through systems biology","Background
Flux Balance Analysis (FBA) is a key metabolic modeling method used to simulate cellular metabolism under steady-state conditions. Its simplicity and versatility have led to various strategies incorporating transcriptomic and proteomic data into FBA, successfully predicting flux distribution and phenotypic results. However, despite these advances, the untapped potential lies in leveraging gene-related connections like co-expression patterns for valuable insights.
Results
To fill this gap, we introduce ICON-GEMs, an innovative constraint-based model to incorporate gene co-expression network into the FBA model, facilitating more precise determination of flux distributions and functional pathways. In this study, transcriptomic data from both 
Escherichia coli
 and 
Saccharomyces cerevisiae
 were integrated into their respective genome-scale metabolic models. A comprehensive gene co-expression network was constructed as a global view of metabolic mechanism of the cell. By leveraging quadratic programming, we maximized the alignment between pairs of reaction fluxes and the correlation of their corresponding genes in the co-expression network. The outcomes notably demonstrated that ICON-GEMs outperformed existing methodologies in predictive accuracy. Flux variabilities over subsystems and functional modules also demonstrate promising results. Furthermore, a comparison involving different types of biological networks, including protein–protein interactions and random networks, reveals insights into the utilization of the co-expression network in genome-scale metabolic engineering.
Conclusion
ICON-GEMs introduce an innovative constrained model capable of simultaneous integration of gene co-expression networks, ready for board application across diverse transcriptomic data sets and multiple organisms. It is freely available as open-source at 
https://github.com/ThummaratPaklao/ICOM-GEMs.git
."
1,IgMAT: immunoglobulin sequence multi-species annotation tool for any species including those with incomplete antibody annotation or unusual characteristics,"Background
The advent and continual improvement of high-throughput sequencing technologies has made immunoglobulin repertoire sequencing accessible and informative regardless of study species. However, to fully map dynamic changes in polyclonal responses precise framework and complementarity determining region annotation of rearranging genes is pivotal. Most sequence annotation tools are designed primarily for use with human and mouse antibody sequences which use databases with fixed species lists, applying very specific assumptions which select against unique structural characteristics. For this reason, data agnostic tools able to learn from presented data can be very useful with new species or with novel datasets.
Results
We have developed IgMAT, which utilises a reduced amino acid alphabet, that incorporates multiple HMM alignments into a single consensus to automatically annotate immunoglobulin sequences from most organisms. Additionally, the software allows the incorporation of user defined databases to better represent the species and/or antibody class of interest. To demonstrate the accuracy and utility of IgMAT, we present analysis of sequences extracted from structural data and immunoglobulin sequence datasets from several different species.
Conclusions
IgMAT is fully open-sourced and freely available on GitHub (
https://github.com/TPI-Immunogenetics/igmat
) for download under GPLv3 license. It can be used as a CLI application or as a python module to be integrated in custom scripts."
2,PWSC: a novel clustering method based on polynomial weight-adjusted sparse clustering for sparse biomedical data and its application in cancer subtyping,"Background
Clustering analysis is widely used to interpret biomedical data and uncover new knowledge and patterns. However, conventional clustering methods are not effective when dealing with sparse biomedical data. To overcome this limitation, we propose a hierarchical clustering method called polynomial weight-adjusted sparse clustering (PWSC).
Results
The PWSC algorithm adjusts feature weights using a polynomial function, redefines the distances between samples, and performs hierarchical clustering analysis based on these adjusted distances. Additionally, we incorporate a consensus clustering approach to determine the optimal number of classifications. This consensus approach utilizes relative change in the cumulative distribution function to identify the best number of clusters, resulting in more stable clustering results. Leveraging the PWSC algorithm, we successfully classified a cohort of gastric cancer patients, enabling categorization of patients carrying different types of altered genes. Further evaluation using Entropy showed a significant improvement (
p
 = 2.905e−05), while using the Calinski–Harabasz index demonstrates a remarkable 100% improvement in the quality of the best classification compared to conventional algorithms. Similarly, significantly increased entropy (
p
 = 0.0336) and comparable CHI, were observed when classifying another colorectal cancer cohort with microbial abundance. The above attempts in cancer subtyping demonstrate that PWSC is highly applicable to different types of biomedical data. To facilitate its application, we have developed a user-friendly tool that implements the PWSC algorithm, which canbe accessed at 
http://pwsc.aiyimed.com/
.
Conclusions
PWSC addresses the limitations of conventional approaches when clustering sparse biomedical data. By adjusting feature weights and employing consensus clustering, we achieve improved clustering results compared to conventional methods. The PWSC algorithm provides a valuable tool for researchers in the field, enabling more accurate and stable clustering analysis. Its application can enhance our understanding of complex biological systems and contribute to advancements in various biomedical disciplines."
3,Injectiondesign: web service of plate design with optimized stratified block randomization for modern GC/LC-MS-based sample preparation,"Background
Plate design is a necessary and time-consuming operation for GC/LC-MS-based sample preparation. The implementation of the inter-batch balancing algorithm and the intra-batch randomization algorithm can have a significant impact on the final results. For researchers without programming skills, a stable and efficient online service for plate design is necessary.
Results
Here we describe InjectionDesign, a free online plate design service focused on GC/LC-MS-based multi-omics experiment design. It offers the ability to separate the position design from the sequence design, making the output more compatible with the requirements of a modern mass spectrometer-based laboratory. In addition, it has implemented an optimized block randomization algorithm, which can be better applied to sample stratification with block randomization for an unbalanced distribution. It is easy to use, with built-in support for common instrument models and quick export to a worksheet.
Conclusions
InjectionDesign is an open-source project based on Java. Researchers can get the source code for the project from Github: 
https://github.com/CSi-Studio/InjectionDesign
. A free web service is also provided: 
http://www.injection.design
."
4,Advancing drug–target interaction prediction: a comprehensive graph-based approach integrating knowledge graph embedding and ProtBert pretraining,"Background
The pharmaceutical field faces a significant challenge in validating drug target interactions (DTIs) due to the time and cost involved, leading to only a fraction being experimentally verified. To expedite drug discovery, accurate computational methods are essential for predicting potential interactions. Recently, machine learning techniques, particularly graph-based methods, have gained prominence. These methods utilize networks of drugs and targets, employing knowledge graph embedding (KGE) to represent structured information from knowledge graphs in a continuous vector space. This phenomenon highlights the growing inclination to utilize graph topologies as a means to improve the precision of predicting DTIs, hence addressing the pressing requirement for effective computational methodologies in the field of drug discovery.
Results
The present study presents a novel approach called DTIOG for the prediction of DTIs. The methodology employed in this study involves the utilization of a KGE strategy, together with the incorporation of contextual information obtained from protein sequences. More specifically, the study makes use of Protein Bidirectional Encoder Representations from Transformers (ProtBERT) for this purpose. DTIOG utilizes a two-step process to compute embedding vectors using KGE techniques. Additionally, it employs ProtBERT to determine target–target similarity. Different similarity measures, such as Cosine similarity or Euclidean distance, are utilized in the prediction procedure. In addition to the contextual embedding, the proposed unique approach incorporates local representations obtained from the Simplified Molecular Input Line Entry Specification (SMILES) of drugs and the amino acid sequences of protein targets.
Conclusions
The effectiveness of the proposed approach was assessed through extensive experimentation on datasets pertaining to Enzymes, Ion Channels, and G-protein-coupled Receptors. The remarkable efficacy of DTIOG was showcased through the utilization of diverse similarity measures in order to calculate the similarities between drugs and targets. The combination of these factors, along with the incorporation of various classifiers, enabled the model to outperform existing algorithms in its ability to predict DTIs. The consistent observation of this advantage across all datasets underlines the robustness and accuracy of DTIOG in the domain of DTIs. Additionally, our case study suggests that the DTIOG can serve as a valuable tool for discovering new DTIs."
5,PAPerFly: Partial Assembly-based Peak Finder for ab initio binding site reconstruction,"Background
The specific recognition of a DNA locus by a given transcription factor is a widely studied issue. It is generally agreed that the recognition can be influenced not only by the binding motif but by the larger context of the binding site. In this work, we present a novel heuristic algorithm that can reconstruct the unique binding sites captured in a sequencing experiment without using the reference genome.
Results
We present PAPerFly, the Partial Assembly-based Peak Finder, a tool for the binding site and binding context reconstruction from the sequencing data without any prior knowledge. This tool operates without the need to know the reference genome of the respective organism. We employ algorithmic approaches that are used during genome assembly. The proposed algorithm constructs a de Bruijn graph from the sequencing data. Based on this graph, sequences and their enrichment are reconstructed using a novel heuristic algorithm. The reconstructed sequences are aligned and the peaks in the sequence enrichment are identified. Our approach was tested by processing several ChIP-seq experiments available in the ENCODE database and comparing the results of Paperfly and standard methods.
Conclusions
We show that PAPerFly, an algorithm tailored for experiment analysis without the reference genome, yields better results than an aggregation of ChIP-seq agnostic tools. Our tool is freely available at 
https://github.com/Caeph/paperfly/
 or on Zenodo (
https://doi.org/10.5281/zenodo.7116424
)."
6,BioEGRE: a linguistic topology enhanced method for biomedical relation extraction based on BioELECTRA and graph pointer neural network,"Background
Automatic and accurate extraction of diverse biomedical relations from literature is a crucial component of bio-medical text mining. Currently, stacking various classification networks on pre-trained language models to perform fine-tuning is a common framework to end-to-end solve the biomedical relation extraction (BioRE) problem. However, the sequence-based pre-trained language models underutilize the graphical topology of language to some extent. In addition, sequence-oriented deep neural networks have limitations in processing graphical features.
Results
In this paper, we propose a novel method for sentence-level BioRE task, BioEGRE (
BioE
LECTRA and 
G
raph pointer neural net-work for 
R
elation 
E
xtraction), aimed at leveraging the linguistic topological features. First, the biomedical literature is preprocessed to retain sentences involving pre-defined entity pairs. Secondly, SciSpaCy is employed to conduct dependency parsing; sentences are modeled as graphs based on the parsing results; BioELECTRA is utilized to generate token-level representations, which are modeled as attributes of nodes in the sentence graphs; a graph pointer neural network layer is employed to select the most relevant multi-hop neighbors to optimize representations; a fully-connected neural network layer is employed to generate the sentence-level representation. Finally, the Softmax function is employed to calculate the probabilities. Our proposed method is evaluated on three BioRE tasks: a multi-class (CHEMPROT) and two binary tasks (GAD and EU-ADR). The results show that our method achieves F1-scores of 79.97% (CHEMPROT), 83.31% (GAD), and 83.51% (EU-ADR), surpassing the performance of existing state-of-the-art models.
Conclusion
The experimental results on 3 biomedical benchmark datasets demonstrate the effectiveness and generalization of BioEGRE, which indicates that linguistic topology and a graph pointer neural network layer explicitly improve performance for BioRE tasks."
7,PEPMatch: a tool to identify short peptide sequence matches in large sets of proteins,"Background
Numerous tools exist for biological sequence comparisons and search. One case of particular interest for immunologists is finding matches for linear peptide T cell epitopes, typically between 8 and 15 residues in length, in a large set of protein sequences. Both to find exact matches or matches that account for residue substitutions. The utility of such tools is critical in applications ranging from identifying conservation across viral epitopes, identifying putative epitope targets for allergens, and finding matches for cancer-associated neoepitopes to examine the role of tolerance in tumor recognition.
Results
We defined a set of benchmarks that reflect the different practical applications of short peptide sequence matching. We evaluated a suite of existing methods for speed and recall and developed a new tool, PEPMatch. The tool uses a deterministic 
k
-mer mapping algorithm that preprocesses proteomes before searching, achieving a 50-fold increase in speed over methods such as the Basic Local Alignment Search Tool (BLAST) without compromising recall. PEPMatch’s code and benchmark datasets are publicly available.
Conclusions
PEPMatch offers significant speed and recall advantages for peptide sequence matching. While it is of immediate utility for immunologists, the developed benchmarking framework also provides a standard against which future tools can be evaluated for improvements. The tool is available at 
https://nextgen-tools.iedb.org
, and the source code can be found at 
https://github.com/IEDB/PEPMatch
."
8,GPDRP: a multimodal framework for drug response prediction with graph transformer,"Background
In the field of computational personalized medicine, drug response prediction (DRP) is a critical issue. However, existing studies often characterize drugs as strings, a representation that does not align with the natural description of molecules. Additionally, they ignore gene pathway-specific combinatorial implication.
Results
In this study, we propose drug Graph and gene Pathway based Drug response prediction method (GPDRP), a new multimodal deep learning model for predicting drug responses based on drug molecular graphs and gene pathway activity. In GPDRP, drugs are represented by molecular graphs, while cell lines are described by gene pathway activity scores. The model separately learns these two types of data using Graph Neural Networks (GNN) with Graph Transformers and deep neural networks. Predictions are subsequently made through fully connected layers.
Conclusions
Our results indicate that Graph Transformer-based model delivers superior performance. We apply GPDRP on hundreds of cancer cell lines’ bulk RNA-sequencing data, and it outperforms some recently published models. Furthermore, the generalizability and applicability of GPDRP are demonstrated through its predictions on unknown drug-cell line pairs and xenografts. This underscores the interpretability achieved by incorporating gene pathways."
9,"TCGAplot
: an R package for integrative pan-cancer analysis and visualization of TCGA multi-omics data","Background
Pan-cancer analysis examines both the commonalities and heterogeneity among genomic and cellular alterations across numerous types of tumors. Pan-cancer analysis of gene expression, tumor mutational burden (TMB), microsatellite instability (MSI), and tumor immune microenvironment (TIME), and methylation becomes available based on the multi-omics data from The Cancer Genome Atlas Program (TCGA). Some online tools provide analysis of gene and protein expression, mutation, methylation, and survival for TCGA data. However, these online tools were either Uni-functional or were not able to perform analysis of user-defined functions. Therefore, we created the 
TCGAplot
 R package to facilitate perform pan-cancer analysis and visualization of the built-in multi-omic TCGA data.
Results
TCGAplot
 provides several functions to perform pan-cancer paired/unpaired differential gene expression analysis, pan-cancer correlation analysis between gene expression and TMB, MSI, TIME, and promoter methylation. Functions for visualization include paired/unpaired boxplot, survival plot, ROC curve, heatmap, scatter, radar chart, and forest plot. Moreover, gene set based pan-cancer and tumor specific analyses were also available. Finally, all these built-in multi-omic data could be extracted for implementation for user-defined functions, making the pan-cancer analysis much more convenient.\
Conclusions
We developed an R-package for integrative pan-cancer analysis and visualization of TCGA multi-omics data. The source code and pre-built package are available at GitHub (
https://github.com/tjhwangxiong/TCGAplot
)."
10,A compressed large language model embedding dataset of ICD 10 CM descriptions,"This paper presents novel datasets providing numerical representations of ICD-10-CM codes by generating description embeddings using a large language model followed by a dimension reduction via autoencoder. The embeddings serve as informative input features for machine learning models by capturing relationships among categories and preserving inherent context information. The model generating the data was validated in two ways. First, the dimension reduction was validated using an autoencoder, and secondly, a supervised model was created to estimate the ICD-10-CM hierarchical categories. Results show that the dimension of the data can be reduced to as few as 10 dimensions while maintaining the ability to reproduce the original embeddings, with the fidelity decreasing as the reduced-dimension representation decreases. Multiple compression levels are provided, allowing users to choose as per their requirements, download and use without any other setup. The readily available datasets of ICD-10-CM codes are anticipated to be highly valuable for researchers in biomedical informatics, enabling more advanced analyses in the field. This approach has the potential to significantly improve the utility of ICD-10-CM codes in the biomedical domain."
11,scInterpreter: a knowledge-regularized generative model for interpretably integrating scRNA-seq data,"Background
The rapid emergence of single-cell RNA-seq (scRNA-seq) data presents remarkable opportunities for broad investigations through integration analyses. However, most integration models are black boxes that lack interpretability or are hard to train.
Results
To address the above issues, we propose scInterpreter, a deep learning-based interpretable model. scInterpreter substantially outperforms other state-of-the-art (SOTA) models in multiple benchmark datasets. In addition, scInterpreter is extensible and can integrate and annotate atlas scRNA-seq data. We evaluated the robustness of scInterpreter in a variety of situations. Through comparison experiments, we found that with a knowledge prior, the training process can be significantly accelerated. Finally, we conducted interpretability analysis for each dimension (pathway) of cell representation in the embedding space.
Conclusions
The results showed that the cell representations obtained by scInterpreter are full of biological significance. Through weight sorting, we found several new genes related to pathways in PBMC dataset. In general, scInterpreter is an effective and interpretable integration tool. It is expected that scInterpreter will bring great convenience to the study of single-cell transcriptomics."
12,Cellstitch: 3D cellular anisotropic image segmentation via optimal transport,"Background
Spatial mapping of transcriptional states provides valuable biological insights into cellular functions and interactions in the context of the tissue. Accurate 3D cell segmentation is a critical step in the analysis of this data towards understanding diseases and normal development in situ. Current approaches designed to automate 3D segmentation include stitching masks along one dimension, training a 3D neural network architecture from scratch, and reconstructing a 3D volume from 2D segmentations on all dimensions. However, the applicability of existing methods is hampered by inaccurate segmentations along the non-stitching dimensions, the lack of high-quality diverse 3D training data, and inhomogeneity of image resolution along orthogonal directions due to acquisition constraints; as a result, they have not been widely used in practice.
Methods
To address these challenges, we formulate the problem of finding cell correspondence across layers with a novel optimal transport (OT) approach. We propose CellStitch, a flexible pipeline that segments cells from 3D images without requiring large amounts of 3D training data. We further extend our method to interpolate internal slices from highly anisotropic cell images to recover isotropic cell morphology.
Results
We evaluated the performance of CellStitch through eight 3D plant microscopic datasets with diverse anisotropic levels and cell shapes. CellStitch substantially outperforms the state-of-the art methods on anisotropic images, and achieves comparable segmentation quality against competing methods in isotropic setting. We benchmarked and reported 3D segmentation results of all the methods with instance-level precision, recall and average precision (AP) metrics.
Conclusions
The proposed OT-based 3D segmentation pipeline outperformed the existing state-of-the-art methods on different datasets with nonzero anisotropy, providing high fidelity recovery of 3D cell morphology from microscopic images."
13,A novel and innovative cancer classification framework through a consecutive utilization of hybrid feature selection,"Cancer prediction in the early stage is a topic of major interest in medicine since it allows accurate and efficient actions for successful medical treatments of cancer. Mostly cancer datasets contain various gene expression levels as features with less samples, so firstly there is a need to eliminate similar features to permit faster convergence rate of classification algorithms. These features (genes) enable us to identify cancer disease, choose the best prescription to prevent cancer and discover deviations amid different techniques. To resolve this problem, we proposed a hybrid novel technique CSSMO-based gene selection for cancer classification. First, we made alteration of the fitness of spider monkey optimization (SMO) with cuckoo search algorithm (CSA) algorithm viz., CSSMO for feature selection, which helps to combine the benefit of both metaheuristic algorithms to discover a subset of genes which helps to predict a cancer disease in early stage. Further, to enhance the accuracy of the CSSMO algorithm, we choose a cleaning process, minimum redundancy maximum relevance (mRMR) to lessen the gene expression of cancer datasets. Next, these subsets of genes are classified using deep learning (DL) to identify different groups or classes related to a particular cancer disease. Eight different benchmark microarray gene expression datasets of cancer have been utilized to analyze the performance of the proposed approach with different evaluation matrix such as recall, precision, F1-score, and confusion matrix. The proposed gene selection method with DL achieves much better classification accuracy than other existing DL and machine learning classification models with all large gene expression dataset of cancer."
14,Subtyping irritable bowel syndrome using cluster analysis: a systematic review,"Background
Irritable bowel syndrome (IBS) is a common chronic functional gastrointestinal disorder associated with a wide range of clinical symptoms. Some researchers have used cluster analysis (CA), a group of non-supervised learning methods that identifies homogenous clusters within different entities based on their similarity.
Objective and methods
This literature review aims to identify published articles that apply CA to IBS patients. We searched relevant keywords in PubMed, Embase, Web of Science, and Scopus. We reviewed studies in terms of the selected variables, participants’ characteristics, data collection, methodology, number of clusters, clusters’ profiles, and results.
Results
Among the 14 articles focused on the heterogeneity of IBS, eight of them utilized K-means Cluster Analysis (K-means CA), four employed Hierarchical Cluster Analysis, and only two studies utilized Latent Class Analysis. Seven studies focused on clinical symptoms, while four articles examined anocolorectal functions. Two studies were centered around immunological findings, and only one study explored microbial composition. The number of clusters obtained ranged from two to seven, showing variation across the studies. Males exhibited lower symptom severity and fewer psychological findings. The association between symptom severity and rectal perception suggests that altered rectal perception serves as a biological indicator of IBS. Ultra-slow waves observed in IBS patients are linked to increased activity of the anal sphincter, higher anal pressure, dystonia, and dyschezia.
Conclusion
IBS has different subgroups based on different factors. Most IBS patients have low clinical severity, good QoL, high rectal sensitivity, delayed left colon transit time, increased systemic cytokines, and changes in microbial composition, including increased Firmicutes-associated taxa and depleted Bacteroidetes-related taxa. However, the number of clusters is inconsistent across studies due to the methodological heterogeneity. CA, a valuable non-supervised learning method, is sensitive to hyperparameters like the number of clusters and random initialization of cluster centers. The random nature of these parameters leads to diverse outcomes even with the same algorithm. This has implications for future research and practical applications, necessitating further studies to improve our understanding of IBS and develop personalized treatments."
15,Machine learning-based donor permission extraction from informed consent documents,"Background
With more clinical trials are offering optional participation in the collection of bio-specimens for biobanking comes the increasing complexity of requirements of informed consent forms. The aim of this study is to develop an automatic natural language processing (NLP) tool to annotate informed consent documents to promote biorepository data regulation, sharing, and decision support. We collected informed consent documents from several publicly available sources, then manually annotated them, covering sentences containing permission information about the sharing of either bio-specimens or donor data, or conducting genetic research or future research using bio-specimens or donor data.
Results
We evaluated a variety of machine learning algorithms including random forest (RF) and support vector machine (SVM) for the automatic identification of these sentences. 120 informed consent documents containing 29,204 sentences were annotated, of which 1250 sentences (4.28%) provide answers to a permission question. A support vector machine (SVM) model achieved a F-1 score of 0.95 on classifying the sentences when using a gold standard, which is a prefiltered corpus containing all relevant sentences.
Conclusions
This study provides the feasibility of using machine learning tools to classify permission-related sentences in informed consent documents."
16,Predicting potential microbe-disease associations based on auto-encoder and graph convolution network,"The increasing body of research has consistently demonstrated the intricate correlation between the human microbiome and human well-being. Microbes can impact the efficacy and toxicity of drugs through various pathways, as well as influence the occurrence and metastasis of tumors. In clinical practice, it is crucial to elucidate the association between microbes and diseases. Although traditional biological experiments accurately identify this association, they are time-consuming, expensive, and susceptible to experimental conditions. Consequently, conducting extensive biological experiments to screen potential microbe-disease associations becomes challenging. The computational methods can solve the above problems well, but the previous computational methods still have the problems of low utilization of node features and the prediction accuracy needs to be improved. To address this issue, we propose the DAEGCNDF model predicting potential associations between microbes and diseases. Our model calculates four similar features for each microbe and disease. These features are fused to obtain a comprehensive feature matrix representing microbes and diseases. Our model first uses the graph convolutional network module to extract low-rank features with graph information of microbes and diseases, and then uses a deep sparse Auto-Encoder to extract high-rank features of microbe-disease pairs, after which the low-rank and high-rank features are spliced to improve the utilization of node features. Finally, Deep Forest was used for microbe-disease potential relationship prediction. The experimental results show that combining low-rank and high-rank features helps to improve the model performance and Deep Forest has better classification performance than the baseline model."
17,"PhyloSophos: a high-throughput scientific name mapping algorithm augmented with explicit consideration of taxonomic science, and its application on natural product (NP) occurrence database processing","Background
The standardization of biological data using unique identifiers is vital for seamless data integration, comprehensive interpretation, and reproducibility of research findings, contributing to advancements in bioinformatics and systems biology. Despite being widely accepted as a universal identifier, scientific names for biological species have inherent limitations, including lack of stability, uniqueness, and convertibility, hindering their effective use as identifiers in databases, particularly in natural product (NP) occurrence databases, posing a substantial obstacle to utilizing this valuable data for large-scale research applications.
Result
To address these challenges and facilitate high-throughput analysis of biological data involving scientific names, we developed PhyloSophos, a Python package that considers the properties of scientific names and taxonomic systems to accurately map name inputs to entries within a chosen reference database. We illustrate the importance of assessing multiple taxonomic databases and considering taxonomic syntax-based pre-processing using NP occurrence databases as an example, with the ultimate goal of integrating heterogeneous information into a single, unified dataset.
Conclusions
We anticipate PhyloSophos to significantly aid in the systematic processing of poorly digitized and curated biological data, such as biodiversity information and ethnopharmacological resources, enabling full-scale bioinformatics analysis using these valuable data resources."
18,Twnbiome: a public database of the healthy Taiwanese gut microbiome,"With new advances in next generation sequencing (NGS) technology at reduced costs, research on bacterial genomes in the environment has become affordable. Compared to traditional methods, NGS provides high-throughput sequencing reads and the ability to identify many species in the microbiome that were previously unknown. Numerous bioinformatics tools and algorithms have been developed to conduct such analyses. However, in order to obtain biologically meaningful results, the researcher must select the proper tools and combine them to construct an efficient pipeline. This complex procedure may include tens of tools, each of which require correct parameter settings. Furthermore, an NGS data analysis involves multiple series of command-line tools and requires extensive computational resources, which imposes a high barrier for biologists and clinicians to conduct NGS analysis and even interpret their own data. Therefore, we established a public gut microbiome database, which we call Twnbiome, created using healthy subjects from Taiwan, with the goal of enabling microbiota research for the Taiwanese population. Twnbiome provides users with a baseline gut microbiome panel from a healthy Taiwanese cohort, which can be utilized as a reference for conducting case-control studies for a variety of diseases. It is an interactive, informative, and user-friendly database. Twnbiome additionally offers an analysis pipeline, where users can upload their data and download analyzed results. Twnbiome offers an online database which non-bioinformatics users such as clinicians and doctors can not only utilize to access a control set of data, but also analyze raw data with a few easy clicks. All results are customizable with ready-made plots and easily downloadable tables. Database URL: 
http://twnbiome.cgm.ntu.edu.tw/
."
19,DL-PPI: a method on prediction of sequenced protein–protein interaction based on deep learning,"Purpose
Sequenced Protein–Protein Interaction (PPI) prediction represents a pivotal area of study in biology, playing a crucial role in elucidating the mechanistic underpinnings of diseases and facilitating the design of novel therapeutic interventions. Conventional methods for extracting features through experimental processes have proven to be both costly and exceedingly complex. In light of these challenges, the scientific community has turned to computational approaches, particularly those grounded in deep learning methodologies. Despite the progress achieved by current deep learning technologies, their effectiveness diminishes when applied to larger, unfamiliar datasets.
Results
In this study, the paper introduces a novel deep learning framework, termed DL-PPI, for predicting PPIs based on sequence data. The proposed framework comprises two key components aimed at improving the accuracy of feature extraction from individual protein sequences and capturing relationships between proteins in unfamiliar datasets. 1. Protein Node Feature Extraction Module: To enhance the accuracy of feature extraction from individual protein sequences and facilitate the understanding of relationships between proteins in unknown datasets, the paper devised a novel protein node feature extraction module utilizing the Inception method. This module efficiently captures relevant patterns and representations within protein sequences, enabling more informative feature extraction. 2. Feature-Relational Reasoning Network (FRN): In the Global Feature Extraction module of our model, the paper developed a novel FRN that leveraged Graph Neural Networks to determine interactions between pairs of input proteins. The FRN effectively captures the underlying relational information between proteins, contributing to improved PPI predictions. DL-PPI framework demonstrates state-of-the-art performance in the realm of sequence-based PPI prediction."
20,Performance analysis of conventional and AI-based variant callers using short and long reads,"Background
The accurate detection of variants is essential for genomics-based studies. Currently, there are various tools designed to detect genomic variants, however, it has always been a challenge to decide which tool to use, especially when various major genome projects have chosen to use different tools. Thus far, most of the existing tools were mainly developed to work on short-read data (i.e., Illumina); however, other sequencing technologies (e.g. PacBio, and Oxford Nanopore) have recently shown that they can also be used for variant calling. In addition, with the emergence of artificial intelligence (AI)-based variant calling tools, there is a pressing need to compare these tools in terms of efficiency, accuracy, computational power, and ease of use.
Results
In this study, we evaluated five of the most widely used conventional and AI-based variant calling tools (BCFTools, GATK4, Platypus, DNAscope, and DeepVariant) in terms of accuracy and computational cost using both short-read and long-read data derived from three different sequencing technologies (Illumina, PacBio HiFi, and ONT) for the same set of samples from the Genome In A Bottle project. The analysis showed that AI-based variant calling tools supersede conventional ones for calling SNVs and INDELs using both long and short reads in most aspects. In addition, we demonstrate the advantages and drawbacks of each tool while ranking them in each aspect of these comparisons.
Conclusion
This study provides best practices for variant calling using AI-based and conventional variant callers with different types of sequencing data."
21,ORFeus: a computational method to detect programmed ribosomal frameshifts and other non-canonical translation events,"Background
In canonical protein translation, ribosomes initiate translation at a specific start codon, maintain a single reading frame throughout elongation, and terminate at the first in-frame stop codon. However, ribosomal behavior can deviate at each of these steps, sometimes in a programmed manner. Certain mRNAs contain sequence and structural elements that cause ribosomes to begin translation at alternative start codons, shift reading frame, read through stop codons, or reinitiate on the same mRNA. These processes represent important translational control mechanisms that can allow an mRNA to encode multiple functional protein products or regulate protein expression. The prevalence of these events remains uncertain, due to the difficulty of systematic detection.
Results
We have developed a computational model to infer non-canonical translation events from ribosome profiling data.
Conclusion
ORFeus identifies known examples of alternative open reading frames and recoding events across different organisms and enables transcriptome-wide searches for novel events."
22,PhytoPipe: a phytosanitary pipeline for plant pathogen detection and diagnosis using RNA-seq data,"Background
Detection of exotic plant pathogens and preventing their entry and establishment are critical for the protection of agricultural systems while securing the global trading of agricultural commodities. High-throughput sequencing (HTS) has been applied successfully for plant pathogen discovery, leading to its current application in routine pathogen detection. However, the analysis of massive amounts of HTS data has become one of the major challenges for the use of HTS more broadly as a rapid diagnostics tool. Several bioinformatics pipelines have been developed to handle HTS data with a focus on plant virus and viroid detection. However, there is a need for an integrative tool that can simultaneously detect a wider range of other plant pathogens in HTS data, such as bacteria (including phytoplasmas), fungi, and oomycetes, and this tool should also be capable of generating a comprehensive report on the phytosanitary status of the diagnosed specimen.
Results
We have developed an open-source bioinformatics pipeline called PhytoPipe (Phytosanitary Pipeline) to provide the plant pathology diagnostician community with a user-friendly tool that integrates analysis and visualization of HTS RNA-seq data. PhytoPipe includes quality control of reads, read classification, assembly-based annotation, and reference-based mapping. The final product of the analysis is a comprehensive report for easy interpretation of not only viruses and viroids but also bacteria (including phytoplasma), fungi, and oomycetes. PhytoPipe is implemented in Snakemake workflow with Python 3 and bash scripts in a Linux environment. The source code for PhytoPipe is freely available and distributed under a BSD-3 license.
Conclusions
PhytoPipe provides an integrative bioinformatics pipeline that can be used for the analysis of HTS RNA-seq data. PhytoPipe is easily installed on a Linux or Mac system and can be conveniently used with a Docker image, which includes all dependent packages and software related to analyses. It is publicly available on GitHub at 
https://github.com/healthyPlant/PhytoPipe
 and on Docker Hub at 
https://hub.docker.com/r/healthyplant/phytopipe
."
23,Rendering protein mutation movies with MutAmore,"Background
The success of 
AlphaFold2
 in reliable protein three-dimensional (3D) structure prediction, assists the move of structural biology toward studies of protein dynamics and mutational impact on structure and function. This transition needs tools that qualitatively assess alternative 3D conformations.
Results
We introduce 
MutAmore
, a bioinformatics tool that renders individual images of protein 3D structures for, e.g., sequence mutations into a visually intuitive movie format. 
MutAmore
 streamlines a pipeline casting single amino-acid variations (SAVs) into a dynamic 3D mutation movie providing a qualitative perspective on the mutational landscape of a protein. By default, the tool first generates all possible variants of the sequence reachable through SAVs (L*19 for proteins with L residues). Next, it predicts the structural conformation for all L*19 variants using state-of-the-art models. Finally, it visualizes the mutation matrix and produces a color-coded 3D animation. Alternatively, users can input other types of variants, e.g., from experimental structures.
Conclusion
MutAmore
 samples alternative protein configurations to study the dynamical space accessible from SAVs in the post-AlphaFold2 era of structural biology. As the field shifts towards the exploration of alternative conformations of proteins, 
MutAmore
 aids in the understanding of the structural impact of mutations by providing a flexible pipeline for the generation of protein mutation movies using current and future structure prediction models."
24,primerJinn: a tool for rationally designing multiplex PCR primer sets for amplicon sequencing and performing in silico PCR,"Background
Multiplex PCR amplifies numerous targets in a single tube reaction and is essential in molecular biology and clinical diagnostics. One of its most important applications is in the targeted sequencing of pathogens. Despite this importance, few tools are available for designing multiplex primers.
Results
We developed primerJinn, a tool that designs a set of multiplex primers and allows for the in silico PCR evaluation of primer sets against numerous input genomes. We used primerJinn to create a multiplex PCR for the sequencing of drug resistance-conferring gene regions from 
Mycobacterium tuberculosis
, which were then successfully sequenced.
Conclusions
primerJinn provides a user-friendly, efficient, and accurate method for designing multiplex PCR primers for targeted sequencing and performing in silico PCR. It can be used for various applications in molecular biology and bioinformatics research, including the design of assays for amplifying and sequencing drug-resistance-conferring regions in important pathogens."
25,A score-based method of immune status evaluation for healthy individuals with complete blood cell counts,"Background
With the COVID-19 outbreak, an increasing number of individuals are concerned about their health, particularly their immune status. However, as of now, there is no available algorithm that effectively assesses the immune status of normal, healthy individuals. In response to this, a new score-based method is proposed that utilizes complete blood cell counts (CBC) to provide early warning of disease risks, such as COVID-19.
Methods
First, data on immune-related CBC measurements from 16,715 healthy individuals were collected. Then, a three-platform model was developed to normalize the data, and a Gaussian mixture model was optimized with expectation maximization (EM-GMM) to cluster the immune status of healthy individuals. Based on the results, Random Forest (RF), Light Gradient Boosting Machine (LightGBM) and Extreme Gradient Boosting (XGBoost) were used to determine the correlation of each CBC index with the immune status. Consequently, a weighted sum model was constructed to calculate a continuous immunity score, enabling the evaluation of immune status.
Results
The results demonstrated a significant negative correlation between the immunity score and the age of healthy individuals, thereby validating the effectiveness of the proposed method. In addition, a nonlinear polynomial regression model was developed to depict this trend. By comparing an individual’s immune status with the reference value corresponding to their age, their immune status can be evaluated.
Conclusion
In summary, this study has established a novel model for evaluating the immune status of healthy individuals, providing a good approach for early detection of abnormal immune status in healthy individuals. It is helpful in early warning of the risk of infectious diseases and of significant importance."
26,An oscillating reaction network with an exact closed form solution in the time domain,"Background
Oscillatory behavior is critical to many life sustaining processes such as cell cycles, circadian rhythms, and notch signaling. Important biological functions depend on the characteristics of these oscillations (hereafter, oscillation characteristics or OCs): frequency (e.g., event timings), amplitude (e.g., signal strength), and phase (e.g., event sequencing). Numerous oscillating reaction networks have been documented or proposed. Some investigators claim that oscillations in reaction networks require nonlinear dynamics in that at least one rate law is a nonlinear function of species concentrations. No one has shown that oscillations can be produced for a reaction network with linear dynamics. Further, no one has obtained closed form solutions for the frequency, amplitude and phase of any oscillating reaction network. Finally, no one has published an algorithm for constructing oscillating reaction networks with desired OCs.
Results
This is a theoretical study that analyzes reaction networks in terms of their representation as systems of ordinary differential equations. Our contributions are: (a) construction of an oscillating, two species reaction network [two species harmonic oscillator (2SHO)] that has no nonlinearity; (b) obtaining closed form formulas that calculate frequency, amplitude, and phase in terms of the parameters of the 2SHO reaction network, something that has not been done for any published oscillating reaction network; and (c) development of an algorithm that parameterizes the 2SHO to achieve desired oscillation, a capability that has not been produced for any published oscillating reaction network.
Conclusions
Our 2SHO demonstrates the feasibility of creating an oscillating reaction network whose dynamics are described by a system of 
linear
 differential equations. Because it is a linear system, we can derive closed form expressions for the frequency, amplitude, and phase of oscillations, something that has not been done for other published reaction networks. With these formulas, we can design 2SHO reaction networks to have desired oscillation characteristics. Finally, our sensitivity analysis suggests an approach to constructing a 2SHO for a biochemical system."
27,Hierarchical classification-based pan-cancer methylation analysis to classify primary cancer,"Hierarchical classification offers a more specific categorization of data and breaks down large classification problems into subproblems, providing improved prediction accuracy and predictive power for undefined categories, while also mitigating the impact of poor-quality data. Despite these advantages, its application in predicting primary cancer is rare. To leverage the similarity of cancers and the specificity of methylation patterns among them, we developed the Cancer Hierarchy Classification Tool (CHCT) using the idea of hierarchical classification, with methylation data from 30 cancer types and 8239 methylome samples downloaded from publicly available databases (The Cancer Genome Atlas (TCGA) and the Gene Expression Omnibus (GEO)). We used unsupervised clustering to divide the classification subproblems and screened differentially methylated sites using Analysis of variance (ANOVA) test, Tukey-kramer test, and Boruta algorithms to construct models for each classifier module. After validation, CHCT accurately classified 1568 out of 1660 cases in the test set, with an average accuracy of 94.46%. We further curated an independent validation cohort of 677 cancer samples from GEO and assigned a diagnosis using CHCT, which showed high diagnostic potential with generally high accuracies (an average accuracy of 91.40%). Moreover, CHCT demonstrates predictive capability for additional cancer types beyond its original classifier scope as demonstrated in the medulloblastoma and pituitary tumor datasets. In summary, CHCT can hierarchically classify primary cancer by methylation profile, by splitting a large-scale classification of 30 cancer types into ten smaller classification problems. These results indicate that cancer hierarchical classification has the potential to be an accurate and robust cancer classification method."
28,Allele-specific binding (ASB) analyzer for annotation of allele-specific binding SNPs,"Background
Allele-specific binding (ASB) events occur when transcription factors (TFs) bind more favorably to one of the two parental alleles at heterozygous single nucleotide polymorphisms (SNPs). Evidence suggests that ASB events could reveal the impact of sequence variations on TF binding and may have implications for the risk of diseases.
Results
Here we present ASB-analyzer, a software platform that enables the users to quickly and efficiently input raw sequencing data to generate individual reports containing the cytogenetic map of ASB SNPs and their associated phenotypes. This interactive tool thereby combines ASB SNP identification, biological annotation, motif analysis, phenotype associations and report summary in one pipeline. With this pipeline, we identified 3772 ASB SNPs from thirty GM12878 ChIP-seq datasets and demonstrated that the ASB SNPs were more likely to be enriched at important sites in TF-binding domains.
Conclusions
ASB-analyzer is a user-friendly tool that enables the detection, characterization and visualization of ASB SNPs. It is implemented in Python, R and bash shell and packaged in the Conda environment. It is available as an open-source tool on GitHub at 
https://github.com/Liying1996/ASBanalyzer
."
29,SingleScan: a comprehensive resource for single-cell sequencing data processing and mining,"Single-cell sequencing has shed light on previously inaccessible biological questions from different fields of research, including organism development, immune function, and disease progression. The number of single-cell-based studies increased dramatically over the past decade. Several new methods and tools have been continuously developed, making it extremely tricky to navigate this research landscape and develop an up-to-date workflow to analyze single-cell sequencing data, particularly for researchers seeking to enter this field without computational experience. Moreover, choosing appropriate tools and optimal parameters to meet the demands of researchers represents a major challenge in processing single-cell sequencing data. However, a specific resource for easy access to detailed information on single-cell sequencing methods and data processing pipelines is still lacking. In the present study, an online resource called SingleScan was developed to curate all up-to-date single-cell transcriptome/genome analyzing tools and pipelines. All the available tools were categorized according to their main tasks, and several typical workflows for single-cell data analysis were summarized. In addition, spatial transcriptomics, which is a breakthrough molecular analysis method that enables researchers to measure all gene activity in tissue samples and map the site of activity, was included along with a portion of single-cell and spatial analysis solutions. For each processing step, the available tools and specific parameters used in published articles are provided and how these parameters affect the results is shown in the resource. All information used in the resource was manually extracted from related literature. An interactive website was designed for data retrieval, visualization, and download. By analyzing the included tools and literature, users can gain insights into the trends of single-cell studies and easily grasp the specific usage of a specific tool. SingleScan will facilitate the analysis of single-cell sequencing data and promote the development of new tools to meet the growing and diverse needs of the research community. The SingleScan database is publicly accessible via the website at 
http://cailab.labshare.cn/SingleScan
."
30,Incorporating mutational heterogeneity to identify genes that are enriched for synonymous mutations in cancer,"Background
Synonymous mutations, which change the DNA sequence but not the encoded protein sequence, can affect protein structure and function, mRNA maturation, and mRNA half-lives. The possibility that synonymous mutations might be enriched in cancer has been explored in several recent studies. However, none of these studies control for all three types of mutational heterogeneity (patient, histology, and gene) that are known to affect the accurate identification of non-synonymous cancer-associated genes. Our goal is to adopt the current standard for non-synonymous mutations in an investigation of synonymous mutations.
Results
Here, we create an algorithm, MutSigCVsyn, an adaptation of MutSigCV, to identify cancer-associated genes that are enriched for synonymous mutations based on a non-coding background model that takes into account the mutational heterogeneity across these levels. Using MutSigCVsyn, we first analyzed 2572 cancer whole-genome samples from the Pan-cancer Analysis of Whole Genomes (PCAWG) to identify non-synonymous cancer drivers as a quality control. Indicative of the algorithm accuracy we find that 58.6% of these candidate genes were also found in Cancer Census Gene (CGC) list, and 66.2% were found within the PCAWG cancer driver list. We then applied it to identify 30 putative cancer-associated genes that are enriched for synonymous mutations within the same samples. One of the promising gene candidates is the B cell lymphoma 2 (BCL-2) gene. BCL-2 regulates apoptosis by antagonizing the action of proapoptotic BCL-2 family member proteins. The synonymous mutations in BCL2 are enriched in its anti-apoptotic domain and likely play a role in cancer cell proliferation.
Conclusion
Our study introduces MutSigCVsyn, an algorithm that accounts for mutational heterogeneity at patient, histology, and gene levels, to identify cancer-associated genes that are enriched for synonymous mutations using whole genome sequencing data. We identified 30 putative candidate genes that will benefit from future experimental studies on the role of synonymous mutations in cancer biology."
31,Lokatt: a hybrid DNA nanopore basecaller with an explicit duration hidden Markov model and a residual LSTM network,"Background
Basecalling long DNA sequences is a crucial step in nanopore-based DNA sequencing protocols. In recent years, the CTC-RNN model has become the leading basecalling model, supplanting preceding hidden Markov models (HMMs) that relied on pre-segmenting ion current measurements. However, the CTC-RNN model operates independently of prior biological and physical insights.
Results
We present a novel basecaller named Lokatt: explicit duration Markov model and residual-LSTM network. It leverages an explicit duration HMM (EDHMM) designed to model the nanopore sequencing processes. Trained on a newly generated library with methylation-free Ecoli samples and MinION R9.4.1 chemistry, the Lokatt basecaller achieves basecalling performances with a median single read identity score of 0.930, a genome coverage ratio of 99.750%, on par with existing state-of-the-art structure when trained on the same datasets.
Conclusion
Our research underlines the potential of incorporating prior knowledge into the basecalling processes, particularly through integrating HMMs and recurrent neural networks. The Lokatt basecaller showcases the efficacy of a hybrid approach, emphasizing its capacity to achieve high-quality basecalling performance while accommodating the nuances of nanopore sequencing. These outcomes pave the way for advanced basecalling methodologies, with potential implications for enhancing the accuracy and efficiency of nanopore-based DNA sequencing protocols."
32,Efficient design of synthetic gene circuits under cell-to-cell variability,"Background
Synthetic biologists use and combine diverse biological parts to build systems such as genetic circuits that perform desirable functions in, for example, biomedical or industrial applications. Computer-aided design methods have been developed to help choose appropriate network structures and biological parts for a given design objective. However, they almost always model the behavior of the network in an average cell, despite pervasive cell-to-cell variability.
Results
Here, we present a computational framework and an efficient algorithm to guide the design of synthetic biological circuits while accounting for cell-to-cell variability explicitly. Our design method integrates a Non-linear Mixed-Effects (NLME) framework into a Markov Chain Monte-Carlo (MCMC) algorithm for design based on ordinary differential equation (ODE) models. The analysis of a recently developed transcriptional controller demonstrates first insights into design guidelines when trying to achieve reliable performance under cell-to-cell variability.
Conclusion
We anticipate that our method not only facilitates the rational design of synthetic networks under cell-to-cell variability, but also enables novel applications by supporting design objectives that specify the desired behavior of cell populations."
33,"pyComBat, a Python tool for batch effects correction in high-throughput molecular data using empirical Bayes methods","Background
Variability in datasets is not only the product of biological processes: they are also the product of technical biases. ComBat and ComBat-Seq are among the most widely used tools for correcting those technical biases, called batch effects, in, respectively, microarray and RNA-Seq expression data.
Results
In this technical note, we present a new Python implementation of ComBat and ComBat-Seq. While the mathematical framework is strictly the same, we show here that our implementations: (i) have similar results in terms of batch effects correction; (ii) are as fast or faster than the original implementations in R and; (iii) offer new tools for the bioinformatics community to participate in its development. pyComBat is implemented in the Python language and is distributed under GPL-3.0 (
https://www.gnu.org/licenses/gpl-3.0.en.html
) license as a module of the inmoose package. Source code is available at 
https://github.com/epigenelabs/inmoose
 and Python package at 
https://pypi.org/project/inmoose
.
Conclusions
We present a new Python implementation of state-of-the-art tools ComBat and ComBat-Seq for the correction of batch effects in microarray and RNA-Seq data. This new implementation, based on the same mathematical frameworks as ComBat and ComBat-Seq, offers similar power for batch effect correction, at reduced computational cost."
34,"Detection for melanoma skin cancer through ACCF, BPPF, and CLF techniques with machine learning approach","Intense sun exposure is a major risk factor for the development of melanoma, an abnormal proliferation of skin cells. Yet, this more prevalent type of skin cancer can also develop in less-exposed areas, such as those that are shaded. Melanoma is the sixth most common type of skin cancer. In recent years, computer-based methods for imaging and analyzing biological systems have made considerable strides. This work investigates the use of advanced machine learning methods, specifically ensemble models with Auto Correlogram Methods, Binary Pyramid Pattern Filter, and Color Layout Filter, to enhance the detection accuracy of Melanoma skin cancer. These results suggest that the Color Layout Filter model of the Attribute Selection Classifier provides the best overall performance. Statistics for ROC, PRC, Kappa, F-Measure, and Matthews Correlation Coefficient were as follows: 90.96% accuracy, 0.91 precision, 0.91 recall, 0.95 ROC, 0.87 PRC, 0.87 Kappa, 0.91 F-Measure, and 0.82 Matthews Correlation Coefficient. In addition, its margins of error are the smallest. The research found that the Attribute Selection Classifier performed well when used in conjunction with the Color Layout Filter to improve image quality."
35,G-bic: generating synthetic benchmarks for biclustering,"Background
Biclustering is increasingly used in biomedical data analysis, recommendation tasks, and text mining domains, with hundreds of biclustering algorithms proposed. When assessing the performance of these algorithms, more than real datasets are required as they do not offer a solid ground truth. Synthetic data surpass this limitation by producing reference solutions to be compared with the found patterns. However, generating synthetic datasets is challenging since the generated data must ensure reproducibility, pattern representativity, and real data resemblance.
Results
We propose G-Bic, a dataset generator conceived to produce synthetic benchmarks for the normative assessment of biclustering algorithms. Beyond expanding on aspects of pattern coherence, data quality, and positioning properties, it further handles specificities related to mixed-type datasets and time-series data.G-Bic has the flexibility to replicate real data regularities from diverse domains. We provide the default configurations to generate reproducible benchmarks to evaluate and compare diverse aspects of biclustering algorithms. Additionally, we discuss empirical strategies to simulate the properties of real data.
Conclusion
G-Bic is a parametrizable generator for biclustering analysis, offering a solid means to assess biclustering solutions according to internal and external metrics robustly."
36,Protein–protein interaction site prediction by model ensembling with hybrid feature and self-attention,"Background
Protein–protein interactions (PPIs) are crucial in various biological functions and cellular processes. Thus, many computational approaches have been proposed to predict PPI sites. Although significant progress has been made, these methods still have limitations in encoding the characteristics of each amino acid in sequences. Many feature extraction methods rely on the sliding window technique, which simply merges all the features of residues into a vector. The importance of some key residues may be weakened in the feature vector, leading to poor performance.
Results
We propose a novel sequence-based method for PPI sites prediction. The new network model, PPINet, contains multiple feature processing paths. For a residue, the PPINet extracts the features of the targeted residue and its context separately. These two types of features are processed by two paths in the network and combined to form a protein representation, where the two types of features are of relatively equal importance. The model ensembling technique is applied to make use of more features. The base models are trained with different features and then ensembled via stacking. In addition, a data balancing strategy is presented, by which our model can get significant improvement on highly unbalanced data.
Conclusion
The proposed method is evaluated on a fused dataset constructed from Dset186, Dset_72, and PDBset_164, as well as the public Dset_448 dataset. Compared with current state-of-the-art methods, the performance of our method is better than the others. In the most important metrics, such as AUPRC and recall, it surpasses the second-best programmer on the latter dataset by 6.9% and 4.7%, respectively. We also demonstrated that the improvement is essentially due to using the ensemble model, especially, the hybrid feature. We share our code for reproducibility and future research at 
https://github.com/CandiceCong/StackingPPINet
."
37,HostNet: improved sequence representation in deep neural networks for virus-host prediction,"Background
The escalation of viruses over the past decade has highlighted the need to determine their respective hosts, particularly for emerging ones that pose a potential menace to the welfare of both human and animal life. Yet, the traditional means of ascertaining the host range of viruses, which involves field surveillance and laboratory experiments, is a laborious and demanding undertaking. A computational tool with the capability to reliably predict host ranges for novel viruses can provide timely responses in the prevention and control of emerging infectious diseases. The intricate nature of viral-host prediction involves issues such as data imbalance and deficiency. Therefore, developing highly accurate computational tools capable of predicting virus-host associations is a challenging and pressing demand.
Results
To overcome the challenges of virus-host prediction, we present HostNet, a deep learning framework that utilizes a Transformer-CNN-BiGRU architecture and two enhanced sequence representation modules. The first module, k-mer to vector, pre-trains a background vector representation of k-mers from a broad range of virus sequences to address the issue of data deficiency. The second module, an adaptive sliding window, truncates virus sequences of various lengths to create a uniform number of informative and distinct samples for each sequence to address the issue of data imbalance. We assess HostNet's performance on a benchmark dataset of “Rabies lyssavirus” and an in-house dataset of “Flavivirus”. Our results show that HostNet surpasses the state-of-the-art deep learning-based method in host-prediction accuracies and F1 score. The enhanced sequence representation modules, significantly improve HostNet's training generalization, performance in challenging classes, and stability.
Conclusion
HostNet is a promising framework for predicting virus hosts from genomic sequences, addressing challenges posed by sparse and varying-length virus sequence data. Our results demonstrate its potential as a valuable tool for virus-host prediction in various biological contexts. Virus-host prediction based on genomic sequences using deep neural networks is a promising approach to identifying their potential hosts accurately and efficiently, with significant impacts on public health, disease prevention, and vaccine development."
38,PMFFRC: a large-scale genomic short reads compression optimizer via memory modeling and redundant clustering,"Background
Genomic sequencing reads compressors are essential for balancing high-throughput sequencing short reads generation speed, large-scale genomic data sharing, and infrastructure storage expenditure. However, most existing short reads compressors rarely utilize big-memory systems and duplicative information between diverse sequencing files to achieve a higher compression ratio for conserving reads data storage space.
Results
We employ compression ratio as the optimization objective and propose a large-scale genomic sequencing short reads data compression optimizer, named PMFFRC, through novelty memory modeling and redundant reads clustering technologies. By cascading PMFFRC, in 982 GB fastq format sequencing data, with 274 GB and 3.3 billion short reads, the state-of-the-art and reference-free compressors HARC, SPRING, Mstcom, and FastqCLS achieve 77.89%, 77.56%, 73.51%, and 29.36% average maximum compression ratio gains, respectively. PMFFRC saves 39.41%, 41.62%, 40.99%, and 20.19% of storage space sizes compared with the four unoptimized compressors.
Conclusions
PMFFRC rational usage big-memory of compression server, effectively saving the sequencing reads data storage space sizes, which relieves the basic storage facilities costs and community sharing transmitting overhead. Our work furnishes a novel solution for improving sequencing reads compression and saving storage space. The proposed PMFFRC algorithm is packaged in a same-name Linux toolkit, available un-limited at 
https://github.com/fahaihi/PMFFRC
."
39,Completing a genomic characterisation of microscopic tumour samples with copy number,"Background
Genomic insights in settings where tumour sample sizes are limited to just hundreds or even tens of cells hold great clinical potential, but also present significant technical challenges. We previously developed the DigiPico sequencing platform to accurately identify somatic mutations from such samples.
Results
Here, we complete this genomic characterisation with copy number. We present a novel protocol, PicoCNV, to call allele-specific somatic copy number alterations from picogram quantities of tumour DNA. We find that PicoCNV provides exactly accurate copy number in 84% of the genome for even the smallest samples, and demonstrate its clinical potential in maintenance therapy.
Conclusions
PicoCNV complements our existing platform, allowing for accurate and comprehensive genomic characterisations of cancers in settings where only microscopic samples are available."
40,A seed expansion-based method to identify essential proteins by integrating protein–protein interaction sub-networks and multiple biological characteristics,"Background
The identification of essential proteins is of great significance in biology and pathology. However, protein–protein interaction (PPI) data obtained through high-throughput technology include a high number of false positives. To overcome this limitation, numerous computational algorithms based on biological characteristics and topological features have been proposed to identify essential proteins.
Results
In this paper, we propose a novel method named SESN for identifying essential proteins. It is a seed expansion method based on PPI sub-networks and multiple biological characteristics. Firstly, SESN utilizes gene expression data to construct PPI sub-networks. Secondly, seed expansion is performed simultaneously in each sub-network, and the expansion process is based on the topological features of predicted essential proteins. Thirdly, the error correction mechanism is based on multiple biological characteristics and the entire PPI network. Finally, SESN analyzes the impact of each biological characteristic, including protein complex, gene expression data, GO annotations, and subcellular localization, and adopts the biological data with the best experimental results. The output of SESN is a set of predicted essential proteins.
Conclusions
The analysis of each component of SESN indicates the effectiveness of all components. We conduct comparison experiments using three datasets from two species, and the experimental results demonstrate that SESN achieves superior performance compared to other methods."
41,Biocaiv: an integrative webserver for motif-based clustering analysis and interactive visualization of biological networks,"Background
As an important task in bioinformatics, clustering analysis plays a critical role in understanding the functional mechanisms of many complex biological systems, which can be modeled as biological networks. The purpose of clustering analysis in biological networks is to identify functional modules of interest, but there is a lack of online clustering tools that visualize biological networks and provide in-depth biological analysis for discovered clusters.
Results
Here we present BioCAIV, a novel webserver dedicated to maximize its accessibility and applicability on the clustering analysis of biological networks. This, together with its user-friendly interface, assists biological researchers to perform an accurate clustering analysis for biological networks and identify functionally significant modules for further assessment.
Conclusions
BioCAIV is an efficient clustering analysis webserver designed for a variety of biological networks. BioCAIV is freely available without registration requirements at 
http://bioinformatics.tianshanzw.cn:8888/BioCAIV/
."
42,Detection of continuous hierarchical heterogeneity by single-cell surface antigen analysis in the prognosis evaluation of acute myeloid leukaemia,"Background
Acute myeloid leukaemia (AML) is characterised by the malignant accumulation of myeloid progenitors with a high recurrence rate after chemotherapy. Blasts (leukaemia cells) exhibit a complete myeloid differentiation hierarchy hiding a wide range of temporal information from initial to mature clones, including genesis, phenotypic transformation, and cell fate decisions, which might contribute to relapse in AML patients.
Methods
Based on the landscape of AML surface antigens generated by mass cytometry (CyTOF), we combined manifold analysis and principal curve-based trajectory inference algorithm to align myelocytes on a single-linear evolution axis by considering their phenotype continuum that correlated with differentiation order. Backtracking the trajectory from mature clusters located automatically at the terminal, we recurred the molecular dynamics during AML progression and confirmed the evolution stage of single cells. We also designed a ‘dispersive antigens in neighbouring clusters exhibition (DANCE)’ feature selection method to simplify and unify trajectories, which enabled the exploration and comparison of relapse-related traits among 43 paediatric AML bone marrow specimens.
Results
The feasibility of the proposed trajectory analysis method was verified with public datasets. After aligning single cells on the pseudotime axis, primitive clones were recognized precisely from AML blasts, and the expression of the inner molecules before and after drug stimulation was accurately plotted on the trajectory. Applying DANCE to 43 clinical samples with different responses for chemotherapy, we selected 12 antigens as a general panel for myeloblast differentiation performance, and obtain trajectories to those patients. For the trajectories with unified molecular dynamics, CD11c overexpression in the primitive stage indicated a good chemotherapy outcome. Moreover, a later initial peak of stemness heterogeneity tended to be associated with a higher risk of relapse compared with complete remission.
Conclusions
In this study, pseudotime was generated as a new single-cell feature. Minute differences in temporal traits among samples could be exhibited on a trajectory, thus providing a new strategy for predicting AML relapse and monitoring drug responses over time scale."
43,Machine learning-based approaches for ubiquitination site prediction in human proteins,"Protein ubiquitination is a critical post-translational modification (PTMs) involved in numerous cellular processes. Identifying ubiquitination sites (Ubi-sites) on proteins offers valuable insights into their function and regulatory mechanisms. Due to the cost- and time-consuming nature of traditional approaches for Ubi-site detection, there has been a growing interest in leveraging artificial intelligence for computer-aided Ubi-site prediction. In this study, we collected experimentally verified Ubi-sites of human proteins from the dbPTM database, then conducted comprehensive state-of-the art computational methods along with standard evaluation metrics and a proper validation strategy for Ubi-site prediction. We presented the effectiveness of our framework by comparing ten machine learning (ML) based approaches in three different categories: feature-based conventional ML methods, end-to-end sequence-based deep learning (DL) techniques, and hybrid feature-based DL models. Our results revealed that DL approaches outperformed the classical ML methods, achieving a 0.902 F1-score, 0.8198 accuracy, 0.8786 precision, and 0.9147 recall as the best performance for a DL model using both raw amino acid sequences and hand-crafted features. Interestingly, our experimental results disclosed that the performance of DL methods had a positive correlation with the length of amino acid fragments, suggesting that utilizing the entire sequence can lead to more accurate predictions in future research endeavors. Additionally, we developed a meticulously curated benchmark for Ubi-site prediction in human proteins. This benchmark serves as a valuable resource for future studies, enabling fair and accurate comparisons between different methods. Overall, our work highlights the potential of ML, particularly DL techniques, in predicting Ubi-sites and furthering our knowledge of protein regulation through ubiquitination in cells."
44,Predicting anticancer synergistic drug combinations based on multi-task learning,"Background
The discovery of anticancer drug combinations is a crucial work of anticancer treatment. In recent years, pre-screening drug combinations with synergistic effects in a large-scale search space adopting computational methods, especially deep learning methods, is increasingly popular with researchers. Although achievements have been made to predict anticancer synergistic drug combinations based on deep learning, the application of multi-task learning in this field is relatively rare. The successful practice of multi-task learning in various fields shows that it can effectively learn multiple tasks jointly and improve the performance of all the tasks.
Methods
In this paper, we propose MTLSynergy which is based on multi-task learning and deep neural networks to predict synergistic anticancer drug combinations. It simultaneously learns two crucial prediction tasks in anticancer treatment, which are synergy prediction of drug combinations and sensitivity prediction of monotherapy. And MTLSynergy integrates the classification and regression of prediction tasks into the same model. Moreover, autoencoders are employed to reduce the dimensions of input features.
Results
Compared with the previous methods listed in this paper, MTLSynergy achieves the lowest mean square error of 216.47 and the highest Pearson correlation coefficient of 0.76 on the drug synergy prediction task. On the corresponding classification task, the area under the receiver operator characteristics curve and the area under the precision–recall curve are 0.90 and 0.62, respectively, which are equivalent to the comparison methods. Through the ablation study, we verify that multi-task learning and autoencoder both have a positive effect on prediction performance. In addition, the prediction results of MTLSynergy in many cases are also consistent with previous studies.
Conclusion
Our study suggests that multi-task learning is significantly beneficial for both drug synergy prediction and monotherapy sensitivity prediction when combining these two tasks into one model. The ability of MTLSynergy to discover new anticancer synergistic drug combinations noteworthily outperforms other state-of-the-art methods. MTLSynergy promises to be a powerful tool to pre-screen anticancer synergistic drug combinations."
45,AptaTrans: a deep neural network for predicting aptamer-protein interaction using pretrained encoders,"Background
Aptamers, which are biomaterials comprised of single-stranded DNA/RNA that form tertiary structures, have significant potential as next-generation materials, particularly for drug discovery. The systematic evolution of ligands by exponential enrichment (SELEX) method is a critical in vitro technique employed to identify aptamers that bind specifically to target proteins. While advanced SELEX-based methods such as Cell- and HT-SELEX are available, they often encounter issues such as extended time consumption and suboptimal accuracy. Several In silico aptamer discovery methods have been proposed to address these challenges. These methods are specifically designed to predict aptamer-protein interaction (API) using benchmark datasets. However, these methods often fail to consider the physicochemical interactions between aptamers and proteins within tertiary structures.
Results
In this study, we propose AptaTrans, a pipeline for predicting API using deep learning techniques. AptaTrans uses transformer-based encoders to handle aptamer and protein sequences at the monomer level. Furthermore, pretrained encoders are utilized for the structural representation. After validation with a benchmark dataset, AptaTrans has been integrated into a comprehensive toolset. This pipeline synergistically combines with Apta-MCTS, a generative algorithm for recommending aptamer candidates.
Conclusion
The results show that AptaTrans outperforms existing models for predicting API, and the efficacy of the AptaTrans pipeline has been confirmed through various experimental tools. We expect AptaTrans will enhance the cost-effectiveness and efficiency of SELEX in drug discovery. The source code and benchmark dataset for AptaTrans are available at 
https://github.com/pnumlb/AptaTrans
."
46,Transformer-based tool recommendation system in Galaxy,"Background
Galaxy is a web-based open-source platform for scientific analyses. Researchers use thousands of high-quality tools and workflows for their respective analyses in Galaxy. Tool recommender system predicts a collection of tools that can be used to extend an analysis. In this work, a tool recommender system is developed by training a transformer on workflows available on Galaxy Europe and its performance is compared to other neural networks such as recurrent, convolutional and dense neural networks.
Results
The transformer neural network achieves two times faster convergence, has significantly lower model usage (model reconstruction and prediction) time and shows a better generalisation that goes beyond training workflows than the older tool recommender system created using RNN in Galaxy. In addition, the transformer also outperforms CNN and DNN on several key indicators. It achieves a faster convergence time, lower model usage time, and higher quality tool recommendations than CNN. Compared to DNN, it converges faster to a higher precision@k metric (approximately 0.98 by transformer compared to approximately 0.9 by DNN) and shows higher quality tool recommendations.
Conclusion
Our work shows a novel usage of transformers to recommend tools for extending scientific workflows. A more robust tool recommendation model, created using a transformer, having significantly lower usage time than RNN and CNN, higher precision@k than DNN, and higher quality tool recommendations than all three neural networks, will benefit researchers in creating scientifically significant workflows and exploratory data analysis in Galaxy. Additionally, the ability to train faster than all three neural networks imparts more scalability for training on larger datasets consisting of millions of tool sequences. Open-source scripts to create the recommendation model are available under MIT licence at 
https://github.com/anuprulez/galaxy_tool_recommendation_transformers"
47,scMuffin: an R package to disentangle solid tumor heterogeneity by single-cell gene expression analysis,"Introduction
Single-cell (SC) gene expression analysis is crucial to dissect the complex cellular heterogeneity of solid tumors, which is one of the main obstacles for the development of effective cancer treatments. Such tumors typically contain a mixture of cells with aberrant genomic and transcriptomic profiles affecting specific sub-populations that might have a pivotal role in cancer progression, whose identification eludes bulk RNA-sequencing approaches. We present scMuffin, an R package that enables the characterization of cell identity in solid tumors on the basis of a various and complementary analyses on SC gene expression data.
Results
scMuffin provides a series of functions to calculate qualitative and quantitative scores, such as: expression of marker sets for normal and tumor conditions, pathway activity, cell state trajectories, Copy Number Variations, transcriptional complexity and proliferation state. Thus, scMuffin facilitates the combination of various evidences that can be used to distinguish normal and tumoral cells, define cell identities, cluster cells in different ways, link genomic aberrations to phenotypes and identify subtle differences between cell subtypes or cell states. We analysed public SC expression datasets of human high-grade gliomas as a proof-of-concept to show the value of scMuffin and illustrate its user interface. Nevertheless, these analyses lead to interesting findings, which suggest that some chromosomal amplifications might underlie the invasive tumor phenotype and the presence of cells that possess tumor initiating cells characteristics.
Conclusions
The analyses offered by scMuffin and the results achieved in the case study show that our tool helps addressing the main challenges in the bioinformatics analysis of SC expression data from solid tumors."
48,CoDock-Ligand: combined template-based docking and CNN-based scoring in ligand binding prediction,"For ligand binding prediction, it is crucial for molecular docking programs to integrate template-based modeling with a precise scoring function. Here, we proposed the CoDock-Ligand docking method that combines template-based modeling and the GNINA scoring function, a Convolutional Neural Network-based scoring function, for the ligand binding prediction in CASP15. Among the 21 targets, we obtained successful predictions in top 5 submissions for 14 targets and partially successful predictions for 4 targets. In particular, for the most complicated target, H1114, which contains 56 metal cofactors and small molecules, our docking method successfully predicted the binding of most ligands. Analysis of the failed systems showed that the predicted receptor protein presented conformational changes in the backbone and side chains of the binding site residues, which may cause large structural deviations in the ligand binding prediction. In summary, our hybrid docking scheme was efficiently adapted to the ligand binding prediction challenges in CASP15."
49,Relating mutational signature exposures to clinical data in cancers via signeR 2.0,"Background
Cancer is a collection of diseases caused by the deregulation of cell processes, which is triggered by somatic mutations. The search for patterns in somatic mutations, known as mutational signatures, is a growing field of study that has already become a useful tool in oncology. Several algorithms have been proposed to perform one or both the following two tasks: (1) de novo estimation of signatures and their exposures, (2) estimation of the exposures of each one of a set of pre-defined signatures.
Results
Our group developed signeR, a Bayesian approach to both of these tasks. Here we present a new version of the software, signeR 2.0, which extends the possibilities of previous analyses to explore the relation of signature exposures to other data of clinical relevance. signeR 2.0 includes a user-friendly interface developed using the R-Shiny framework and improvements in performance. This version allows the analysis of submitted data or public TCGA data, which is embedded in the package for easy access.
Conclusion
signeR 2.0 is a valuable tool to generate and explore exposure data, both from de novo or fitting analyses and is an open-source R package available through the Bioconductor project at (
https://doi.org/10.18129/B9.bioc.signeR
)."
50,MiREx: mRNA levels prediction from gene sequence and miRNA target knowledge,"Messenger RNA (mRNA) has an essential role in the protein production process. Predicting mRNA expression levels accurately is crucial for understanding gene regulation, and various models (statistical and neural network-based) have been developed for this purpose. A few models predict mRNA expression levels from the DNA sequence, exploiting the DNA sequence and gene features (e.g., number of exons/introns, gene length). Other models include information about long-range interaction molecules (i.e., enhancers/silencers) and transcriptional regulators as predictive features, such as transcription factors (TFs) and small RNAs (e.g., microRNAs - miRNAs). Recently, a convolutional neural network (CNN) model, called Xpresso, has been proposed for mRNA expression level prediction leveraging the promoter sequence and mRNAs’ half-life features (gene features). To push forward the mRNA level prediction, we present miREx, a CNN-based tool that includes information about miRNA targets and expression levels in the model. Indeed, each miRNA can target specific genes, and the model exploits this information to guide the learning process. In detail, not all miRNAs are included, only a selected subset with the highest impact on the model. MiREx has been evaluated on four cancer primary sites from the genomics data commons (GDC) database: lung, kidney, breast, and corpus uteri. Results show that mRNA level prediction benefits from selected miRNA targets and expression information. Future model developments could include other transcriptional regulators or be trained with proteomics data to infer protein levels."
51,A novel efficient drug repurposing framework through drug-disease association data integration using convolutional neural networks,"Drug repurposing is an exciting field of research toward recognizing a new FDA-approved drug target for the treatment of a specific disease. It has received extensive attention regarding the tedious, time-consuming, and highly expensive procedure with a high risk of failure of new drug discovery. Data-driven approaches are an important class of methods that have been introduced for identifying a candidate drug against a target disease. In the present study, a model is proposed illustrating the integration of drug-disease association data for drug repurposing using a deep neural network. The model, so-called IDDI-DNN, primarily constructs similarity matrices for drug-related properties (three matrices), disease-related properties (two matrices), and drug-disease associations (one matrix). Then, these matrices are integrated into a unique matrix through a two-step procedure benefiting from the similarity network fusion method. The model uses a constructed matrix for the prediction of novel and unknown drug-disease associations through a convolutional neural network. The proposed model was evaluated comparatively using two different datasets including the gold standard dataset and DNdataset. Comparing the results of evaluations indicates that IDDI-DNN outperforms other state-of-the-art methods concerning prediction accuracy."
52,Improved quality metrics for association and reproducibility in chromatin accessibility data using mutual information,"Background
Correlation metrics are widely utilized in genomics analysis and often implemented with little regard to assumptions of normality, homoscedasticity, and independence of values. This is especially true when comparing values between replicated sequencing experiments that probe chromatin accessibility, such as assays for transposase-accessible chromatin via sequencing (ATAC-seq). Such data can possess several regions across the human genome with little to no sequencing depth and are thus non-normal with a large portion of zero values. Despite distributed use in the epigenomics field, few studies have evaluated and benchmarked how correlation and association statistics behave across ATAC-seq experiments with known differences or the effects of removing specific outliers from the data. Here, we developed a computational simulation of ATAC-seq data to elucidate the behavior of correlation statistics and to compare their accuracy under set conditions of reproducibility.
Results
Using these simulations, we monitored the behavior of several correlation statistics, including the Pearson’s 
R
 and Spearman’s 
\(\rho\)
 coefficients as well as Kendall’s 
\(\tau\)
 and Top–Down correlation. We also test the behavior of association measures, including the coefficient of determination 
R
\(^2\)
, Kendall’s W, and normalized mutual information. Our experiments reveal an insensitivity of most statistics, including Spearman’s 
\(\rho\)
, Kendall’s 
\(\tau\)
, and Kendall’s W, to increasing differences between simulated ATAC-seq replicates. The removal of co-zeros (regions lacking mapped sequenced reads) between simulated experiments greatly improves the estimates of correlation and association. After removing co-zeros, the 
R
\(^2\)
 coefficient and normalized mutual information display the best performance, having a closer one-to-one relationship with the known portion of shared, enhanced loci between simulated replicates. When comparing values between experimental ATAC-seq data using a random forest model, mutual information best predicts ATAC-seq replicate relationships.
Conclusions
Collectively, this study demonstrates how measures of correlation and association can behave in epigenomics experiments. We provide improved strategies for quantifying relationships in these increasingly prevalent and important chromatin accessibility assays."
53,A cell abundance analysis based on efficient PAM clustering for a better understanding of the dynamics of endometrial remodelling,"Background
Single-cell RNA sequencing (scRNA-seq) is a powerful tool for investigating cell abundance changes during tissue regeneration and remodeling processes. Differential cell abundance supports the initial clustering of all cells; then, the number of cells per cluster and sample are evaluated, and the dependence of these counts concerning the phenotypic covariates of the samples is studied. Analysis heavily depends on the clustering method. Partitioning Around Medoids (PAM or k-medoids) represents a well-established clustering procedure that leverages the downstream interpretation of clusters by pinpointing real individuals in the dataset as cluster centers (medoids) without reducing dimensions. Of note, PAM suffers from high computational costs and memory requirements.
Results
This paper proposes a method for differential abundance analysis using PAM as a clustering method and negative binomial regression as a statistical model to relate covariates to cluster/cell counts. We used this approach to study the differential cell abundance of human endometrial cell types throughout the natural secretory phase of the menstrual cycle. We developed a new R package 
-scellpam-
, that incorporates an efficient parallel C++ implementation of PAM, and applied this package in this study. We compared the PAM-BS clustering method with other methods and evaluated both the computational aspects of its implementation and the quality of the classifications obtained using distinct published datasets with known subpopulations that demonstrate promising results.
Conclusions
The implementation of PAM-BS, included in the 
scellpam
 package, exhibits robust performance in terms of speed and memory usage compared to other related methods. PAM allowed quick and robust clustering of sets of cells with a size ranging from 70,000 to 300,000 cells. 
https://cran.r-project.org/web/packages/scellpam/index.html
. Finally, our approach provides important new insights into the transient subpopulations associated with the fertile time frame when applied to the study of changes in the human endometrium during the secretory phase of the menstrual cycle."
54,"Genome-scale metabolic models reveal determinants of phenotypic differences in non-
Saccharomyces
 yeasts","Background
Use of alternative non-
Saccharomyces
 yeasts in wine and beer brewing has gained more attention the recent years. This is both due to the desire to obtain a wider variety of flavours in the product and to reduce the final alcohol content. Given the metabolic differences between the yeast species, we wanted to account for some of the differences by using 
in silico
 models. 
Results
 We created and studied genome-scale metabolic models of five different non-
Saccharomyces
 species using an automated processes. These were: 
Metschnikowia pulcherrima
, 
Lachancea thermotolerans
, 
Hanseniaspora osmophila
, 
Torulaspora delbrueckii
 and 
Kluyveromyces lactis
. Using the models, we predicted that 
M. pulcherrima
, when compared to the other species, conducts more respiration and thus produces less fermentation products, a finding which agrees with experimental data. Complex I of the electron transport chain was to be present in 
M. pulcherrima
, but absent in the others. The predicted importance of Complex I was diminished when we incorporated constraints on the amount of enzymatic protein, as this shifts the metabolism towards fermentation.
Conclusions
 Our results suggest that Complex I in the electron transport chain is a key differentiator between 
Metschnikowia pulcherrima
 and the other yeasts considered. Yet, more annotations and experimental data have the potential to improve model quality in order to increase fidelity and confidence in these results. Further experiments should be conducted to confirm the 
in vivo
 effect of Complex I in 
M. pulcherrima
 and its respiratory metabolism."
55,Image-centric compression of protein structures improves space savings,"Background
Because of the rapid generation of data, the study of compression algorithms to reduce storage and transmission costs is important to bioinformaticians. Much of the focus has been on sequence data, including both genomes and protein amino acid sequences stored in FASTA files. Current standard practice is to use an ordinary lossless compressor such as gzip on a sequential list of atomic coordinates, but this approach expends bits on saving an arbitrary ordering of atoms, and it also prevents reordering the atoms for compressibility. The standard MMTF and BCIF file formats extend this approach with custom encoding of the coordinates. However, the brand new Foldcomp tool introduces a new paradigm of compressing local angles, to great effect. In this article, we explore a different paradigm, showing for the first time that image-based compression using global angles can also significantly improve compression ratios. To this end, we implement a prototype compressor ‘PIC’, specialized for point clouds of atom coordinates contained in PDB and mmCIF files. PIC maps the 3D data to a 2D 8-bit greyscale image and leverages the well developed PNG image compressor to minimize the size of the resulting image, forming the compressed file.
Results
PIC outperforms gzip in terms of compression ratio on proteins over 20,000 atoms in size, with a savings over gzip of up to 37.4% on the proteins compressed. In addition, PIC’s compression ratio increases with protein size.
Conclusion
Image-centric compression as demonstrated by our prototype PIC provides a potential means of constructing 3D structure-aware protein compression software, though future work would be necessary to make this practical."
56,GraphTar: applying word2vec and graph neural networks to miRNA target prediction,"Background
MicroRNAs (miRNAs) are short, non-coding RNA molecules that regulate gene expression by binding to specific mRNAs, inhibiting their translation. They play a critical role in regulating various biological processes and are implicated in many diseases, including cardiovascular, oncological, gastrointestinal diseases, and viral infections. Computational methods that can identify potential miRNA–mRNA interactions from raw data use one-dimensional miRNA–mRNA duplex representations and simple sequence encoding techniques, which may limit their performance.
Results
We have developed GraphTar, a new target prediction method that uses a novel graph-based representation to reflect the spatial structure of the miRNA–mRNA duplex. Unlike existing approaches, we use the word2vec method to accurately encode RNA sequence information. In conjunction with the novel encoding method, we use a graph neural network classifier that can accurately predict miRNA–mRNA interactions based on graph representation learning. As part of a comparative study, we evaluate three different node embedding approaches within the GraphTar framework and compare them with other state-of-the-art target prediction methods. The results show that the proposed method achieves similar performance to the best methods in the field and outperforms them on one of the datasets.
Conclusions
In this study, a novel miRNA target prediction approach called GraphTar is introduced. Results show that GraphTar is as effective as existing methods and even outperforms them in some cases, opening new avenues for further research. However, the expansion of available datasets is critical for advancing the field towards real-world applications."
57,RUBic: rapid unsupervised biclustering,"Biclustering of biologically meaningful binary information is essential in many applications related to drug discovery, like protein–protein interactions and gene expressions. However, for robust performance in recently emerging large health datasets, it is important for new biclustering algorithms to be scalable and fast. We present a rapid unsupervised biclustering (RUBic) algorithm that achieves this objective with a novel encoding and search strategy. RUBic significantly reduces the computational overhead on both synthetic and experimental datasets shows significant computational benefits, with respect to several 
state-of-the-art
 biclustering algorithms. In 100 synthetic binary datasets, our method took 
\(\sim 71.1\)
 s to extract 494,872 biclusters. In the human PPI database of size 
\(4085\times 4085\)
, our method generates 1840 biclusters in 
\(\sim 48.6\)
 s. On a central nervous system embryonic tumor gene expression dataset of size 712,940, our algorithm takes   101 min to produce 747,069 biclusters, while the recent competing algorithms take significantly more time to produce the same result. RUBic is also evaluated on five different gene expression datasets and shows significant speed-up in execution time with respect to existing approaches to extract significant KEGG-enriched bi-clustering. RUBic can operate on two modes, base and flex, where base mode generates maximal biclusters and flex mode generates less number of clusters and faster based on their biological significance with respect to KEGG pathways. The code is available at (
https://github.com/CMATERJU-BIOINFO/RUBic
) for academic use only."
58,PathExpSurv: pathway expansion for explainable survival analysis and disease gene discovery,"Background
In the field of biology and medicine, the interpretability and accuracy are both important when designing predictive models. The interpretability of many machine learning models such as neural networks is still a challenge. Recently, many researchers utilized prior information such as biological pathways to develop neural networks-based methods, so as to provide some insights and interpretability for the models. However, the prior biological knowledge may be incomplete and there still exists some unknown information to be explored.
Results
We proposed a novel method, named PathExpSurv, to gain an insight into the black-box model of neural network for cancer survival analysis. We demonstrated that PathExpSurv could not only incorporate the known prior information into the model, but also explore the unknown possible expansion to the existing pathways. We performed downstream analyses based on the expanded pathways and successfully identified some key genes associated with the diseases and original pathways.
Conclusions
Our proposed PathExpSurv is a novel, effective and interpretable method for survival analysis. It has great utility and value in medical diagnosis and offers a promising framework for biological research."
59,Protein language models can capture protein quaternary state,"Background
Determining a protein’s quaternary state, 
i.e.
 the number of monomers in a functional unit, is a critical step in protein characterization. Many proteins form multimers for their activity, and over 50% are estimated to naturally form homomultimers. Experimental quaternary state determination can be challenging and require extensive work. To complement these efforts, a number of computational tools have been developed for quaternary state prediction, often utilizing experimentally validated structural information. Recently, dramatic advances have been made in the field of deep learning for predicting protein structure and other characteristics. Protein language models, such as ESM-2, that apply computational natural-language models to proteins successfully capture secondary structure, protein cell localization and other characteristics, from a single sequence. Here we hypothesize that information about the protein quaternary state may be contained within protein sequences as well, allowing us to benefit from these novel approaches in the context of quaternary state prediction.
Results
We generated ESM-2 embeddings for a large dataset of proteins with quaternary state labels from the curated QSbio dataset. We trained a model for quaternary state classification and assessed it on a non-overlapping set of distinct folds (ECOD family level). Our model, named QUEEN (QUaternary state prediction using dEEp learNing), performs worse than approaches that include information from solved crystal structures. However, it successfully learned to distinguish multimers from monomers, and predicts the specific quaternary state with moderate success, better than simple sequence similarity-based annotation transfer. Our results demonstrate that complex, quaternary state related information is included in such embeddings.
Conclusions
QUEEN is the first to investigate the power of embeddings for the prediction of the quaternary state of proteins. As such, it lays out strengths as well as limitations of a sequence-based protein language model approach, compared to structure-based approaches. Since it does not require any structural information and is fast, we anticipate that it will be of wide use both for in-depth investigation of specific systems, as well as for studies of large sets of protein sequences. A simple colab implementation is available at: 
https://colab.research.google.com/github/Furman-Lab/QUEEN/blob/main/QUEEN_prediction_notebook.ipynb
."
60,Similarity-assisted variational autoencoder for nonlinear dimension reduction with application to single-cell RNA sequencing data,"Background
Deep generative models naturally become nonlinear dimension reduction tools to visualize large-scale datasets such as single-cell RNA sequencing datasets for revealing latent grouping patterns or identifying outliers. The variational autoencoder (VAE) is a popular deep generative method equipped with encoder/decoder structures. The encoder and decoder are useful when a new sample is mapped to the latent space and a data point is generated from a point in a latent space. However, the VAE tends not to show grouping pattern clearly without additional annotation information. On the other hand, similarity-based dimension reduction methods such as t-SNE or UMAP present clear grouping patterns even though these methods do not have encoder/decoder structures.
Results
To bridge this gap, we propose a new approach that adopts similarity information in the VAE framework. In addition, for biological applications, we extend our approach to a conditional VAE to account for covariate effects in the dimension reduction step. In the simulation study and real single-cell RNA sequencing data analyses, our method shows great performance compared to existing state-of-the-art methods by producing clear grouping structures using an inferred encoder and decoder. Our method also successfully adjusts for covariate effects, resulting in more useful dimension reduction.
Conclusions
Our method is able to produce clearer grouping patterns than those of other regularized VAE methods by utilizing similarity information encoded in the data via the highly celebrated UMAP loss function."
61,G-Aligner: a graph-based feature alignment method for untargeted LC–MS-based metabolomics,"Background
Liquid chromatography–mass spectrometry is widely used in untargeted metabolomics for composition profiling. In multi-run analysis scenarios, features of each run are aligned into consensus features by feature alignment algorithms to observe the intensity variations across runs. However, most of the existing feature alignment methods focus more on accurate retention time correction, while underestimating the importance of feature matching. None of the existing methods can comprehensively consider feature correspondences among all runs and achieve optimal matching.
Results
To comprehensively analyze feature correspondences among runs, we propose G-Aligner, a graph-based feature alignment method for untargeted LC–MS data. In the feature matching stage, G-Aligner treats features and potential correspondences as nodes and edges in a multipartite graph, considers the multi-run feature matching problem an unbalanced multidimensional assignment problem, and provides three combinatorial optimization algorithms to find optimal matching solutions. In comparison with the feature alignment methods in OpenMS, MZmine2 and XCMS on three public metabolomics benchmark datasets, G-Aligner achieved the best feature alignment performance on all the three datasets with up to 9.8% and 26.6% increase in accurately aligned features and analytes, and helped all comparison software obtain more accurate results on their self-extracted features by integrating G-Aligner to their analysis workflow. G-Aligner is open-source and freely available at 
https://github.com/CSi-Studio/G-Aligner
 under a permissive license. Benchmark datasets, manual annotation results, evaluation methods and results are available at 
https://doi.org/10.5281/zenodo.8313034
Conclusions
In this study, we proposed G-Aligner to improve feature matching accuracy for untargeted metabolomics LC–MS data. G-Aligner comprehensively considered potential feature correspondences between all runs, converting the feature matching problem as a multidimensional assignment problem (MAP). In evaluations on three public metabolomics benchmark datasets, G-Aligner achieved the highest alignment accuracy on manual annotated and popular software extracted features, proving the effectiveness and robustness of the algorithm."
62,DG-Affinity: predicting antigen–antibody affinity with language models from sequences,"Background
Antibody-mediated immune responses play a crucial role in the immune defense of human body. The evolution of bioengineering has led the progress of antibody-derived drugs, showing promising efficacy in cancer and autoimmune disease therapy. A critical step of this development process is obtaining the affinity between antibodies and their binding antigens.
Results
In this study, we introduce a novel sequence-based antigen–antibody affinity prediction method, named DG-Affinity. DG-Affinity uses deep neural networks to efficiently and accurately predict the affinity between antibodies and antigens from sequences, without the need for structural information. The sequences of both the antigen and the antibody are first transformed into embedding vectors by two pre-trained language models, then these embeddings are concatenated into an ConvNeXt framework with a regression task. The results demonstrate the superiority of DG-Affinity over the existing structure-based prediction methods and the sequence-based tools, achieving a Pearson’s correlation of over 0.65 on an independent test dataset.
Conclusions
Compared to the baseline methods, DG-Affinity achieves the best performance and can advance the development of antibody design. It is freely available as an easy-to-use web server at 
https://www.digitalgeneai.tech/solution/affinity
."
63,A novel two-way rebalancing strategy for identifying carbonylation sites,"Background
As an irreversible post-translational modification, protein carbonylation is closely related to many diseases and aging. Protein carbonylation prediction for related patients is significant, which can help clinicians make appropriate therapeutic schemes. Because carbonylation sites can be used to indicate change or loss of protein function, integrating these protein carbonylation site data has been a promising method in prediction. Based on these protein carbonylation site data, some protein carbonylation prediction methods have been proposed. However, most data is highly class imbalanced, and the number of un-carbonylation sites greatly exceeds that of carbonylation sites. Unfortunately, existing methods have not addressed this issue adequately.
Results
In this work, we propose a novel two-way rebalancing strategy based on the attention technique and generative adversarial network (Carsite_AGan) for identifying protein carbonylation sites. Specifically, Carsite_AGan proposes a novel undersampling method based on attention technology that allows sites with high importance value to be selected from un-carbonylation sites. The attention technique can obtain the value of each sample’s importance. In the meanwhile, Carsite_AGan designs a generative adversarial network-based oversampling method to generate high-feasibility carbonylation sites. The generative adversarial network can generate high-feasibility samples through its generator and discriminator. Finally, we use a classifier like a nonlinear support vector machine to identify protein carbonylation sites.
Conclusions
Experimental results demonstrate that our approach significantly outperforms other resampling methods. Using our approach to resampling carbonylation data can significantly improve the effect of identifying protein carbonylation sites."
64,Optimizing diabetes classification with a machine learning-based framework,"Background
Diabetes is a metabolic disorder usually caused by insufficient secretion of insulin from the pancreas or insensitivity of cells to insulin, resulting in long-term elevated blood sugar levels in patients. Patients usually present with frequent urination, thirst, and hunger. If left untreated, it can lead to various complications that can affect essential organs and even endanger life. Therefore, developing an intelligent diagnosis framework for diabetes is necessary.
Result
This paper proposes a machine learning-based diabetes classification framework machine learning optimized GAN. The framework encompasses several methodological approaches to address the diverse challenges encountered during the analysis. These approaches encompass the implementation of the mean and median joint filling method for handling missing values, the application of the cap method for outlier processing, and the utilization of SMOTEENN to mitigate sample imbalance. Additionally, the framework incorporates the employment of the proposed Diabetes Classification Model based on Generative Adversarial Network and employs logistic regression for detailed feature analysis. The effectiveness of the framework is evaluated using both the PIMA dataset and the diabetes dataset obtained from the GEO database. The experimental findings showcase our model achieved exceptional results, including a binary classification accuracy of 96.27%, tertiary classification accuracy of 99.31%, precision and f1 score of 0.9698, recall of 0.9698, and an AUC of 0.9702.
Conclusion
The experimental results show that the framework proposed in this paper can accurately classify diabetes and provide new ideas for intelligent diagnosis of diabetes."
65,Mdwgan-gp: data augmentation for gene expression data based on multiple discriminator WGAN-GP,"Background
Although gene expression data play significant roles in biological and medical studies, their applications are hampered due to the difficulty and high expenses of gathering them through biological experiments. It is an urgent problem to generate high quality gene expression data with computational methods. WGAN-GP, a generative adversarial network-based method, has been successfully applied in augmenting gene expression data. However, mode collapse or over-fitting may take place for small training samples due to just one discriminator is adopted in the method.
Results
In this study, an improved data augmentation approach MDWGAN-GP, a generative adversarial network model with multiple discriminators, is proposed. In addition, a novel method is devised for enriching training samples based on linear graph convolutional network. Extensive experiments were implemented on real biological data.
Conclusions
The experimental results have demonstrated that compared with other state-of-the-art methods, the MDWGAN-GP method can produce higher quality generated gene expression data in most cases."
66,Statistical modeling to quantify the uncertainty of FoldX-predicted protein folding and binding stability,"Background
Computational methods of predicting protein stability changes upon missense mutations are invaluable tools in high-throughput studies involving a large number of protein variants. However, they are limited by a wide variation in accuracy and difficulty of assessing prediction uncertainty. Using a popular computational tool, FoldX, we develop a statistical framework that quantifies the uncertainty of predicted changes in protein stability.
Results
We show that multiple linear regression models can be used to quantify the uncertainty associated with FoldX prediction for individual mutations. Comparing the performance among models with varying degrees of complexity, we find that the model precision improves significantly when we utilize molecular dynamics simulation as part of the FoldX workflow. Based on the model that incorporates information from molecular dynamics, biochemical properties, as well as FoldX energy terms, we can generally expect upper bounds on the uncertainty of folding stability predictions of ± 2.9 kcal/mol and ± 3.5 kcal/mol for binding stability predictions. The uncertainty for individual mutations varies; our model estimates it using FoldX energy terms, biochemical properties of the mutated residue, as well as the variability among snapshots from molecular dynamics simulation.
Conclusions
Using a linear regression framework, we construct models to predict the uncertainty associated with FoldX prediction of stability changes upon mutation. This technique is straightforward and can be extended to other computational methods as well."
67,Role of environmental specificity in CASP results,"Background
Recently, significant progress has been made in the field of protein structure prediction by the application of artificial intelligence techniques, as shown by the results of the CASP13 and CASP14 (Critical Assessment of Structure Prediction) competition. However, the question of the mechanism behind the protein folding process itself remains unanswered. Correctly predicting the structure also does not solve the problem of, for example, amyloid proteins, where a polypeptide chain with an unaltered sequence adopts a different 3D structure.
Results
This work was an attempt at explaining the structural variation by considering the contribution of the environment to protein structuring. The application of the fuzzy oil drop (FOD) model to assess the validity of the selected models provided in the CASP13, CASP14 and CASP15 projects reveals the need for an environmental factor to determine the 3D structure of proteins. Consideration of the external force field in the form of polar water (Fuzzy Oil Drop) and a version modified by the presence of the hydrophobic compounds, FOD-M (FOD-Modified) reveals that the protein folding process is environmentally dependent. An analysis of selected models from the CASP competitions indicates the need for structure prediction as dependent on the consideration of the protein folding environment.
Conclusions
The conditions governed by the environment direct the protein folding process occurring in a certain environment. Therefore, the variation of the external force field should be taken into account in the models used in protein structure prediction."
68,"ILIAD
: a suite of automated Snakemake workflows for processing genomic data for downstream applications","Background
Processing raw genomic data for downstream applications such as imputation, association studies, and modeling requires numerous third-party bioinformatics software tools. It is highly time-consuming and resource-intensive with computational demands and storage limitations that pose significant challenges that increase cost. The use of software tools independent of one another, in a disjointed stepwise fashion, increases the difficulty and sets forth higher error rates because of fragmented job executions in alignment, variant calling, and/or build conversion complications. As sequencing data availability grows, the ability for biologists to process it using stable, automated, and reproducible workflows is paramount as it significantly reduces the time to generate clean and reliable data.
Results
The 
Iliad
 suite of genomic data workflows was developed to provide users with seamless file transitions from raw genomic data to a quality-controlled variant call format (VCF) file for downstream applications. 
Iliad
 benefits from the efficiency of the Snakemake best practices framework coupled with Singularity and Docker containers for repeatability, portability, and ease of installation. This feat is accomplished from the onset with download acquisitions of any raw data type (FASTQ, CRAM, IDAT) straight through to the generation of a clean merged data file that can combine any user-preferred datasets using robust programs such as BWA, Samtools, and BCFtools. Users can customize and direct their workflow with one straightforward configuration file. 
Iliad
 is compatible with Linux, MacOS, and Windows platforms and scalable from a local machine to a high-performance computing cluster.
Conclusion
Iliad
 offers automated workflows with optimized time and resource management that are comparable to other workflows available but generates analysis-ready VCF files from the most common datatypes using a single command. The storage footprint challenge of genomic data is overcome by utilizing temporary intermediate files before the final VCF is generated. This file is ready for use in imputation, genome-wide association study (GWAS) pipelines, high-throughput population genetics studies, select gene candidate studies, and more. 
Iliad
 was developed to be portable, compatible, scalable, robust, and repeatable with a simplistic setup, so biologists that are less familiar with programming can manage their own big data with this open-source suite of workflows."
69,A model-based clustering via mixture of hierarchical models with covariate adjustment for detecting differentially expressed genes from paired design,"The causes of many complex human diseases are still largely unknown. Genetics plays an important role in uncovering the molecular mechanisms of complex human diseases. A key step to characterize the genetics of a complex human disease is to unbiasedly identify disease-associated gene transcripts on a whole-genome scale. Confounding factors could cause false positives. Paired design, such as measuring gene expression before and after treatment for the same subject, can reduce the effect of known confounding factors. However, not all known confounding factors can be controlled in a paired/match design. Model-based clustering, such as mixtures of hierarchical models, has been proposed to detect gene transcripts differentially expressed between paired samples. To the best of our knowledge, no model-based gene clustering methods have the capacity to adjust for the effects of covariates yet. In this article, we proposed a novel mixture of hierarchical models with covariate adjustment in identifying differentially expressed transcripts using high-throughput whole-genome data from paired design. Both simulation study and real data analysis show the good performance of the proposed method."
70,Comparative Study of Single-stranded Oligonucleotides Secondary Structure Prediction Tools,"Background
Single-stranded nucleic acids (ssNAs) have important biological roles and a high biotechnological potential linked to their ability to bind to numerous molecular targets. This depends on the different spatial conformations they can assume. The first level of ssNAs spatial organisation corresponds to their base pairs pattern, i.e. their secondary structure. Many computational tools have been developed to predict the ssNAs secondary structures, making the choice of the appropriate tool difficult, and an up-to-date guide on the limits and applicability of current secondary structure prediction tools is missing. Therefore, we performed a comparative study of the performances of 9 freely available tools (mfold, RNAfold, CentroidFold, CONTRAfold, MC-Fold, LinearFold, UFold, SPOT-RNA, and MXfold2) on a dataset of 538 ssNAs with known experimental secondary structure.
Results
The minimum free energy-based tools, namely mfold and RNAfold, and some tools based on artificial intelligence, namely CONTRAfold and MXfold2, provided the best results, with 
\(\sim 50\%\)
 of exact predictions, whilst MC-fold seemed to be the worst performing tool, with only 
\(\sim 11\%\)
 of exact predictions. In addition, UFold and SPOT-RNA are the only options for pseudoknots prediction. Including in the analysis of mfold and RNAfold results 5–10 suboptimal solutions further improved the performances of these tools. Nevertheless, we could observe issues in predicting particular motifs, such as multiple-ways junctions and mini-dumbbells, or the ssNAs whose structure has been determined in complex with a protein. In addition, our benchmark shows that some effort has to be paid for ssDNA secondary structure predictions.
Conclusions
In general, Mfold, RNAfold, and MXfold2 seem to currently be the best choice for the ssNAs secondary structure prediction, although they still show some limits linked to specific structural motifs. Nevertheless, actual trends suggest that artificial intelligence has a high potential to overcome these remaining issues, for example the recently developed UFold and SPOT-RNA have a high success rate in predicting pseudoknots."
71,"Fast alignment of mass spectra in large proteomics datasets, capturing dissimilarities arising from multiple complex modifications of peptides","Background
In proteomics, the interpretation of mass spectra representing peptides carrying multiple complex modifications remains challenging, as it is difficult to strike a balance between reasonable execution time, a limited number of false positives, and a huge search space allowing any number of modifications without a priori. The scientific community needs new developments in this area to aid in the discovery of novel post-translational modifications that may play important roles in disease.
Results
To make progress on this issue, we implemented SpecGlobX (SpecGlob eXTended to eXperimental spectra), a standalone Java application that quickly determines the best spectral alignments of a (possibly very large) list of Peptide-to-Spectrum Matches (PSMs) provided by any open modification search method, or generated by the user. As input, SpecGlobX reads a file containing spectra in MGF or mzML format and a semicolon-delimited spreadsheet describing the PSMs. SpecGlobX returns the best alignment for each PSM as output, splitting the mass difference between the spectrum and the peptide into one or more shifts while considering the possibility of non-aligned masses (a phenomenon resulting from many situations including neutral losses). SpecGlobX is fast, able to align one million PSMs in about 1.5 min on a standard desktop. Firstly, we remind the foundations of the algorithm and detail how we adapted SpecGlob (the method we previously developed following the same aim, but limited to the interpretation of perfect simulated spectra) to the interpretation of imperfect experimental spectra. Then, we highlight the interest of SpecGlobX as a complementary tool downstream to three open modification search methods on a large simulated spectra dataset. Finally, we ran SpecGlobX on a proteome-wide dataset downloaded from PRIDE to demonstrate that SpecGlobX functions just as well on simulated and experimental spectra. We then carefully analyzed a limited set of interpretations.
Conclusions
SpecGlobX is helpful as a decision support tool, providing keys to interpret peptides carrying complex modifications still poorly considered by current open modification search software. Better alignment of PSMs enhances confidence in the identification of spectra provided by open modification search methods and should improve the interpretation rate of spectra."
72,Sctensor detects many-to-many cell–cell interactions from single cell RNA-sequencing data,"Background
Complex biological systems are described as a multitude of cell–cell interactions (CCIs). Recent single-cell RNA-sequencing studies focus on CCIs based on ligand–receptor (L–R) gene co-expression but the analytical methods are not appropriate to detect many-to-many CCIs. 
Results
In this work, we propose 
scTensor
, a novel method for extracting representative triadic relationships (or hypergraphs), which include ligand-expression, receptor-expression, and related L–R pairs.
Conclusions
Through extensive studies with simulated and empirical datasets, we have shown that 
scTensor
 can detect some hypergraphs that cannot be detected using conventional CCI detection methods, especially when they include many-to-many relationships. 
scTensor
 is implemented as a freely available R/Bioconductor package."
73,Hybrid deep learning approach to improve classification of low-volume high-dimensional data,"Background
The performance of machine learning classification methods relies heavily on the choice of features. In many domains, feature generation can be labor-intensive and require domain knowledge, and feature selection methods do not scale well in high-dimensional datasets. Deep learning has shown success in feature generation but requires large datasets to achieve high classification accuracy. Biology domains typically exhibit these challenges with numerous handcrafted features (high-dimensional) and small amounts of training data (low volume).
Method
A hybrid learning approach is proposed that first trains a deep network on the training data, extracts features from the deep network, and then uses these features to re-express the data for input to a non-deep learning method, which is trained to perform the final classification.
Results
The approach is systematically evaluated to determine the best layer of the deep learning network from which to extract features and the threshold on training data volume that prefers this approach. Results from several domains show that this hybrid approach outperforms standalone deep and non-deep learning methods, especially on low-volume, high-dimensional datasets. The diverse collection of datasets further supports the robustness of the approach across different domains.
Conclusions
The hybrid approach combines the strengths of deep and non-deep learning paradigms to achieve high performance on high-dimensional, low volume learning tasks that are typical in biology domains."
74,Ab initio protein structure prediction: the necessary presence of external force field as it is delivered by Hsp40 chaperone,"Background
The aqueous environment directs the protein folding process towards the generation of micelle-type structures, which results in the exposure of hydrophilic residues on the surface (polarity) and the concentration of hydrophobic residues in the center (hydrophobic core). Obtaining a structure without a hydrophobic core requires a different type of external force field than those generated by a water. The examples are membrane proteins, where the distribution of hydrophobicity is opposite to that of water-soluble proteins. Apart from these two extreme examples, the process of protein folding can be directed by chaperones, resulting in a structure devoid of a hydrophobic core.
Results
The current work presents such example: DnaJ Hsp40 in complex with alkaline phosphatase PhoA-U (PDB ID—6PSI)—the client molecule. The availability of WT form of the folding protein—alkaline phosphatase (PDB ID—1EW8) enables a comparative analysis of the structures: at the stage of interaction with the chaperone and the final, folded structure of this biologically active protein. The fuzzy oil drop model in its modified FOD-M version was used in this analysis, taking into account the influence of an external force field, in this case coming from a chaperone.
Conclusions
The FOD-M model identifies the external force field introduced by chaperon influencing the folding proces. The identified specific external force field can be applied in Ab Initio protein structure prediction as the environmental conditioning the folding proces."
75,ScLSTM: single-cell type detection by siamese recurrent network and hierarchical clustering,"Motivation
Categorizing cells into distinct types can shed light on biological tissue functions and interactions, and uncover specific mechanisms under pathological conditions. Since gene expression throughout a population of cells is averaged out by conventional sequencing techniques, it is challenging to distinguish between different cell types. The accumulation of single-cell RNA sequencing (scRNA-seq) data provides the foundation for a more precise classification of cell types. It is crucial building a high-accuracy clustering approach to categorize cell types since the imbalance of cell types and differences in the distribution of scRNA-seq data affect single-cell clustering and visualization outcomes.
Result
To achieve single-cell type detection, we propose a meta-learning-based single-cell clustering model called ScLSTM. Specifically, ScLSTM transforms the single-cell type detection problem into a hierarchical classification problem based on feature extraction by the siamese long-short term memory (LSTM) network. The similarity matrix derived from the improved sigmoid kernel is mapped to the siamese LSTM feature space to analyze the differences between cells. ScLSTM demonstrated superior classification performance on 8 scRNA-seq data sets of different platforms, species, and tissues. Further quantitative analysis and visualization of the human breast cancer data set validated the superiority and capability of ScLSTM in recognizing cell types."
76,Multilayer network alignment based on topological assessment via embeddings,"Background
Network graphs allow modelling the real world objects in terms of interactions. In a multilayer network, the interactions are distributed over layers (i.e., intralayer and interlayer edges). Network alignment (NA) is a methodology that allows mapping nodes between two or multiple given networks, by preserving topologically similar regions. For instance, NA can be applied to transfer knowledge from one biological species to another. In this paper, we present 
DANTEml
, a software tool for the Pairwise Global NA (PGNA) of multilayer networks, based on topological assessment. It builds its own similarity matrix by processing the node embeddings computed from two multilayer networks of interest, to evaluate their topological similarities. The proposed solution can be used via a user-friendly command line interface, also having a built-in guided mode (step-by-step) for defining input parameters.
Results
We investigated the performance of 
DANTEml
 based on (i) performance evaluation on synthetic multilayer networks, (ii) statistical assessment of the resulting alignments, and (iii) alignment of real multilayer networks. 
DANTEml
 over performed a method that does not consider the distribution of nodes and edges over multiple layers by 1193.62%, and a method for temporal NA by 25.88%; we also performed the statistical assessment, which corroborates the significance of its own node mappings. In addition, we tested the proposed solution by using a real multilayer network in presence of several levels of noise, in accordance with the same outcome pursued for the NA on our dataset of synthetic networks. In this case, the improvement is even more evident: +4008.75% and +111.72%, compared to a method that does not consider the distribution of nodes and edges over multiple layers and a method for temporal NA, respectively.
Conclusions
DANTEml
 is a software tool for the PGNA of multilayer networks based on topological assessment, that is able to provide effective alignments both on synthetic and real multi layer networks, of which node mappings can be validated statistically. Our experimentation reported a high degree of reliability and effectiveness for the proposed solution."
77,Estimating microhaplotype allele frequencies from low-coverage or pooled sequencing data,"Background
Microhaplotypes have the potential to be more cost-effective than SNPs for applications that require genetic panels of highly variable loci. However, development of microhaplotype panels is hindered by a lack of methods for estimating microhaplotype allele frequency from low-coverage whole genome sequencing or pooled sequencing (pool-seq) data.
Results
We developed new methods for estimating microhaplotype allele frequency from low-coverage whole genome sequence and pool-seq data. We validated these methods using datasets from three non-model organisms. These methods allowed estimation of allele frequency and expected heterozygosity at depths routinely achieved from pooled sequencing.
Conclusions
These new methods will allow microhaplotype panels to be designed using low-coverage WGS and pool-seq data to discover and evaluate candidate loci. The python script implementing the two methods and documentation are available at 
https://www.github.com/delomast/mhFromLowDepSeq
."
78,Predicting active enhancers with DNA methylation and histone modification,"Background
Enhancers play a crucial role in gene regulation, and some active enhancers produce noncoding RNAs known as enhancer RNAs (eRNAs) bi-directionally. The most commonly used method for detecting eRNAs is CAGE-seq, but the instability of eRNAs in vivo leads to data noise in sequencing results. Unfortunately, there is currently a lack of research focused on the noise inherent in CAGE-seq data, and few approaches have been developed for predicting eRNAs. Bridging this gap and developing widely applicable eRNA prediction models is of utmost importance.
Results
In this study, we proposed a method to reduce false positives in the identification of eRNAs by adjusting the statistical distribution of expression levels. We also developed eRNA prediction models using joint gene expressions, DNA methylation, and histone modification. These models achieved impressive performance with an AUC value of approximately 0.95 for intra-cell prediction and 0.9 for cross-cell prediction.
Conclusions
Our method effectively attenuates the noise generated by stochastic RNA production, resulting in more accurate detection of eRNAs. Furthermore, our eRNA prediction model exhibited significant accuracy in both intra-cell and cross-cell validation, highlighting its robustness and potential application in various cellular contexts."
79,Deep learning-enabled natural language processing to identify directional pharmacokinetic drug–drug interactions,"Background
During drug development, it is essential to gather information about the change of clinical exposure of a drug (object) due to the pharmacokinetic (PK) drug-drug interactions (DDIs) with another drug (precipitant). While many natural language processing (NLP) methods for DDI have been published, most were designed to evaluate if (and what kind of) DDI relationships exist in the text, without identifying the direction of DDI (object vs. precipitant drug). Here we present a method for the automatic identification of the directionality of a PK DDI from literature or drug labels.
Methods
We reannotated the Text Analysis Conference (TAC) DDI track 2019 corpus for identifying the direction of a PK DDI and evaluated the performance of a fine-tuned BioBERT model on this task by following the training and validation steps prespecified by TAC.
Results
This initial attempt showed the model achieved an F-score of 0.82 in identifying sentences as containing PK DDI and an F-score of 0.97 in identifying object versus precipitant drugs in those sentences.
Discussion and conclusion
Despite a growing list of NLP methods for DDI extraction, most of them use a common set of corpora to perform general purpose tasks (e.g., classifying a sentence into one of several fixed DDI categories). There is a lack of coordination between the drug development and biomedical informatics method development community to develop corpora and methods to perform specific tasks (e.g., extract clinical exposure changes due to PK DDI). We hope that our effort can encourage such a coordination so that more “fit for purpose” NLP methods could be developed and used to facilitate the drug development process."
80,Serial KinderMiner (SKiM) discovers and annotates biomedical knowledge using co-occurrence and transformer models,"Background
The PubMed archive contains more than 34 million articles; consequently, it is becoming increasingly difficult for a biomedical researcher to keep up-to-date with different knowledge domains. Computationally efficient and interpretable tools are needed to help researchers find and understand associations between biomedical concepts. The goal of literature-based discovery (LBD) is to connect concepts in isolated literature domains that would normally go undiscovered. This usually takes the form of an A–B–C relationship, where A and C terms are linked through a B term intermediate. Here we describe Serial KinderMiner (SKiM), an LBD algorithm for finding statistically significant links between an A term and one or more C terms through some B term intermediate(s). The development of SKiM is motivated by the observation that there are only a few LBD tools that provide a functional web interface, and that the available tools are limited in one or more of the following ways: (1) they identify a relationship but not the type of relationship, (2) they do not allow the user to provide their own lists of B or C terms, hindering flexibility, (3) they do not allow for querying thousands of C terms (which is crucial if, for instance, the user wants to query connections between a disease and the thousands of available drugs), or (4) they are specific for a particular biomedical domain (such as cancer). We provide an open-source tool and web interface that improves on all of these issues.
Results
We demonstrate SKiM’s ability to discover useful A–B–C linkages in three control experiments: classic LBD discoveries, drug repurposing, and finding associations related to cancer. Furthermore, we supplement SKiM with a knowledge graph built with transformer machine-learning models to aid in interpreting the relationships between terms found by SKiM. Finally, we provide a simple and intuitive open-source web interface (
https://skim.morgridge.org
) with comprehensive lists of drugs, diseases, phenotypes, and symptoms so that anyone can easily perform SKiM searches.
Conclusions
SKiM is a simple algorithm that can perform LBD searches to discover relationships between arbitrary user-defined concepts. SKiM is generalized for any domain, can perform searches with many thousands of C term concepts, and moves beyond the simple identification of an existence of a relationship; many relationships are given relationship type labels from our knowledge graph."
81,Structure-informed clustering for population stratification in association studies,"Background
Identifying variants associated with complex traits is a challenging task in genetic association studies due to linkage disequilibrium (LD) between genetic variants and population stratification, unrelated to the disease risk. Existing methods of population structure correction use principal component analysis or linear mixed models with a random effect when modeling associations between a trait of interest and genetic markers. However, due to stringent significance thresholds and latent interactions between the markers, these methods often fail to detect genuinely associated variants.
Results
To overcome this, we propose CluStrat, which corrects for complex arbitrarily structured populations while leveraging the linkage disequilibrium induced distances between genetic markers. It performs an agglomerative hierarchical clustering using the Mahalanobis distance covariance matrix of the markers. In simulation studies, we show that our method outperforms existing methods in detecting true causal variants. Applying CluStrat on WTCCC2 and UK Biobank cohorts, we found biologically relevant associations in Schizophrenia and Myocardial Infarction. CluStrat was also able to correct for population structure in polygenic adaptation of height in Europeans.
Conclusions
CluStrat highlights the advantages of biologically relevant distance metrics, such as the Mahalanobis distance, which captures the cryptic interactions within populations in the presence of LD better than the Euclidean distance."
82,LncRNA–protein interaction prediction with reweighted feature selection,"LncRNA–protein interactions are ubiquitous in organisms and play a crucial role in a variety of biological processes and complex diseases. Many computational methods have been reported for lncRNA–protein interaction prediction. However, the experimental techniques to detect lncRNA–protein interactions are laborious and time-consuming. Therefore, to address this challenge, this paper proposes a reweighting boosting feature selection (RBFS) method model to select key features. Specially, a reweighted apporach can adjust the contribution of each observational samples to learning model fitting; let higher weights are given more influence samples than those with lower weights. Feature selection with boosting can efficiently rank to iterate over important features to obtain the optimal feature subset. Besides, in the experiments, the RBFS method is applied to the prediction of lncRNA–protein interactions. The experimental results demonstrate that our method achieves higher accuracy and less redundancy with fewer features."
83,Raman spectroscopy-based prediction of ofloxacin concentration in solution using a novel loss function and an improved GA-CNN model,"Background
A Raman spectroscopy method can quickly and accurately measure the concentration of ofloxacin in solution. This method has the advantages of accuracy and rapidity over traditional detection methods. However, the manual analysis methods for the collected Raman spectral data often ignore the nonlinear characteristics of the data and cannot accurately predict the concentration of the target sample.
Methods
To address this drawback, this paper proposes a novel kernel-Huber loss function that combines the Huber loss function with the Gaussian kernel function. This function is used with an improved genetic algorithm-convolutional neural network (GA-CNN) to model and predict the Raman spectral data of different concentrations of ofloxacin in solution. In addition, the paper introduces recurrent neural networks (RNN), long short-term memory (LSTM), bidirectional long short-term memory (BiLSTM) and gated recurrent units (GRU) models to conduct multiple experiments and use root mean square error (RMSE) and residual predictive deviation (RPD) as evaluation metrics.
Results
The proposed method achieved an 
\(R^2\)
 of 0.9989 on the test set data and improved by 3% over the traditional CNN. Multiple experiments were also conducted using RNN, LSTM, BiLSTM, and GRU models and evaluated their performance using RMSE, RPD, and other metrics. The results showed that the proposed method consistently outperformed these models.
Conclusions
This paper demonstrates the effectiveness of the proposed method for predicting the concentration of ofloxacin in solution based on Raman spectral data, in addition to discussing the advantages and limitations of the proposed method, and the study proposes a solution to the problem of deep learning methods for Raman spectral concentration prediction."
84,Roastgsa: a comparison of rotation-based scores for gene set enrichment analysis,"Background
Gene-wise differential expression is usually the first major step in the statistical analysis of high-throughput data obtained from techniques such as microarrays or RNA-sequencing. The analysis at gene level is often complemented by interrogating the data in a broader biological context that considers as unit of measure groups of genes that may have a common function or biological trait. Among the vast number of publications about gene set analysis (GSA), the rotation test for gene set analysis, also referred to as roast, is a general sample randomization approach that maintains the integrity of the intra-gene set correlation structure in defining the null distribution of the test.
Results
We present 
roastgsa
, an R package that contains several enrichment score functions that feed the roast algorithm for hypothesis testing. These implemented methods are evaluated using both simulated and benchmarking data in microarray and RNA-seq datasets. We find that computationally intensive measures based on Kolmogorov-Smirnov (KS) statistics fail to improve the rates of simpler measures of GSA like mean and maxmean scores. We also show the importance of accounting for the gene linear dependence structure of the testing set, which is linked to the loss of effective signature size. Complete graphical representation of the results, including an approximation for the effective signature size, can be obtained as part of the 
roastgsa
 output.
Conclusions
We encourage the usage of the absmean (non-directional), mean (directional) and maxmean (directional) scores for roast GSA analysis as these are simple measures of enrichment that have presented dominant results in all provided analyses in comparison to the more complex KS measures."
85,Biomarker detection using corrected degree of domesticity in hybrid social network feature selection for improving classifier performance,"Background
Dimension reduction, especially feature selection, is an important step in improving classification performance for high-dimensional data. Particularly in cancer research, when reducing the number of features, i.e., genes, it is important to select the most informative features/potential biomarkers that could affect the diagnostic accuracy. Therefore, researchers continuously try to explore more efficient ways to reduce the large number of features/genes to a small but informative subset before the classification task. Hybrid methods have been extensively investigated for this purpose, and research to find the optimal approach is ongoing. Social network analysis is used as a part of a hybrid method, although there are several issues that have arisen when using social network tools, such as using a single environment for computing, constructing an adjacency matrix or computing network measures. Therefore, in our study, we apply a hybrid feature selection method consisting of several machine learning algorithms in addition to social network analysis with our proposed network metric, called the corrected degree of domesticity, in a single environment, R, to improve the support vector machine classifier’s performance. In addition, we evaluate and compare the performances of several combinations used in the different steps of the method with a simulation experiment.
Results
The proposed method improves the classifier’s performance compared to using the whole feature set in all the cases we investigate. Additionally, in terms of the area under the receiver operating characteristic (ROC) curve, our approach improves classification performance compared to several approaches in the literature.
Conclusion
When using the corrected degree of domesticity as a network degree centrality measure, it is important to use our correction to compare nodes/features with no connection outside of their community since it provides a more accurate ranking among the features. Due to the nature of the hybrid method, which includes social network analysis, it is necessary to investigate possible combinations to provide an optimal solution for the microarray data used in the research."
86,Tri-model classifiers for EEG based mental task classification: hybrid optimization assisted framework,"The commercial adoption of BCI technologies for both clinical and non-clinical applications is drawing scientists to the creation of wearable devices for daily living. Emotions are essential to human existence and have a significant impact on thinking. Emotion is frequently linked to rational decision-making, perception, interpersonal interaction, and even basic human intellect. The requirement for trustworthy and implementable methods for the detection of individual emotional responses is needed with rising attention of the scientific community towards the establishment of some significant emotional connections among people and computers. This work introduces EEG recognition model, where the input signal is pre-processed using band pass filter. Then, the features like discrete wavelet transform (DWT), band power, spectral flatness, and improved Entropy are extracted. Further, for recognition, tri-classifiers like long short term memory (LSTM), improved deep belief network (DBN) and recurrent neural network (RNN) are used. Also to enhance tri-model classifier performance, the weights of LSTM, improved DBN, and RNN are tuned by model named as shark smell updated BES optimization (SSU-BES). Finally, the perfection of SSU-BES is demonstrated over diverse metrics."
87,Extracting cancer concepts from clinical notes using natural language processing: a systematic review,"Background
Extracting information from free texts using natural language processing (NLP) can save time and reduce the hassle of manually extracting large quantities of data from incredibly complex clinical notes of cancer patients. This study aimed to systematically review studies that used NLP methods to identify cancer concepts from clinical notes automatically.
Methods
PubMed, Scopus, Web of Science, and Embase were searched for English language papers using a combination of the terms concerning “Cancer”, “NLP”, “Coding”, and “Registries” until June 29, 2021. Two reviewers independently assessed the eligibility of papers for inclusion in the review.
Results
Most of the software programs used for concept extraction reported were developed by the researchers (
n
 = 7). Rule-based algorithms were the most frequently used algorithms for developing these programs. In most articles, the criteria of accuracy (
n
 = 14) and sensitivity (
n
 = 12) were used to evaluate the algorithms. In addition, Systematized Nomenclature of Medicine-Clinical Terms (SNOMED-CT) and Unified Medical Language System (UMLS) were the most commonly used terminologies to identify concepts. Most studies focused on breast cancer (
n
 = 4, 19%) and lung cancer (
n
 = 4, 19%).
Conclusion
The use of NLP for extracting the concepts and symptoms of cancer has increased in recent years. The rule-based algorithms are well-liked algorithms by developers. Due to these algorithms' high accuracy and sensitivity in identifying and extracting cancer concepts, we suggested that future studies use these algorithms to extract the concepts of other diseases as well."
88,Picky with peakpicking: assessing chromatographic peak quality with simple metrics in metabolomics,"Background
Chromatographic peakpicking continues to represent a significant bottleneck in automated LC–MS workflows. Uncontrolled false discovery rates and the lack of manually-calibrated quality metrics require researchers to visually evaluate individual peaks, requiring large amounts of time and breaking replicability. This problem is exacerbated in noisy environmental datasets and for novel separation methods such as hydrophilic interaction columns in metabolomics, creating a demand for a simple, intuitive, and robust metric of peak quality.
Results
Here, we manually labeled four HILIC oceanographic particulate metabolite datasets to assess the performance of individual peak quality metrics. We used these datasets to construct a predictive model calibrated to the likelihood that visual inspection by an MS expert would include a given mass feature in the downstream analysis. We implemented two novel peak quality metrics, a custom signal-to-noise metric and a test of similarity to a bell curve, both calculated from the raw data in the extracted ion chromatogram, and found that these outperformed existing measurements of peak quality. A simple logistic regression model built on two metrics reduced the fraction of false positives in the analysis from 70–80% down to 1–5% and showed minimal overfitting when applied to novel datasets. We then explored the implications of this quality thresholding on the conclusions obtained by the downstream analysis and found that while only 10% of the variance in the dataset could be explained by depth in the default output from the peakpicker, approximately 40% of the variance was explained when restricted to high-quality peaks alone.
Conclusions
We conclude that the poor performance of peakpicking algorithms significantly reduces the power of both univariate and multivariate statistical analyses to detect environmental differences. We demonstrate that simple models built on intuitive metrics and derived from the raw data are more robust and can outperform more complex models when applied to new data. Finally, we show that in properly curated datasets, depth is a major driver of variability in the marine microbial metabolome and identify several interesting metabolite trends for future investigation."
89,SeQual-Stream: approaching stream processing to quality control of NGS datasets,"Background
Quality control of DNA sequences is an important data preprocessing step in many genomic analyses. However, all existing parallel tools for this purpose are based on a batch processing model, needing to have the complete genetic dataset before processing can even begin. This limitation clearly hinders quality control performance in those scenarios where the dataset must be downloaded from a remote repository and/or copied to a distributed file system for its parallel processing.
Results
In this paper we present SeQual-Stream, a streaming tool that allows performing multiple quality control operations on genomic datasets in a fast, distributed and scalable way. To do so, our approach relies on the Apache Spark framework and the Hadoop Distributed File System (HDFS) to fully exploit the stream paradigm and accelerate the preprocessing of large datasets as they are being downloaded and/or copied to HDFS. The experimental results have shown significant improvements in the execution times of SeQual-Stream when compared to a batch processing tool with similar quality control features, providing a maximum speedup of 2.7
\(\times\)
 when processing a dataset with more than 250 million DNA sequences, while also demonstrating good scalability features.
Conclusion
Our solution provides a more scalable and higher performance way to carry out quality control of large genomic datasets by taking advantage of stream processing features. The tool is distributed as free open-source software released under the GNU AGPLv3 license and is publicly available to download at 
https://github.com/UDC-GAC/SeQual-Stream
."
90,deltaXpress (ΔXpress): a tool for mapping differentially correlated genes using single-cell qPCR data,"Background
High-throughput experiments provide deep insight into the molecular biology of different species, but more tools need to be developed to handle this type of data. At the transcriptomics level, quantitative Polymerase Chain Reaction technology (qPCR) can be affordably adapted to produce high-throughput results through a single-cell approach. In addition to comparative expression profiles between groups, single-cell approaches allow us to evaluate and propose new dependency relationships among markers. However, this alternative has not been explored before for large-scale qPCR-based experiments.
Results
Herein, we present deltaXpress (ΔXpress), a web app for analyzing data from single-cell qPCR experiments using a combination of HTML and R programming languages in a friendly environment. This application uses cycle threshold (Ct) values and categorical information for each sample as input, allowing the best pair of housekeeping genes to be chosen to normalize the expression of target genes. ΔXpress emulates a bulk analysis by observing differentially expressed genes, but in addition, it allows the discovery of pairwise genes differentially correlated when comparing two experimental conditions. Researchers can download normalized data or use subsequent modules to map differentially correlated genes, perform conventional comparisons between experimental groups, obtain additional information about their genes (gene glossary), and generate ready-to-publication images (600 dots per inch).
Conclusions
ΔXpress web app is freely available to non-commercial users at 
https://alexismurillo.shinyapps.io/dXpress/
 and can be used for different experiments in all technologies involving qPCR with at least one housekeeping region."
91,"Deep learning, radiomics and radiogenomics applications in the digital breast tomosynthesis: a systematic review","Background
Recent advancements in computing power and state-of-the-art algorithms have helped in more accessible and accurate diagnosis of numerous diseases. In addition, the development of de novo areas in imaging science, such as radiomics and radiogenomics, have been adding more to personalize healthcare to stratify patients better. These techniques associate imaging phenotypes with the related disease genes. Various imaging modalities have been used for years to diagnose breast cancer. Nonetheless, digital breast tomosynthesis (DBT), a state-of-the-art technique, has produced promising results comparatively. DBT, a 3D mammography, is replacing conventional 2D mammography rapidly. This technological advancement is key to AI algorithms for accurately interpreting medical images.
Objective and methods
This paper presents a comprehensive review of deep learning (DL), radiomics and radiogenomics in breast image analysis. This review focuses on DBT, its extracted synthetic mammography (SM), and full-field digital mammography (FFDM). Furthermore, this survey provides systematic knowledge about DL, radiomics, and radiogenomics for beginners and advanced-level researchers.
Results
A total of 500 articles were identified, with 30 studies included as the set criteria. Parallel benchmarking of radiomics, radiogenomics, and DL models applied to the DBT images could allow clinicians and researchers alike to have greater awareness as they consider clinical deployment or development of new models. This review provides a comprehensive guide to understanding the current state of early breast cancer detection using DBT images.
Conclusion
Using this survey, investigators with various backgrounds can easily seek interdisciplinary science and new DL, radiomics, and radiogenomics directions towards DBT."
92,Pan-genome de Bruijn graph using the bidirectional FM-index,"Background
Pan-genome graphs are gaining importance in the field of bioinformatics as data structures to represent and jointly analyze multiple genomes. Compacted de Bruijn graphs are inherently suited for this purpose, as their graph topology naturally reveals similarity and divergence within the pan-genome. Most state-of-the-art pan-genome graphs are represented explicitly in terms of nodes and edges. Recently, an alternative, implicit graph representation was proposed that builds directly upon the unidirectional FM-index. As such, a memory-efficient graph data structure is obtained that inherits the FM-index’ backward search functionality. However, this representation suffers from a number of shortcomings in terms of functionality and algorithmic performance.
Results
We present a data structure for a pan-genome, compacted de Bruijn graph that aims to address these shortcomings. It is built on the bidirectional FM-index, extending the ability of its unidirectional counterpart to navigate and search the graph in both directions. All basic graph navigation steps can be performed in constant time. Based on these features, we implement subgraph visualization as well as lossless approximate pattern matching to the graph using search schemes. We demonstrate that we can retrieve all occurrences corresponding to a read within a certain edit distance in a very efficient manner. Through a case study, we show the potential of exploiting the information embedded in the graph’s topology through visualization and sequence alignment.
Conclusions
We propose a memory-efficient representation of the pan-genome graph that supports subgraph visualization and lossless approximate pattern matching of reads against the graph using search schemes. The C++ source code of our software, called Nexus, is available at 
https://github.com/biointec/nexus
 under AGPL-3.0 license."
93,Regularized multi-trait multi-locus linear mixed models for genome-wide association studies and genomic selection in crops,"Background
We consider two key problems in genomics involving multiple traits: multi-trait genome wide association studies (GWAS), where the goal is to detect genetic variants associated with the traits; and multi-trait genomic selection (GS), where the emphasis is on accurately predicting trait values. Multi-trait linear mixed models build on the linear mixed model to jointly model multiple traits. Existing estimation methods, however, are limited to the joint analysis of a small number of genotypes; in fact, most approaches consider one SNP at a time. Estimating multi-dimensional genetic and environment effects also results in considerable computational burden. Efficient approaches that incorporate regularization into multi-trait linear models (no random effects) have been recently proposed to identify genomic loci associated with multiple traits (Yu et al. in Multitask learning using task clustering with applications to predictive modeling and GWAS of plant varieties. 
arXiv:1710.01788
, 2017; Yu et al in Front Big Data 2:27, 2019), but these ignore population structure and familial relatedness (Yu et al in Nat Genet 38:203–208, 2006).
Results
This work addresses this gap by proposing a novel class of regularized multi-trait linear 
mixed
 models along with scalable approaches for estimation in the presence of high-dimensional genotypes and a large number of traits. We evaluate the effectiveness of the proposed methods using datasets in maize and sorghum diversity panels, and demonstrate benefits in both achieving high prediction accuracy in GS and in identifying relevant marker-trait associations.
Conclusions
The proposed regularized multivariate linear mixed models are relevant for both GWAS and GS. We hope that they will facilitate agronomy-related research in plant biology and crop breeding endeavors."
94,TreeKernel: interpretable kernel machine tests for interactions between -omics and clinical predictors with applications to metabolomics and COPD phenotypes,"Background
In this paper, we are interested in interactions between a high-dimensional -omics dataset and clinical covariates. The goal is to evaluate the relationship between a phenotype of interest and a high-dimensional omics pathway, where the effect of the omics data depends on subjects’ clinical covariates (age, sex, smoking status, etc.). For instance, metabolic pathways can vary greatly between sexes which may also change the relationship between certain metabolic pathways and a clinical phenotype of interest. We propose partitioning the clinical covariate space and performing a kernel association test within those partitions. To illustrate this idea, we focus on hierarchical partitions of the clinical covariate space and kernel tests on metabolic pathways.
Results
We see that our proposed method outperforms competing methods in most simulation scenarios. It can identify different relationships among clinical groups with higher power in most scenarios while maintaining a proper Type I error rate. The simulation studies also show a robustness to the grouping structure within the clinical space. We also apply the method to the COPDGene study and find several clinically meaningful interactions between metabolic pathways, the clinical space, and lung function.
Conclusion
TreeKernel provides a simple and interpretable process for testing for relationships between high-dimensional omics data and clinical outcomes in the presence of interactions within clinical cohorts. The method is broadly applicable to many studies."
95,"EMDL_m6Am: identifying N6,2′-O-dimethyladenosine sites based on stacking ensemble deep learning","Background
N6, 2'-O-dimethyladenosine (m
6
Am) is an abundant RNA methylation modification on vertebrate mRNAs and is present in the transcription initiation region of mRNAs. It has recently been experimentally shown to be associated with several human disorders, including obesity genes, and stomach cancer, among others. As a result, N6,2′-O-dimethyladenosine (m
6
Am) site will play a crucial part in the regulation of RNA if it can be correctly identified.
Results
This study proposes a novel deep learning-based m
6
Am prediction model, EMDL_m6Am, which employs one-hot encoding to expressthe feature map of the RNA sequence and recognizes m
6
Am sites by integrating different CNN models via stacking. Including DenseNet, Inflated Convolutional Network (DCNN) and Deep Multiscale Residual Network (MSRN), the sensitivity (Sn), specificity (Sp), accuracy (ACC), Mathews correlation coefficient (MCC) and area under the curve (AUC) of our model on the training data set reach 86.62%, 88.94%, 87.78%, 0.7590 and 0.8778, respectively, and the prediction results on the independent test set are as high as 82.25%, 79.72%, 80.98%, 0.6199, and 0.8211.
Conclusions
In conclusion, the experimental results demonstrated that EMDL_m6Am greatly improved the predictive performance of the m
6
Am sites and could provide a valuable reference for the next part of the study. The source code and experimental data are available at: 
https://github.com/13133989982/EMDL-m6Am
."
96,PerFSeeB: designing long high-weight single spaced seeds for full sensitivity alignment with a given number of mismatches,"Background
Technical progress in computational hardware allows researchers to use new approaches for sequence alignment problems. For a given sequence, we usually use smaller subsequences (anchors) to find possible candidate positions within a reference sequence. We may create pairs (“position”, “subsequence”) for the reference sequence and keep all such records without compression, even on a budget computer. As sequences for new and reference genomes differ, the goal is to find anchors, so we tolerate differences and keep the number of candidate positions with the same anchors to a minimum. Spaced seeds (masks ignoring symbols at specific locations) are a way to approach the task. An ideal (full sensitivity) spaced seed should enable us to find all such positions subject to a given maximum number of mismatches permitted.
Results
Several algorithms to assist seed generation are presented. The first one finds all permitted spaced seeds iteratively. We observe specific patterns for the seeds of the highest weight. There are often periodic seeds with a simple relation between block size, length of the seed and read. The second algorithm produces blocks for periodic seeds for blocks of up to 50 symbols and up to nine mismatches. The third algorithm uses those lists to find spaced seeds for reads of an arbitrary length. Finally, we apply seeds to a real dataset and compare results for other popular seeds.
Conclusions
PerFSeeB approach helps to significantly reduce the number of reads’ possible alignment positions for a known number of mismatches. Lists of long, high-weight spaced seeds are available in Additional file 
1
. The seeds are best in weight compared to seeds from other papers and can usually be applied to shorter reads. Codes for all algorithms and periodic blocks can be found at 
https://github.com/vtman/PerFSeeB
."
97,Identification of transcription factor high accumulation DNA zones,"Background
Transcription factors (TF) play a crucial role in the regulation of gene transcription; alterations of their activity and binding to DNA areas are strongly involved in cancer and other disease onset and development. For proper biomedical investigation, it is hence essential to correctly trace TF dense DNA areas, having multiple bindings of distinct factors, and select DNA high occupancy target (HOT) zones, showing the highest accumulation of such bindings. Indeed, systematic and replicable analysis of HOT zones in a large variety of cells and tissues would allow further understanding of their characteristics and could clarify their functional role.
Results
Here, we propose, thoroughly explain and discuss a full computational procedure to study in-depth DNA dense areas of transcription factor accumulation and identify HOT zones. This methodology, developed as a computationally efficient parametric algorithm implemented in an R/Bioconductor package, uses a systematic approach with two alternative methods to examine transcription factor bindings and provide comparative and fully-reproducible assessments. It offers different resolutions by introducing three distinct types of accumulation, which can analyze DNA from single-base to region-oriented levels, and a moving window, which can estimate the influence of the neighborhood for each DNA base under exam.
Conclusions
We quantitatively assessed the full procedure by using our implemented software package, named TFHAZ, in two example applications of biological interest, proving its full reliability and relevance."
98,Designs for the simultaneous inference of concentration–response curves,"Background
An important problem in toxicology in the context of gene expression data is the simultaneous inference of a large number of concentration–response relationships. The quality of the inference substantially depends on the choice of design of the experiments, in particular, on the set of different concentrations, at which observations are taken for the different genes under consideration. As this set has to be the same for all genes, the efficient planning of such experiments is very challenging. We address this problem by determining efficient designs for the simultaneous inference of a large number of concentration–response models. For that purpose, we both construct a 
D
-optimality criterion for simultaneous inference and a 
K
-means procedure which clusters the support points of the locally 
D
-optimal designs of the individual models.
Results
We show that a planning of experiments that addresses the simultaneous inference of a large number of concentration–response relationships yields a substantially more accurate statistical analysis. In particular, we compare the performance of the constructed designs to the ones of other commonly used designs in terms of 
D
-efficiencies and in terms of the quality of the resulting model fits using a real data example dealing with valproic acid. For the quality comparison we perform an extensive simulation study.
Conclusions
The design maximizing the 
D
-optimality criterion for simultaneous inference improves the inference of the different concentration–response relationships substantially. The design based on the 
K
-means procedure also performs well, whereas a log-equidistant design, which was also included in the analysis, performs poorly in terms of the quality of the simultaneous inference. Based on our findings, the 
D
-optimal design for simultaneous inference should be used for upcoming analyses dealing with high-dimensional gene expression data."
99,Artificial Intelligence based wrapper for high dimensional feature selection,"Background
Feature selection is important in high dimensional data analysis. The wrapper approach is one of the ways to perform feature selection, but it is computationally intensive as it builds and evaluates models of multiple subsets of features. The existing wrapper algorithm primarily focuses on shortening the path to find an optimal feature set. However, it underutilizes the capability of feature subset models, which impacts feature selection and its predictive performance.
Method and Results
This study proposes a novel Artificial Intelligence based Wrapper (AIWrap) algorithm that integrates Artificial Intelligence (AI) with the existing wrapper algorithm. The algorithm develops a Performance Prediction Model using AI which predicts the model performance of any feature set and allows the wrapper algorithm to evaluate the feature subset performance in a model without building the model. The algorithm can make the wrapper algorithm more relevant for high-dimensional data. We evaluate the performance of this algorithm using simulated studies and real research studies. AIWrap shows better or at par feature selection and model prediction performance than standard penalized feature selection algorithms and wrapper algorithms.
Conclusion
AIWrap approach provides an alternative algorithm to the existing algorithms for feature selection. The current study focuses on AIWrap application in continuous cross-sectional data. However, it could be applied to other datasets like longitudinal, categorical and time-to-event biological data."
100,Asterics: a simple tool for the ExploRation and Integration of omiCS data,"Background
The rapid development of omics acquisition techniques has induced the production of a large volume of heterogeneous and multi-level omics datasets, which require specific and sometimes complex analyses to obtain relevant biological information. Here, we present ASTERICS (version 2.5), a publicly available web interface for the analyses of omics datasets.
Results
ASTERICS is designed to make both standard and complex exploratory and integration analysis workflows easily available to biologists and to provide high quality interactive plots. Special care has been taken to provide a comprehensive documentation of the implemented analyses and to guide users toward sound analysis choices regarding some specific omics data. Data and analyses are organized in a comprehensive graphical workflow within ASTERICS workspace to facilitate the understanding of successive data editions and analyses leading to a given result.
Conclusion
ASTERICS provides an easy to use platform for omics data exploration and integration. The modular organization of its open source code makes it easy to incorporate new workflows and analyses by external contributors. ASTERICS is available at 
https://asterics.miat.inrae.fr
 and can also be deployed using provided docker images."
101,Advances and challenges in Bioinformatics and Biomedical Engineering: IWBBIO 2020,"This Supplement issue, presents five research articles which are distributed, mainly due to the subject they address, from the 8th International Work-Conference on Bioinformatics and Biomedical Engineering (IWBBIO 2020), which was held on line, during September, 30th–2nd October, 2020. These contributions have been chosen because of their quality and the importance of their findings. Those contributions were then invited to participate in this supplement for the following journals of BMC: BMC Bioinformatics and BMC Genomics. In the present Editorial in BMC journal, we summarize the contributions that provide a clear overview of the thematic areas covered by the IWBBIO conference, ranging from theoretical/review aspects to real-world applications of bioinformatic and biomedical engineering."
102,EasyCGTree: a pipeline for prokaryotic phylogenomic analysis based on core gene sets,"Background
Genome-scale phylogenetic analysis based on core gene sets is routinely used in microbiological research. However, the techniques are still not approachable for individuals with little bioinformatics experience. Here, we present EasyCGTree, a user-friendly and cross-platform pipeline to reconstruct genome-scale maximum-likehood (ML) phylogenetic tree using supermatrix (SM) and supertree (ST) approaches.
Results
EasyCGTree was implemented in Perl programming languages and was built using a collection of published reputable programs. All the programs were precompiled as standalone executable files and contained in the EasyCGTree package. It can run after installing Perl language environment. Several profile hidden Markov models (HMMs) of core gene sets were prepared in advance to construct a profile HMM database (PHD) that was enclosed in the package and available for homolog searching. Customized gene sets can also be used to build profile HMM and added to the PHD via EasyCGTree. Taking 43 genomes of the genus 
Paracoccus
 as the testing data set, consensus (a variant of the typical SM), SM, and ST trees were inferred via EasyCGTree successfully, and the SM trees were compared with those inferred via the pipelines UBCG and bcgTree, using the metrics of cophenetic correlation coefficients (CCC) and Robinson–Foulds distance (topological distance). The results suggested that EasyCGTree can infer SM trees with nearly identical topology (distance < 0.1) and accuracy (CCC > 0.99) to those of trees inferred with the two pipelines.
Conclusions
EasyCGTree is an all-in-one automatic pipeline from input data to phylogenomic tree with guaranteed accuracy, and is much easier to install and use than the reference pipelines. In addition, ST is implemented in EasyCGTree conveniently and can be used to explore prokaryotic evolutionary signals from a different perspective. The EasyCGTree version 4 is freely available for Linux and Windows users at Github (
https://github.com/zdf1987/EasyCGTree4
)."
103,lifex-ep: a robust and efficient software for cardiac electrophysiology simulations,"Background
Simulating the cardiac function requires the numerical solution of multi-physics and multi-scale mathematical models. This underscores the need for streamlined, accurate, and high-performance computational tools. Despite the dedicated endeavors of various research teams, comprehensive and user-friendly software programs for cardiac simulations, capable of accurately replicating both normal and pathological conditions, are still in the process of achieving full maturity within the scientific community.
Results
This work introduces 
\(\texttt {life}^{\text{x}}\)
-ep
, a publicly available software for numerical simulations of the electrophysiology activity of the cardiac muscle, under both normal and pathological conditions. 
\(\texttt {life}^{\text{x}}\)
-ep
 employs the monodomain equation to model the heart’s electrical activity. It incorporates both phenomenological and second-generation ionic models. These models are discretized using the Finite Element method on tetrahedral or hexahedral meshes. Additionally, 
\(\texttt {life}^{\text{x}}\)
-ep
 integrates the generation of myocardial fibers based on Laplace–Dirichlet Rule-Based Methods, previously released in Africa et al., 2023, within 
\(\texttt {life}^{\text{x}}\)
-fiber
. As an alternative, users can also choose to import myofibers from a file. This paper provides a concise overview of the mathematical models and numerical methods underlying 
\(\texttt {life}^{\text{x}}\)
-ep
, along with comprehensive implementation details and instructions for users. 
\(\texttt {life}^{\text{x}}\)
-ep
 features exceptional parallel speedup, scaling efficiently when using up to thousands of cores, and its implementation has been verified against an established benchmark problem for computational electrophysiology. We showcase the key features of 
\(\texttt {life}^{\text{x}}\)
-ep
 through various idealized and realistic simulations conducted in both normal and pathological scenarios. Furthermore, the software offers a user-friendly and flexible interface, simplifying the setup of simulations using self-documenting parameter files.
Conclusions
\(\texttt {life}^{\text{x}}\)
-ep
 provides easy access to cardiac electrophysiology simulations for a wide user community. It offers a computational tool that integrates models and accurate methods for simulating cardiac electrophysiology within a high-performance framework, while maintaining a user-friendly interface. 
\(\texttt {life}^{\text{x}}\)
-ep
 represents a valuable tool for conducting in silico patient-specific simulations."
104,Unbiased image segmentation assessment toolkit for quantitative differentiation of state-of-the-art algorithms and pipelines,"Background
Image segmentation pipelines are commonly used in microscopy to identify cellular compartments like nucleus and cytoplasm, but there are few standards for comparing segmentation accuracy across pipelines. The process of selecting a segmentation assessment pipeline can seem daunting to researchers due to the number and variety of metrics available for evaluating segmentation quality.
Results
Here we present automated pipelines to obtain a comprehensive set of 69 metrics to evaluate segmented data and propose a selection methodology for models based on quantitative analysis, dimension reduction or unsupervised classification techniques and informed selection criteria.
Conclusion
We show that the metrics used here can often be reduced to a small number of metrics that give a more complete understanding of segmentation accuracy, with different groups of metrics providing sensitivity to different types of segmentation error. These tools are delivered as easy to use python libraries, command line tools, Common Workflow Language Tools, and as Web Image Processing Pipeline interactive plugins to ensure a wide range of users can access and use them. We also present how our evaluation methods can be used to observe the changes in segmentations across modern machine learning/deep learning workflows and use cases."
105,cgMSI: pathogen detection within species from nanopore metagenomic sequencing data,"Background
Metagenomic sequencing is an unbiased approach that can potentially detect all the known and unidentified strains in pathogen detection. Recently, nanopore sequencing has been emerging as a highly potential tool for rapid pathogen detection due to its fast turnaround time. However, identifying pathogen within species is nontrivial for nanopore sequencing data due to the high sequencing error rate.
Results
We developed the core gene alleles metagenome strain identification (cgMSI) tool, which uses a two-stage maximum a posteriori probability estimation method to detect pathogens at strain level from nanopore metagenomic sequencing data at low computational cost. The cgMSI tool can accurately identify strains and estimate relative abundance at 1× coverage.
Conclusions
We developed cgMSI for nanopore metagenomic pathogen detection within species. cgMSI is available at 
https://github.com/ZHU-XU-xmu/cgMSI
."
106,Refactoring and performance analysis of the main CNN architectures: using false negative rate minimization to solve the clinical images melanoma detection problem,"Background
Melanoma is one of the deadliest tumors in the world. Early detection is critical for first-line therapy in this tumor pathology and it remains challenging due to the need for histological analysis to ensure correctness in diagnosis. Therefore, multiple computer-aided diagnosis (CAD) systems working on melanoma images were proposed to mitigate the need of a biopsy. However, although the high global accuracy is declared in literature results, the CAD systems for the health fields must focus on the lowest false negative rate (FNR) possible to qualify as a diagnosis support system. The final goal must be to avoid classification type 2 errors to prevent life-threatening situations. Another goal could be to create an easy-to-use system for both physicians and patients.
Results
To achieve the minimization of type 2 error, we performed a wide exploratory analysis of the principal convolutional neural network (CNN) architectures published for the multiple image classification problem; we adapted these networks to the melanoma clinical image binary classification problem (MCIBCP). We collected and analyzed performance data to identify the best CNN architecture, in terms of FNR, usable for solving the MCIBCP problem. Then, to provide a starting point for an easy-to-use CAD system, we used a clinical image dataset (MED-NODE) because clinical images are easier to access: they can be taken by a smartphone or other hand-size devices. Despite the lower resolution than dermoscopic images, the results in the literature would suggest that it would be possible to achieve high classification performance by using clinical images. In this work, we used MED-NODE, which consists of 170 clinical images (70 images of melanoma and 100 images of naevi). We optimized the following CNNs for the MCIBCP problem: Alexnet, DenseNet, GoogleNet Inception V3, GoogleNet, MobileNet, ShuffleNet, SqueezeNet, and VGG16.
Conclusions
The results suggest that a CNN built on the VGG or AlexNet structure can ensure the lowest FNR (0.07) and (0.13), respectively. In both cases, discrete global performance is ensured: 73% (accuracy), 82% (sensitivity) and 59% (specificity) for VGG; 89% (accuracy), 87% (sensitivity) and 90% (specificity) for AlexNet."
107,"disperseNN2
: a neural network for estimating dispersal distance from georeferenced polymorphism data","Spatial genetic variation is shaped in part by an organism’s dispersal ability. We present a deep learning tool, 
disperseNN2
, for estimating the mean per-generation dispersal distance from georeferenced polymorphism data. Our neural network performs feature extraction on pairs of genotypes, and uses the geographic information that comes with each sample. These attributes led 
disperseNN2
 to outperform a state-of-the-art deep learning method that does not use explicit spatial information: the mean relative absolute error was reduced by 33% and 48% using sample sizes of 10 and 100 individuals, respectively. 
disperseNN2
 is particularly useful for non-model organisms or systems with sparse genomic resources, as it uses unphased, single nucleotide polymorphisms as its input. The software is open source and available from 
https://github.com/kr-colab/disperseNN2
, with documentation located at 
https://dispersenn2.readthedocs.io/en/latest/
."
108,MSXFGP: combining improved sparrow search algorithm with XGBoost for enhanced genomic prediction,"Background
With the significant reduction in the cost of high-throughput sequencing technology, genomic selection technology has been rapidly developed in the field of plant breeding. Although numerous genomic selection methods have been proposed by researchers, the existing genomic selection methods still face the problem of poor prediction accuracy in practical applications.
Results
This paper proposes a genome prediction method MSXFGP based on a multi-strategy improved sparrow search algorithm (SSA) to optimize XGBoost parameters and feature selection. Firstly, logistic chaos mapping, elite learning, adaptive parameter adjustment, Levy flight, and an early stop strategy are incorporated into the SSA. This integration serves to enhance the global and local search capabilities of the algorithm, thereby improving its convergence accuracy and stability. Subsequently, the improved SSA is utilized to concurrently optimize XGBoost parameters and feature selection, leading to the establishment of a new genomic selection method, MSXFGP. Utilizing both the coefficient of determination R
2
 and the Pearson correlation coefficient as evaluation metrics, MSXFGP was evaluated against six existing genomic selection models across six datasets. The findings reveal that MSXFGP prediction accuracy is comparable or better than existing widely used genomic selection methods, and it exhibits better accuracy when R
2
 is utilized as an assessment metric. Additionally, this research provides a user-friendly Python utility designed to aid breeders in the effective application of this innovative method. MSXFGP is accessible at 
https://github.com/DIBreeding/MSXFGP
.
Conclusions
The experimental results show that the prediction accuracy of MSXFGP is comparable or better than existing genome selection methods, providing a new approach for plant genome selection."
109,Network-based prediction approach for cancer-specific driver missense mutations using a graph neural network,"Background
In cancer genomic medicine, finding driver mutations involved in cancer development and tumor growth is crucial. Machine-learning methods to predict driver missense mutations have been developed because variants are frequently detected by genomic sequencing. However, even though the abnormalities in molecular networks are associated with cancer, many of these methods focus on individual variants and do not consider molecular networks. Here we propose a new network-based method, Net-DMPred, to predict driver missense mutations considering molecular networks. Net-DMPred consists of the graph part and the prediction part. In the graph part, molecular networks are learned by a graph neural network (GNN). The prediction part learns whether variants are driver variants using features of individual variants combined with the graph features learned in the graph part.
Results
Net-DMPred, which considers molecular networks, performed better than conventional methods. Furthermore, the prediction performance differed by the molecular network structure used in learning, suggesting that it is important to consider not only the local network related to cancer but also the large-scale network in living organisms.
Conclusions
We propose a network-based machine learning method, Net-DMPred, for predicting cancer driver missense mutations. Our method enables us to consider the entire graph architecture representing the molecular network because it uses GNN. Net-DMPred is expected to detect driver mutations from a lot of missense mutations that are not known to be associated with cancer."
110,Multimodal hybrid convolutional neural network based brain tumor grade classification,"An abnormal growth or fatty mass of cells in the brain is called a tumor. They can be either healthy (normal) or become cancerous, depending on the structure of their cells. This can result in increased pressure within the cranium, potentially causing damage to the brain or even death. As a result, diagnostic procedures such as computed tomography, magnetic resonance imaging, and positron emission tomography, as well as blood and urine tests, are used to identify brain tumors. However, these methods can be labor-intensive and sometimes yield inaccurate results. Instead of these time-consuming methods, deep learning models are employed because they are less time-consuming, require less expensive equipment, produce more accurate results, and are easy to set up. In this study, we propose a method based on transfer learning, utilizing the pre-trained VGG-19 model. This approach has been enhanced by applying a customized convolutional neural network framework and combining it with pre-processing methods, including normalization and data augmentation. For training and testing, our proposed model used 80% and 20% of the images from the dataset, respectively. Our proposed method achieved remarkable success, with an accuracy rate of 99.43%, a sensitivity of 98.73%, and a specificity of 97.21%. The dataset, sourced from Kaggle for training purposes, consists of 407 images, including 257 depicting brain tumors and 150 without tumors. These models could be utilized to develop clinically useful solutions for identifying brain tumors in CT images based on these outcomes."
111,New statistical selection method for pleiotropic variants associated with both quantitative and qualitative traits,"Background
Identification of pleiotropic variants associated with multiple phenotypic traits has received increasing attention in genetic association studies. Overlapping genetic associations from multiple traits help to detect weak genetic associations missed by single-trait analyses. Many statistical methods were developed to identify pleiotropic variants with most of them being limited to quantitative traits when pleiotropic effects on both quantitative and qualitative traits have been observed. This is a statistically challenging problem because there does not exist an appropriate multivariate distribution to model both quantitative and qualitative data together. Alternatively, meta-analysis methods can be applied, which basically integrate summary statistics of individual variants associated with either a quantitative or a qualitative trait without accounting for correlations among genetic variants.
Results
We propose a new statistical selection method based on a unified selection score quantifying how a genetic variant, i.e., a pleiotropic variant associates with both quantitative and qualitative traits. In our extensive simulation studies where various types of pleiotropic effects on both quantitative and qualitative traits were considered, we demonstrated that the proposed method outperforms the existing meta-analysis methods in terms of true positive selection. We also applied the proposed method to a peanut dataset with 6 quantitative and 2 qualitative traits, and a cowpea dataset with 2 quantitative and 6 qualitative traits. We were able to detect some potentially pleiotropic variants missed by the existing methods in both analyses.
Conclusions
The proposed method is able to locate pleiotropic variants associated with both quantitative and qualitative traits. It has been implemented into an R package ‘UNISS’, which can be downloaded from 
http://github.com/statpng/uniss."
112,Cross-study analyses of microbial abundance using generalized common factor methods,"Background
By creating networks of biochemical pathways, communities of micro-organisms are able to modulate the properties of their environment and even the metabolic processes within their hosts. Next-generation high-throughput sequencing has led to a new frontier in microbial ecology, promising the ability to leverage the microbiome to make crucial advancements in the environmental and biomedical sciences. However, this is challenging, as genomic data are high-dimensional, sparse, and noisy. Much of this noise reflects the exact conditions under which sequencing took place, and is so significant that it limits consensus-based validation of study results.
Results
We propose an ensemble approach for cross-study exploratory analyses of microbial abundance data in which we first estimate the variance-covariance matrix of the underlying abundances from each dataset on the log scale assuming Poisson sampling, and subsequently model these covariances jointly so as to find a shared low-dimensional subspace of the feature space.
Conclusions
By viewing the projection of the latent true abundances onto this common structure, the variation is pared down to that which is shared among all datasets, and is likely to reflect more generalizable biological signal than can be inferred from individual datasets. We investigate several ways of achieving this, demonstrate that they work well on simulated and real metagenomic data in terms of signal retention and interpretability, and recommend a particular implementation."
113,A hybrid Stacking-SMOTE model for optimizing the prediction of autistic genes,"Purpose
Autism spectrum disorder(ASD) is a disease associated with the neurodevelopment of the brain. The autism spectrum can be observed in early childhood, where the symptoms of the disease usually appear in children within the first year of their life. Currently, ASD can only be diagnosed based on the apparent symptoms due to the lack of information on genes related to the disease. Therefore, in this paper, we need to predict the largest number of disease-causing genes for a better diagnosis.
Methods
A hybrid stacking ensemble model with Synthetic Minority Oversampling TEchnique (Stack-SMOTE) is proposed to predict the genes associated with ASD. The proposed model uses the gene ontology database to measure the similarities between the genes using a hybrid gene similarity function(HGS). HGS is effective in measuring the similarity as it combines the features of information gain-based methods and graph-based methods. The proposed model solves the imbalanced ASD dataset problem using the Synthetic Minority Oversampling Technique (SMOTE), which generates synthetic data rather than duplicates the data to reduce the overfitting. Sequentially, a gradient boosting-based random forest classifier (GBBRF) is introduced as a new combination technique to enhance the prediction of ASD genes. Moreover, the GBBRF classifier combined with random forest(RF), k-nearest neighbor, support vector machine(SVM), and logistic regression(LR) to form the proposed Stacking-SMOTE model to optimize the prediction of ASD genes.
Results
The proposed Stacking-SMOTE model is evaluated using the Simons Foundation Autism Research Initiative (SFARI) gene database and a set of candidates ASD genes.The results of the proposed model-based SMOTE outperform other reported undersampling and oversampling techniques. Sequentially, the results of GBBRF achieve higher accuracy than using the basic classifiers. Moreover, the experimental results show that the proposed Stacking-SMOTE model outperforms the existing ASD prediction models with approximately 95.5% accuracy.
Conclusion
The proposed Stacking-SMOTE model demonstrates that SMOTE is effective in handling the autism imbalanced data. Sequentially, the integration between the gradient boosting and random forest classifier (GBBRF) support to build a robust stacking ensemble model(Stacking-SMOTE)."
114,CurvAGN: Curvature-based Adaptive Graph Neural Networks for Predicting Protein-Ligand Binding Affinity,"Accurately predicting the binding affinity between proteins and ligands is crucial for drug discovery. Recent advances in graph neural networks (GNNs) have made significant progress in learning representations of protein-ligand complexes to estimate binding affinities. To improve the performance of GNNs, there frequently needs to look into protein-ligand complexes from geometric perspectives. While the “off-the-shelf” GNNs could incorporate some basic geometric structures of molecules, such as distances and angles, through modeling the complexes as homophilic graphs, these solutions seldom take into account the higher-level geometric attributes like curvatures and homology, and also heterophilic interactions.To address these limitations, we introduce the Curvature-based Adaptive Graph Neural Network (CurvAGN). This GNN comprises two components: a curvature block and an adaptive attention guided neural block (AGN). The curvature block encodes multiscale curvature informaton, then the AGN, based on an adaptive graph attention mechanism, incorporates geometry structure including angle, distance, and multiscale curvature, long-range molecular interactions, and heterophily of the graph into the protein-ligand complex representation. We demonstrate the superiority of our proposed model through experiments conducted on the PDBbind-V2016 core dataset."
115,"Mabs, a suite of tools for gene-informed genome assembly","Background
Despite constantly improving genome sequencing methods, error-free eukaryotic genome assembly has not yet been achieved. Among other kinds of problems of eukaryotic genome assembly are so-called ""haplotypic duplications"", which may manifest themselves as cases of alleles being mistakenly assembled as paralogues. Haplotypic duplications are dangerous because they create illusions of gene family expansions and, thus, may lead scientists to incorrect conclusions about genome evolution and functioning.
Results
Here, I present Mabs, a suite of tools that serve as parameter optimizers of the popular genome assemblers Hifiasm and Flye. By optimizing the parameters of Hifiasm and Flye, Mabs tries to create genome assemblies with the genes assembled as accurately as possible. Tests on 6 eukaryotic genomes showed that in 6 out of 6 cases, Mabs created assemblies with more accurately assembled genes than those generated by Hifiasm and Flye when they were run with default parameters. When assemblies of Mabs, Hifiasm and Flye were postprocessed by a popular tool for haplotypic duplication removal, Purge_dups, genes were better assembled by Mabs in 5 out of 6 cases.
Conclusions
Mabs is useful for making high-quality genome assemblies. It is available at 
https://github.com/shelkmike/Mabs"
116,PRMxAI: protein arginine methylation sites prediction based on amino acid spatial distribution using explainable artificial intelligence,"Background
Protein methylation, a post-translational modification, is crucial in regulating various cellular functions. Arginine methylation is required to understand crucial biochemical activities and biological functions, like gene regulation, signal transduction, etc. However, some experimental methods, including Chip–Chip, mass spectrometry, and methylation-specific antibodies, exist for the prediction of methylated proteins. These experimental methods are expensive and tedious. As a result, computational methods based on machine learning play an efficient role in predicting arginine methylation sites.
Results
In this research, a novel method called PRMxAI has been proposed to predict arginine methylation sites. The proposed PRMxAI extract sequence-based features, such as dipeptide composition, physicochemical properties, amino acid composition, and information theory-based features (Arimoto, Havrda-Charvat, Renyi, and Shannon entropy), to represent the protein sequences into numerical format. Various machine learning algorithms are implemented to select the better classifier, such as Decision trees, Naive Bayes, Random Forest, Support vector machines, and K-nearest neighbors. The random forest algorithm is selected as the underlying classifier for the PRMxAI model. The performance of PRMxAI is evaluated by employing 10-fold cross-validation, and it yields 87.17% and 90.40% accuracy on mono-methylarginine and di-methylarginine data sets, respectively. This research also examines the impact of various features on both data sets using explainable artificial intelligence.
Conclusions
The proposed PRMxAI shows the effectiveness of the features for predicting arginine methylation sites. Additionally, the SHapley Additive exPlanation method is used to interpret the predictive mechanism of the proposed model. The results indicate that the proposed PRMxAI model outperforms other state-of-the-art predictors."
117,"Graph regularized non-negative matrix factorization with 
\(L_{2,1}\)
 norm regularization terms for drug–target interactions prediction","Background
Identifying drug–target interactions (DTIs) plays a key role in drug development. Traditional wet experiments to identify DTIs are costly and time consuming. Effective computational methods to predict DTIs are useful to speed up the process of drug discovery. A variety of non-negativity matrix factorization based methods are proposed to predict DTIs, but most of them overlooked the sparsity of feature matrices and the convergence of adopted matrix factorization algorithms, therefore their performances can be further improved.
Results
In order to predict DTIs more accurately, we propose a novel method iPALM-DLMF. iPALM-DLMF models DTIs prediction as a problem of non-negative matrix factorization with graph dual regularization terms and 
\(L_{2,1}\)
 norm regularization terms. The graph dual regularization terms are used to integrate the information from the drug similarity matrix and the target similarity matrix, and 
\(L_{2,1}\)
 norm regularization terms are used to ensure the sparsity of the feature matrices obtained by non-negative matrix factorization. To solve the model, iPALM-DLMF adopts non-negative double singular value decomposition to initialize the nonnegative matrix factorization, and an inertial Proximal Alternating Linearized Minimization iterating process, which has been proved to converge to a KKT point, to obtain the final result of the matrix factorization. Extensive experimental results show that iPALM-DLMF has better performance than other state-of-the-art methods. In case studies, in 50 highest-scoring proteins targeted by the drug gabapentin predicted by iPALM-DLMF, 46 have been validated, and in 50 highest-scoring drugs targeting prostaglandin-endoperoxide synthase 2 predicted by iPALM-DLMF, 47 have been validated."
118,DrugRep-HeSiaGraph: when heterogenous siamese neural network meets knowledge graphs for drug repurposing,"Background
Drug repurposing is an approach that holds promise for identifying new therapeutic uses for existing drugs. Recently, knowledge graphs have emerged as significant tools for addressing the challenges of drug repurposing. However, there are still major issues with constructing and embedding knowledge graphs.
Results
This study proposes a two-step method called DrugRep-HeSiaGraph to address these challenges. The method integrates the drug-disease knowledge graph with the application of a heterogeneous siamese neural network. In the first step, a drug-disease knowledge graph named DDKG-V1 is constructed by defining new relationship types, and then numerical vector representations for the nodes are created using the distributional learning method. In the second step, a heterogeneous siamese neural network called HeSiaNet is applied to enrich the embedding of drugs and diseases by bringing them closer in a new unified latent space. Then, it predicts potential drug candidates for diseases. DrugRep-HeSiaGraph achieves impressive performance metrics, including an AUC-ROC of 91.16%, an AUC-PR of 90.32%, an accuracy of 84.63%, a BS of 0.119, and an MCC of 69.31%.
Conclusion
We demonstrate the effectiveness of the proposed method in identifying potential drugs for COVID-19 as a case study. In addition, this study shows the role of dipeptidyl peptidase 4 (DPP-4) as a potential receptor for SARS-CoV-2 and the effectiveness of DPP-4 inhibitors in facing COVID-19. This highlights the practical application of the model in addressing real-world challenges in the field of drug repurposing. The code and data for DrugRep-HeSiaGraph are publicly available at 
https://github.com/CBRC-lab/DrugRep-HeSiaGraph
."
119,SeqPredNN: a neural network that generates protein sequences that fold into specified tertiary structures,"Background
The relationship between the sequence of a protein, its structure, and the resulting connection between its structure and function, is a foundational principle in biological science. Only recently has the computational prediction of protein structure based only on protein sequence been addressed effectively by AlphaFold, a neural network approach that can predict the majority of protein structures with X-ray crystallographic accuracy.
A question that is now of acute relevance is the “inverse protein folding problem”: predicting the sequence of a protein that folds into a specified structure. This will be of immense value in protein engineering and biotechnology, and will allow the design and expression of recombinant proteins that can, for instance, fold into specified structures as a scaffold for the attachment of recombinant antigens, or enzymes with modified or novel catalytic activities.
Here we describe the development of SeqPredNN, a feed-forward neural network trained with X-ray crystallographic structures from the RCSB Protein Data Bank to predict the identity of amino acids in a protein structure using only the relative positions, orientations, and backbone dihedral angles of nearby residues.
Results
We predict the sequence of a protein expected to fold into a specified structure and assess the accuracy of the prediction using both AlphaFold and RoseTTAFold to computationally generate the fold of the derived sequence. We show that the sequences predicted by SeqPredNN fold into a structure with a median TM-score of 0.638 when compared to  the crystal structure according to AlphaFold predictions, yet these sequences are unique and only 28.4% identical to the sequence of the crystallized protein.
Conclusions
We propose that SeqPredNN will be a valuable tool to generate proteins of defined structure for the design of novel biomaterials, pharmaceuticals, catalysts, and reporter systems. The low sequence identity of its predictions compared to the native sequence could prove useful for developing proteins with modified physical properties, such as water solubility and thermal stability. The speed and ease of use of SeqPredNN offers a significant advantage over physics-based protein design methods."
120,An effective correlation-based data modeling framework for automatic diabetes prediction using machine and deep learning techniques,"The rising risk of diabetes, particularly in emerging countries, highlights the importance of early detection. Manual prediction can be a challenging task, leading to the need for automatic approaches. The major challenge with biomedical datasets is data scarcity. Biomedical data is often difficult to obtain in large quantities, which can limit the ability to train deep learning models effectively. Biomedical data can be noisy and inconsistent, which can make it difficult to train accurate models. To overcome the above-mentioned challenges, this work presents a new framework for data modeling that is based on correlation measures between features and can be used to process data effectively for predicting diabetes. The standard, publicly available Pima Indians Medical Diabetes (PIMA) dataset is utilized to verify the effectiveness of the proposed techniques. Experiments using the PIMA dataset showed that the proposed data modeling method improved the accuracy of machine learning models by an average of 9%, with deep convolutional neural network models achieving an accuracy of 96.13%. Overall, this study demonstrates the effectiveness of the proposed strategy in the early and reliable prediction of diabetes."
121,HoCoRT: host contamination removal tool,"Background
Shotgun metagenome sequencing data obtained from a host environment will usually be contaminated with sequences from the host organism. Host sequences should be removed before further analysis to avoid biases, reduce downstream computational load, or ensure privacy in the case of a human host. The tools that we identified, as designed specifically to perform host contamination sequence removal, were either outdated, not maintained, or complicated to use. Consequently, we have developed HoCoRT, a fast and user-friendly tool that implements several methods for optimised host sequence removal. We have evaluated the speed and accuracy of these methods.
Results
HoCoRT is an open-source command-line tool for host contamination removal. It is designed to be easy to install and use, offering a one-step option for genome indexing. HoCoRT employs a variety of well-known mapping, classification, and alignment methods to classify reads. The user can select the underlying classification method and its parameters, allowing adaptation to different scenarios. Based on our investigation of various methods and parameters using synthetic human gut and oral microbiomes, and on assessment of publicly available data, we provide recommendations for typical datasets with short and long reads.
Conclusions
To decontaminate a human gut microbiome with short reads using HoCoRT, we found the optimal combination of speed and accuracy with BioBloom, Bowtie2 in end-to-end mode, and HISAT2. Kraken2 consistently demonstrated the highest speed, albeit with a trade-off in accuracy. The same applies to an oral microbiome, but here Bowtie2 was notably slower than the other tools. For long reads, the detection of human host reads is more difficult. In this case, a combination of Kraken2 and Minimap2 achieved the highest accuracy and detected 59% of human reads. In comparison to the dedicated DeconSeq tool, HoCoRT using Bowtie2 in end-to-end mode proved considerably faster and slightly more accurate. HoCoRT is available as a Bioconda package, and the source code can be accessed at 
https://github.com/ignasrum/hocort
 along with the documentation. It is released under the MIT licence and is compatible with Linux and macOS (except for the BioBloom module)."
122,FLARE: a fast and flexible workflow for identifying RNA editing foci,"Background
Fusion of RNA-binding proteins (RBPs) to RNA base-editing enzymes (such as APOBEC1 or ADAR) has emerged as a powerful tool for the discovery of RBP binding sites. However, current methods that analyze sequencing data from RNA-base editing experiments are vulnerable to false positives due to off-target editing, genetic variation and sequencing errors.
Results
We present FLagging Areas of RNA-editing Enrichment (FLARE), a Snakemake-based pipeline that builds on the outputs of the SAILOR edit site discovery tool to identify regions statistically enriched for RNA editing. FLARE can be configured to analyze any type of RNA editing, including C to U and A to I. We applied FLARE to C-to-U editing data from a RBFOX2-APOBEC1 STAMP experiment, to show that our approach attains high specificity for detecting RBFOX2 binding sites. We also applied FLARE to detect regions of exogenously introduced as well as endogenous A-to-I editing.
Conclusions
FLARE is a fast and flexible workflow that identifies significantly edited regions from RNA-seq data. The FLARE codebase is available at 
https://github.com/YeoLab/FLARE
."
123,Reference-based genome compression using the longest matched substrings with parallelization consideration,"Background
A large number of researchers have devoted to accelerating the speed of genome sequencing and reducing the cost of genome sequencing for decades, and they have made great strides in both areas, making it easier for researchers to study and analyze genome data. However, how to efficiently store and transmit the vast amount of genome data generated by high-throughput sequencing technologies has become a challenge for data compression researchers. Therefore, the research of genome data compression algorithms to facilitate the efficient representation of genome data has gradually attracted the attention of these researchers. Meanwhile, considering that the current computing devices have multiple cores, how to make full use of the advantages of the computing devices and improve the efficiency of parallel processing is also an important direction for designing genome compression algorithms.
Results
We proposed an algorithm (LMSRGC) based on reference genome sequences, which uses the suffix array (SA) and the longest common prefix (LCP) array to find the longest matched substrings (LMS) for the compression of genome data in FASTA format. The proposed algorithm utilizes the characteristics of SA and the LCP array to select all appropriate LMSs between the genome sequence to be compressed and the reference genome sequence and then utilizes LMSs to compress the target genome sequence. To speed up the operation of the algorithm, we use GPUs to parallelize the construction of SA, while using multiple threads to parallelize the creation of the LCP array and the filtering of LMSs.
Conclusions
Experiment results demonstrate that our algorithm is competitive with the current state-of-the-art algorithms in compression ratio and compression time."
124,Predicting tumour content of liquid biopsies from cell-free DNA,"Background
Liquid biopsy is a minimally-invasive method of sampling bodily fluids, capable of revealing evidence of cancer. The distribution of cell-free DNA (cfDNA) fragment lengths has been shown to differ between healthy subjects and cancer patients, whereby the distributional shift correlates with the sample’s tumour content. These fragmentomic data have not yet been utilised to directly quantify the proportion of tumour-derived cfDNA in a liquid biopsy.
Results
We used statistical learning to predict tumour content from Fourier and wavelet transforms of cfDNA length distributions in samples from 118 cancer patients. The model was validated on an independent dilution series of patient plasma.
Conclusions
This proof of concept suggests that our fragmentomic methodology could be useful for predicting tumour content in liquid biopsies."
125,DGDTA: dynamic graph attention network for predicting drug–target binding affinity,"Background
Obtaining accurate drug–target binding affinity (DTA) information is significant for drug discovery and drug repositioning. Although some methods have been proposed for predicting DTA, the features of proteins and drugs still need to be further analyzed. Recently, deep learning has been successfully used in many fields. Hence, designing a more effective deep learning method for predicting DTA remains attractive.
Results
Dynamic graph DTA (DGDTA), which uses a dynamic graph attention network combined with a bidirectional long short-term memory (Bi-LSTM) network to predict DTA is proposed in this paper. DGDTA adopts drug compound as input according to its corresponding simplified molecular input line entry system (SMILES) and protein amino acid sequence. First, each drug is considered a graph of interactions between atoms and edges, and dynamic attention scores are used to consider which atoms and edges in the drug are most important for predicting DTA. Then, Bi-LSTM is used to better extract the contextual information features of protein amino acid sequences. Finally, after combining the obtained drug and protein feature vectors, the DTA is predicted by a fully connected layer. The source code is available from GitHub at 
https://github.com/luojunwei/DGDTA
.
Conclusions
The experimental results show that DGDTA can predict DTA more accurately than some other methods."
126,A robust approach to 3D neuron shape representation for quantification and classification,"We consider the problem of finding an accurate representation of neuron shapes, extracting sub-cellular features, and classifying neurons based on neuron shapes. In neuroscience research, the skeleton representation is often used as a compact and abstract representation of neuron shapes. However, existing methods are limited to getting and analyzing “curve” skeletons which can only be applied for tubular shapes. This paper presents a 3D neuron morphology analysis method for more general and complex neuron shapes. First, we introduce the concept of skeleton mesh to represent general neuron shapes and propose a novel method for computing mesh representations from 3D surface point clouds. A skeleton graph is then obtained from skeleton mesh and is used to extract sub-cellular features. Finally, an unsupervised learning method is used to embed the skeleton graph for neuron classification. Extensive experiment results are provided and demonstrate the robustness of our method to analyze neuron morphology."
127,Automatic echocardiographic anomalies interpretation using a stacked residual-dense network model,"Echocardiographic interpretation during the prenatal or postnatal period is important for diagnosing cardiac septal abnormalities. However, manual interpretation can be time consuming and subject to human error. Automatic segmentation of echocardiogram can support cardiologists in making an initial interpretation. However, such a process does not always provide straightforward information to make a complete interpretation. The segmentation process only identifies the region of cardiac septal abnormality, whereas complete interpretation should determine based on the position of defect. In this study, we proposed a stacked residual-dense network model to segment the entire region of cardiac and classifying their defect positions to generate automatic echocardiographic interpretation. We proposed the generalization model with incorporated two modalities: prenatal and postnatal echocardiography. To further evaluate the effectiveness of our model, its performance was verified by five cardiologists. We develop a pipeline process using 1345 echocardiograms for training data and 181 echocardiograms for unseen data from prospective patients acquired during standard clinical practice at Muhammad Hoesin General Hospital in Indonesia. As a result, the proposed model produced of 58.17% intersection over union (IoU), 75.75% dice similarity coefficient (DSC), and 76.36% mean average precision (mAP) for the validation data. Using unseen data, we achieved 42.39% IoU, 55.72% DSC, and 51.04% mAP. Further, the classification of defect positions using unseen data had approximately 92.27% accuracy, 94.33% specificity, and 92.05% sensitivity. Finally, our proposed model is validated with human expert with varying Kappa value. On average, these results hold promise of increasing suitability in clinical practice as a supporting diagnostic tool for establishing the diagnosis."
128,Fuzzy optimization for identifying antiviral targets for treating SARS-CoV-2 infection in the heart,"In this paper, a fuzzy hierarchical optimization framework is proposed for identifying potential antiviral targets for treating severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) infection in the heart. The proposed framework comprises four objectives for evaluating the elimination of viral biomass growth and the minimization of side effects during treatment. In the application of the framework, Dulbecco’s modified eagle medium (DMEM) and Ham’s medium were used as uptake nutrients on an antiviral target discovery platform. The prediction results from the framework reveal that most of the antiviral enzymes in the aforementioned media are involved in fatty acid metabolism and amino acid metabolism. However, six enzymes involved in cholesterol biosynthesis in Ham’s medium and three enzymes involved in glycolysis in DMEM are unable to eliminate the growth of the SARS-CoV-2 biomass. Three enzymes involved in glycolysis, namely BPGM, GAPDH, and ENO1, in DMEM combine with the supplemental uptake of L-cysteine to increase the cell viability grade and metabolic deviation grade. Moreover, six enzymes involved in cholesterol biosynthesis reduce and fail to reduce viral biomass growth in a culture medium if a cholesterol uptake reaction does not occur and occurs in this medium, respectively."
129,A heterogeneous graph convolutional attention network method for classification of autism spectrum disorder,"Background
Autism spectrum disorder (ASD) is a serious developmental disorder of the brain. Recently, various deep learning methods based on functional magnetic resonance imaging (fMRI) data have been developed for the classification of ASD. Among them, graph neural networks, which generalize deep neural network models to graph structured data, have shown great advantages. However, in graph neural methods, because the graphs constructed are homogeneous, the phenotype information of the subjects cannot be fully utilized. This affects the improvement of the classification performance.
Methods
To fully utilize the phenotype information, this paper proposes a heterogeneous graph convolutional attention network (HCAN) model to classify ASD. By combining an attention mechanism and a heterogeneous graph convolutional network, important aggregated features can be extracted in the HCAN. The model consists of a multilayer HCAN feature extractor and a multilayer perceptron (MLP) classifier. First, a heterogeneous population graph was constructed based on the fMRI and phenotypic data. Then, a multilayer HCAN is used to mine graph-based features from the heterogeneous graph. Finally, the extracted features are fed into an MLP for the final classification.
Results
The proposed method is assessed on the autism brain imaging data exchange (ABIDE) repository. In total, 871 subjects in the ABIDE I dataset are used for the classification task. The best classification accuracy of 82.9% is achieved. Compared to the other methods using exactly the same subjects in the literature, the proposed method achieves superior performance to the best reported result.
Conclusions
The proposed method can effectively integrate heterogeneous graph convolutional networks with a semantic attention mechanism so that the phenotype features of the subjects can be fully utilized. Moreover, it shows great potential in the diagnosis of brain functional disorders with fMRI data."
130,Inferring circadian gene regulatory relationships from gene expression data with a hybrid framework,"Background
The central biological clock governs numerous facets of mammalian physiology, including sleep, metabolism, and immune system regulation. Understanding gene regulatory relationships is crucial for unravelling the mechanisms that underlie various cellular biological processes. While it is possible to infer circadian gene regulatory relationships from time-series gene expression data, relying solely on correlation-based inference may not provide sufficient information about causation. Moreover, gene expression data often have high dimensions but a limited number of observations, posing challenges in their analysis.
Methods
In this paper, we introduce a new hybrid framework, referred to as Circadian Gene Regulatory Framework (CGRF), to infer circadian gene regulatory relationships from gene expression data of rats. The framework addresses the challenges of high-dimensional data by combining the fuzzy C-means clustering algorithm with dynamic time warping distance. Through this approach, we efficiently identify the clusters of genes related to the target gene. To determine the significance of genes within a specific cluster, we employ the Wilcoxon signed-rank test. Subsequently, we use a dynamic vector autoregressive method to analyze the selected significant gene expression profiles and reveal directed causal regulatory relationships based on partial correlation.
Conclusion
The proposed CGRF framework offers a comprehensive and efficient solution for understanding circadian gene regulation. Circadian gene regulatory relationships are inferred from the gene expression data of rats based on the 
Aanat
 target gene. The results show that genes 
Pde10a, Atp7b, Prok2, Per1, Rhobtb3
 and 
Dclk1
 stand out, which have been known to be essential for the regulation of circadian activity. The potential relationships between genes 
Tspan15, Eprs, Eml5
 and 
Fsbp
 with a circadian rhythm need further experimental research."
131,"CrMP-Sol database: classification, bioinformatic analyses and comparison of cancer-related membrane proteins and their water-soluble variant designs","Membrane proteins are critical mediators for tumor progression and present enormous therapeutic potentials. Although gene profiling can identify their cancer-specific signatures, systematic correlations between protein functions and tumor-related mechanisms are still unclear. We present here the CrMP-Sol database (
https://bio-gateway.aigene.org.cn/g/CrMP
), which aims to breach the gap between the two. Machine learning was used to extract key functional descriptions for protein visualization in the 3D-space, where spatial distributions provide function-based predictive connections between proteins and cancer types. CrMP-Sol also presents QTY-enabled water-soluble designs to facilitate native membrane protein studies despite natural hydrophobicity. Five examples with varying transmembrane helices in different categories were used to demonstrate the feasibility. Native and redesigned proteins exhibited highly similar characteristics, predicted structures and binding pockets, and slightly different docking poses against known ligands, although task-specific designs are still required for proteins more susceptible to internal hydrogen bond formations. The database can accelerate therapeutic developments and biotechnological applications of cancer-related membrane proteins."
132,Fast and sensitive validation of fusion transcripts in whole-genome sequencing data,"Background
In cancer, genomic rearrangements can create fusion genes that either combine protein-coding sequences from two different partner genes or place one gene under the control of the promoter of another gene. These fusion genes can act as oncogenic drivers in tumor development and several fusions involving kinases have been successfully exploited as drug targets. Expressed fusions can be identified in RNA sequencing (RNA-Seq) data, but fusion prediction software often has a high fraction of false positive fusion transcript predictions. This is problematic for both research and clinical applications.
Results
We describe a method for validation of fusion transcripts detected by RNA-Seq in matched whole-genome sequencing (WGS) data. Our pipeline uses discordant read pairs to identify supported fusion events and analyzes soft-clipped read alignments to determine genomic breakpoints. We have tested it on matched RNA-Seq and WGS data for both tumors and cancer cell lines and show that it can be used to validate both new predicted gene fusions and experimentally validated fusion events. It was considerably faster and more sensitive than using BreakDancer and Manta, software that is instead designed to detect many different types of structural variants on a genome-wide scale.
Conclusions
We have developed a fast and very sensitive pipeline for validation of gene fusions detected by RNA-Seq in matched WGS data. It can be used to identify high-quality gene fusions for further bioinformatic and experimental studies, including validation of genomic breakpoints and studies of the mechanisms that generate fusions. In a clinical setting, it could help find expressed gene fusions for personalized therapy."
133,"Designing and development of multi-epitope chimeric vaccine against 
Helicobacter pylori
 by exploring its entire immunogenic epitopes: an immunoinformatic approach","Background
Helicobacter pylori
 is a prominent causative agent of gastric ulceration, gastric adenocarcinoma and gastric lymphoma and have been categorised as a group 1 carcinogen by WHO. The treatment of 
H. pylori
 with proton pump inhibitors and antibiotics is effective but also leads to increased antibiotic resistance, patient dissatisfaction, and chances of reinfection. Therefore, an effective vaccine remains the most suitable prophylactic option for mass administration against this infection.
Results
We modelled a multi-chimera subunit vaccine candidate against 
H. pylori
 by screening its secretory/outer membrane proteins. We identified B-cell, MHC-II and IFN-γ-inducing epitopes within these proteins. The population coverage, antigenicity, physiochemical properties and secondary structure were evaluated using different 
in-silico
 tools, which showed it can be a good and effective vaccine candidate. The 3-D construct was predicted, refined, validated and docked with TLRs. Finally, we performed the molecular docking/simulation and immune simulation studies to validate the stability of interaction and in-silico cloned the epitope sequences into a pET28b(+) plasmid vector.
Conclusion
The multiepitope-constructed vaccine contains T- cells, B-cells along with IFN-γ inducing epitopes that have the property to generate good cell-mediated immunity and humoral response. This vaccine can protect most of the world’s population. The docking study and immune simulation revealed a good binding with TLRs and cell-mediated and humoral immune responses, respectively. Overall, we attempted to design a multiepitope vaccine and expect this vaccine will show an encouraging result against 
H. pylori
 infection in in-vivo use."
134,Identification of plant vacuole proteins by using graph neural network and contact maps,"Plant vacuoles are essential organelles in the growth and development of plants, and accurate identification of their proteins is crucial for understanding their biological properties. In this study, we developed a novel model called GraphIdn for the identification of plant vacuole proteins. The model uses SeqVec, a deep representation learning model, to initialize the amino acid sequence. We utilized the AlphaFold2 algorithm to obtain the structural information of corresponding plant vacuole proteins, and then fed the calculated contact maps into a graph convolutional neural network. GraphIdn achieved accuracy values of 88.51% and 89.93% in independent testing and fivefold cross-validation, respectively, outperforming previous state-of-the-art predictors. As far as we know, this is the first model to use predicted protein topology structure graphs to identify plant vacuole proteins. Furthermore, we assessed the effectiveness and generalization capability of our GraphIdn model by applying it to identify and locate peroxisomal proteins, which yielded promising outcomes. The source code and datasets can be accessed at 
https://github.com/SJNNNN/GraphIdn
."
135,TIPred: a novel stacked ensemble approach for the accelerated discovery of tyrosinase inhibitory peptides,"Background
Tyrosinase is an enzyme involved in melanin production in the skin. Several hyperpigmentation disorders involve the overproduction of melanin and instability of tyrosinase activity resulting in darker, discolored patches on the skin. Therefore, discovering tyrosinase inhibitory peptides (TIPs) is of great significance for basic research and clinical treatments. However, the identification of TIPs using experimental methods is generally cost-ineffective and time-consuming.
Results
Herein, a stacked ensemble learning approach, called TIPred, is proposed for the accurate and quick identification of TIPs by using sequence information. TIPred explored a comprehensive set of various baseline models derived from well-known machine learning (ML) algorithms and heterogeneous feature encoding schemes from multiple perspectives, such as chemical structure properties, physicochemical properties, and composition information. Subsequently, 130 baseline models were trained and optimized to create new probabilistic features. Finally, the feature selection approach was utilized to determine the optimal feature vector for developing TIPred. Both tenfold cross-validation and independent test methods were employed to assess the predictive capability of TIPred by using the stacking strategy. Experimental results showed that TIPred significantly outperformed the state-of-the-art method in terms of the independent test, with an accuracy of 0.923, MCC of 0.757 and an AUC of 0.977.
Conclusions
The proposed TIPred approach could be a valuable tool for rapidly discovering novel TIPs and effectively identifying potential TIP candidates for follow-up experimental validation. Moreover, an online webserver of TIPred is publicly available at 
http://pmlabstack.pythonanywhere.com/TIPred
."
136,KidneyGPS: a user-friendly web application to help prioritize kidney function genes and variants based on evidence from genome-wide association studies,"Background
Genome-wide association studies (GWAS) have identified hundreds of genetic loci associated with kidney function. By combining these findings with post-GWAS information (e.g., statistical fine-mapping to identify independent association signals and to narrow down signals to causal variants; or different sources of annotation data), new hypotheses regarding physiology and disease aetiology can be obtained. These hypotheses need to be tested in laboratory experiments, for example, to identify new therapeutic targets. For this purpose, the evidence obtained from GWAS and post-GWAS analyses must be processed and presented in a way that they are easily accessible to kidney researchers without specific GWAS expertise.
Main
Here we present KidneyGPS, a user-friendly web-application that combines genetic variant association for estimated glomerular filtration rate (eGFR) from the Chronic Kidney Disease Genetics consortium with annotation of (i) genetic variants with functional or regulatory effects (“SNP-to-gene” mapping), (ii) genes with kidney phenotypes in mice or human (“gene-to-phenotype”), and (iii) drugability of genes (to support re-purposing). KidneyGPS adopts a comprehensive approach summarizing evidence for all 5906 genes in the 424 GWAS loci for eGFR identified previously and the 35,885 variants in the 99% credible sets of 594 independent signals. KidneyGPS enables user-friendly access to the abundance of information by search functions for genes, variants, and regions. KidneyGPS also provides a function (“GPS tab”) to generate lists of genes with specific characteristics thus enabling customizable Gene Prioritisation (GPS). These specific characteristics can be as broad as any gene in the 424 loci with a known kidney phenotype in mice or human; or they can be highly focussed on genes mapping to genetic variants or signals with particularly with high statistical support. KidneyGPS is implemented with RShiny in a modularized fashion to facilitate update of input data (
https://kidneygps.ur.de/gps/
).
Conclusion
With the focus on kidney function related evidence, KidneyGPS fills a gap between large general platforms for accessing GWAS and post-GWAS results and the specific needs of the kidney research community. This makes KidneyGPS an important platform for kidney researchers to help translate in silico research results into in vitro or in vivo research."
137,Critical assessment of on-premise approaches to scalable genome analysis,"Background
Plummeting DNA sequencing cost in recent years has enabled genome sequencing projects to scale up by several orders of magnitude, which is transforming genomics into a highly data-intensive field of research. This development provides the much needed statistical power required for genotype–phenotype predictions in complex diseases.
Methods
In order to efficiently leverage the wealth of information, we here assessed several genomic data science tools. The rationale to focus on on-premise installations is to cope with situations where data confidentiality and compliance regulations etc. rule out cloud based solutions. We established a comprehensive qualitative and quantitative comparison between BCFtools, SnpSift, Hail, GEMINI, and OpenCGA. The tools were compared in terms of data storage technology, query speed, scalability, annotation, data manipulation, visualization, data output representation, and availability.
Results
Tools that leverage sophisticated data structures are noted as the most suitable for large-scale projects in varying degrees of scalability in comparison to flat-file manipulation (e.g., BCFtools, and SnpSift). Remarkably, for small to mid-size projects, even lightweight relational database.
Conclusion
The assessment criteria provide insights into the typical questions posed in scalable genomics and serve as guidance for the development of scalable computational infrastructure in genomics."
138,HAHNet: a convolutional neural network for HER2 status classification of breast cancer,"Objective
Breast cancer is a significant health issue for women, and human epidermal growth factor receptor-2 (HER2) plays a crucial role as a vital prognostic and predictive factor. The HER2 status is essential for formulating effective treatment plans for breast cancer. However, the assessment of HER2 status using immunohistochemistry (IHC) is time-consuming and costly. Existing computational methods for evaluating HER2 status have limitations and lack sufficient accuracy. Therefore, there is an urgent need for an improved computational method to better assess HER2 status, which holds significant importance in saving lives and alleviating the burden on pathologists.
Results
This paper analyzes the characteristics of histological images of breast cancer and proposes a neural network model named HAHNet that combines multi-scale features with attention mechanisms for HER2 status classification. HAHNet directly classifies the HER2 status from hematoxylin and eosin (H&E) stained histological images, reducing additional costs. It achieves superior performance compared to other computational methods.
Conclusions
According to our experimental results, the proposed HAHNet achieved high performance in classifying the HER2 status of breast cancer using only H&E stained samples. It can be applied in case classification, benefiting the work of pathologists and potentially helping more breast cancer patients."
139,Reply: Correspondence on NanoVar’s performance outlined by Jiang T. et al. in ‘Long-read sequencing settings for efficient structural variation detection based on comprehensive evaluation’,"We published a paper in BMC Bioinformatics comprehensively evaluating the performance of structural variation (SV) calling with long-read SV detection methods based on simulated error-prone long-read data under various sequencing settings. Recently, C.Y.T. et al. wrote a correspondence claiming that the performance of NanoVar was underestimated in our benchmarking and listed some errors in our previous manuscripts. To clarify these matters, we reproduced our previous benchmarking results and carried out a series of parallel experiments on both the newly generated simulated datasets and the ones provided by C.Y.T. et al. The robust benchmark results indicate that NanoVar has unstable performance on simulated data produced from different versions of VISOR, while other tools do not exhibit this phenomenon. Furthermore, the errors proposed by C.Y.T. et al. were due to them using another version of VISOR and Sniffles, which caused many changes in usage and results compared to the versions applied in our previous work. We hope that this commentary proves the validity of our previous publication, clarifies and eliminates the misunderstanding about the commands and results in our benchmarking. Furthermore, we welcome more experts and scholars in the scientific community to pay attention to our research and help us better optimize these valuable works."
140,ECL 3.0: a sensitive peptide identification tool for cross-linking mass spectrometry data analysis,"Background
Cross-linking mass spectrometry (XL-MS) is a powerful technique for detecting protein–protein interactions (PPIs) and modeling protein structures in a high-throughput manner. In XL-MS experiments, proteins are cross-linked by a chemical reagent (namely cross-linker), fragmented, and then fed into a tandem mass spectrum (MS/MS). Cross-linkers are either cleavable or non-cleavable, and each type requires distinct data analysis tools. However, both types of cross-linkers suffer from imbalanced fragmentation efficiency, resulting in a large number of unidentifiable spectra that hinder the discovery of PPIs and protein conformations. To address this challenge, researchers have sought to improve the sensitivity of XL-MS through invention of novel cross-linking reagents, optimization of sample preparation protocols, and development of data analysis algorithms. One promising approach to developing new data analysis methods is to apply a protein feedback mechanism in the analysis. It has significantly improved the sensitivity of analysis methods in the cleavable cross-linking data. The application of the protein feedback mechanism to the analysis of non-cleavable cross-linking data is expected to have an even greater impact because the majority of XL-MS experiments currently employs non-cleavable cross-linkers.
Results
In this study, we applied the protein feedback mechanism to the analysis of both non-cleavable and cleavable cross-linking data and observed a substantial improvement in cross-link spectrum matches (CSMs) compared to conventional methods. Furthermore, we developed a new software program, ECL 3.0, that integrates two algorithms and includes a user-friendly graphical interface to facilitate wider applications of this new program.
Conclusions
ECL 3.0 source code is available at 
https://github.com/yuweichuan/ECL-PF.git
. A quick tutorial is available at 
https://youtu.be/PpZgbi8V2xI
."
141,Correspondence on NanoVar’s performance outlined by Jiang T. et al. in “Long-read sequencing settings for efficient structural variation detection based on comprehensive evaluation”,"A recent paper by Jiang et al. in 
BMC Bioinformatics
 presented guidelines on long-read sequencing settings for structural variation (SV) calling, and benchmarked the performance of various SV calling tools, including NanoVar. In their simulation-based benchmarking, NanoVar was shown to perform poorly compared to other tools, mostly due to low SV recall rates. To investigate the causes for NanoVar's poor performance, we regenerated the simulation datasets (3× to 20×) as specified by Jiang et al. and performed benchmarking for NanoVar and Sniffles. Our results did not reflect the findings described by Jiang et al. In our analysis, NanoVar displayed more than three times the F1 scores and recall rates as reported in Jiang et al. across all sequencing coverages, indicating a previous underestimation of its performance. We also observed that NanoVar outperformed Sniffles in calling SVs with genotype concordance by more than 0.13 in F1 scores, which is contrary to the trend reported by Jiang et al. Besides, we identified multiple detrimental errors encountered during the analysis which were not addressed by Jiang et al. We hope that this commentary clarifies NanoVar's validity as a long-read SV caller and provides assurance to its users and the scientific community."
142,Bulk brain tissue cell-type deconvolution with bias correction for single-nuclei RNA sequencing data using DeTREM,"Background
Quantifying cell-type abundance in bulk tissue RNA-sequencing enables researchers to better understand complex systems. Newer deconvolution methodologies, such as MuSiC, use cell-type signatures derived from single-cell RNA-sequencing (scRNA-seq) data to make these calculations. Single-nuclei RNA-sequencing (snRNA-seq) reference data can be used instead of scRNA-seq data for tissues such as human brain where single-cell data are difficult to obtain, but accuracy suffers due to sequencing differences between the technologies.
Results
We propose a modification to MuSiC entitled ‘DeTREM’ which compensates for sequencing differences between the cell-type signature and bulk RNA-seq datasets in order to better predict cell-type fractions. We show DeTREM to be more accurate than MuSiC in simulated and real human brain bulk RNA-sequencing datasets with various cell-type abundance estimates. We also compare DeTREM to SCDC and CIBERSORTx, two recent deconvolution methods that use scRNA-seq cell-type signatures. We find that they perform well in simulated data but produce less accurate results than DeTREM when used to deconvolute human brain data.
Conclusion
DeTREM improves the deconvolution accuracy of MuSiC and outperforms other deconvolution methods when applied to snRNA-seq data. DeTREM enables accurate cell-type deconvolution in situations where scRNA-seq data are not available. This modification improves characterization cell-type specific effects in brain tissue and identification of cell-type abundance differences under various conditions."
143,Prediction of plant secondary metabolic pathways using deep transfer learning,"Background
Plant secondary metabolites are highly valued for their applications in pharmaceuticals, nutrition, flavors, and aesthetics. It is of great importance to elucidate plant secondary metabolic pathways due to their crucial roles in biological processes during plant growth and development. However, understanding plant biosynthesis and degradation pathways remains a challenge due to the lack of sufficient information in current databases. To address this issue, we proposed a transfer learning approach using a pre-trained hybrid deep learning architecture that combines Graph Transformer and convolutional neural network (GTC) to predict plant metabolic pathways.
Results
GTC provides comprehensive molecular representation by extracting both structural features from the molecular graph and textual information from the SMILES string. GTC is pre-trained on the KEGG datasets to acquire general features, followed by fine-tuning on plant-derived datasets. Four metrics were chosen for model performance evaluation. The results show that GTC outperforms six other models, including three previously reported machine learning models, on the KEGG dataset. GTC yields an accuracy of 96.75%, precision of 85.14%, recall of 83.03%, and F1_score of 84.06%. Furthermore, an ablation study confirms the indispensability of all the components of the hybrid GTC model. Transfer learning is then employed to leverage the shared knowledge acquired from the KEGG metabolic pathways. As a result, the transferred GTC exhibits outstanding accuracy in predicting plant secondary metabolic pathways with an average accuracy of 98.30% in fivefold cross-validation and 97.82% on the final test. In addition, GTC is employed to classify natural products. It achieves a perfect accuracy score of 100.00% for alkaloids, while the lowest accuracy score of 98.42% for shikimates and phenylpropanoids.
Conclusions
The proposed GTC effectively captures molecular features, and achieves high performance in classifying KEGG metabolic pathways and predicting plant secondary metabolic pathways via transfer learning. Furthermore, GTC demonstrates its generalization ability by accurately classifying natural products. A user-friendly executable program has been developed, which only requires the input of the SMILES string of the query compound in a graphical interface."
144,An unsupervised deep learning framework for predicting human essential genes from population and functional genomic data,"Background
The ability to accurately predict essential genes intolerant to loss-of-function (LOF) mutations can dramatically improve the identification of disease-associated genes. Recently, there have been numerous computational methods developed to predict human essential genes from population genomic data. While the existing methods are highly predictive of essential genes of long length, they have limited power in pinpointing short essential genes due to the sparsity of polymorphisms in the human genome.
Results
Motivated by the premise that population and functional genomic data may provide complementary evidence for gene essentiality, here we present an evolution-based deep learning model, DeepLOF, to predict essential genes in an unsupervised manner. Unlike previous population genetic methods, DeepLOF utilizes a novel deep learning framework to integrate both population and functional genomic data, allowing us to pinpoint short essential genes that can hardly be predicted from population genomic data alone. Compared with previous methods, DeepLOF shows unmatched performance in predicting ClinGen haploinsufficient genes, mouse essential genes, and essential genes in human cell lines. Notably, at a false positive rate of 5%, DeepLOF detects 50% more ClinGen haploinsufficient genes than previous methods. Furthermore, DeepLOF discovers 109 novel essential genes that are too short to be identified by previous methods.
Conclusion
The predictive power of DeepLOF shows that it is a compelling computational method to aid in the discovery of essential genes."
145,Automatic segmentation of large-scale CT image datasets for detailed body composition analysis,"Background
Body composition (BC) is an important factor in determining the risk of type 2-diabetes and cardiovascular disease. Computed tomography (CT) is a useful imaging technique for studying BC, however manual segmentation of CT images is time-consuming and subjective. The purpose of this study is to develop and evaluate fully automated segmentation techniques applicable to a 3-slice CT imaging protocol, consisting of single slices at the level of the liver, abdomen, and thigh, allowing detailed analysis of numerous tissues and organs.
Methods
The study used more than 4000 CT subjects acquired from the large-scale SCAPIS and IGT cohort to train and evaluate four convolutional neural network based architectures: ResUNET, UNET++, Ghost-UNET, and the proposed Ghost-UNET++. The segmentation techniques were developed and evaluated for automated segmentation of the liver, spleen, skeletal muscle, bone marrow, cortical bone, and various adipose tissue depots, including visceral (VAT), intraperitoneal (IPAT), retroperitoneal (RPAT), subcutaneous (SAT), deep (DSAT), and superficial SAT (SSAT), as well as intermuscular adipose tissue (IMAT). The models were trained and validated for each target using tenfold cross-validation and test sets.
Results
The Dice scores on cross validation in SCAPIS were: ResUNET 0.964 (0.909–0.996), UNET++ 0.981 (0.927–0.996), Ghost-UNET 0.961 (0.904–0.991), and Ghost-UNET++ 0.968 (0.910–0.994). All four models showed relatively strong results, however UNET++ had the best performance overall. Ghost-UNET++ performed competitively compared to UNET++ and showed a more computationally efficient approach.
Conclusion
Fully automated segmentation techniques can be successfully applied to a 3-slice CT imaging protocol to analyze multiple tissues and organs related to BC. The overall best performance was achieved by UNET++, against which Ghost-UNET++ showed competitive results based on a more computationally efficient approach. The use of fully automated segmentation methods can reduce analysis time and provide objective results in large-scale studies of BC."
146,DeepCAC: a deep learning approach on DNA transcription factors classification based on multi-head self-attention and concatenate convolutional neural network,"Understanding gene expression processes necessitates the accurate classification and identification of transcription factors, which is supported by high-throughput sequencing technologies. However, these techniques suffer from inherent limitations such as time consumption and high costs. To address these challenges, the field of bioinformatics has increasingly turned to deep learning technologies for analyzing gene sequences. Nevertheless, the pursuit of improved experimental results has led to the inclusion of numerous complex analysis function modules, resulting in models with a growing number of parameters. To overcome these limitations, it is proposed a novel approach for analyzing DNA transcription factor sequences, which is named as DeepCAC. This method leverages deep convolutional neural networks with a multi-head self-attention mechanism. By employing convolutional neural networks, it can effectively capture local hidden features in the sequences. Simultaneously, the multi-head self-attention mechanism enhances the identification of hidden features with long-distant dependencies. This approach reduces the overall number of parameters in the model while harnessing the computational power of sequence data from multi-head self-attention. Through training with labeled data, experiments demonstrate that this approach significantly improves performance while requiring fewer parameters compared to existing methods. Additionally, the effectiveness of our approach  is validated in accurately predicting DNA transcription factor sequences."
147,MAVEN: compound mechanism of action analysis and visualisation using transcriptomics and compound structure data in R/Shiny,"Background
Understanding the Mechanism of Action (MoA) of a compound is an often challenging but equally crucial aspect of drug discovery that can help improve both its efficacy and safety. Computational methods to aid MoA elucidation usually either aim to predict direct drug targets, or attempt to understand modulated downstream pathways or signalling proteins. Such methods usually require extensive coding experience and results are often optimised for further computational processing, making them difficult for wet-lab scientists to perform, interpret and draw hypotheses from.
Results
To address this issue, we in this work present MAVEN (Mechanism of Action Visualisation and Enrichment), an R/Shiny app which allows for GUI-based prediction of drug targets based on chemical structure, combined with causal reasoning based on causal protein–protein interactions and transcriptomic perturbation signatures. The app computes a systems-level view of the mechanism of action of the input compound. This is visualised as a sub-network linking predicted or known targets to modulated transcription factors via inferred signalling proteins. The tool includes a selection of MSigDB gene set collections to perform pathway enrichment on the resulting network, and also allows for custom gene sets to be uploaded by the researcher. MAVEN is hence a user-friendly, flexible tool for researchers without extensive bioinformatics or cheminformatics knowledge to generate interpretable hypotheses of compound Mechanism of Action.
Conclusions
MAVEN is available as a fully open-source tool at 
https://github.com/laylagerami/MAVEN
 with options to install in a Docker or Singularity container. Full documentation, including a tutorial on example data, is available at 
https://laylagerami.github.io/MAVEN
."
148,BG2: Bayesian variable selection in generalized linear mixed models with nonlocal priors for non-Gaussian GWAS data,"Background
Genome-wide association studies (GWASes) aim to identify single nucleotide polymorphisms (SNPs) associated with a given phenotype. A common approach for the analysis of GWAS is single marker analysis (SMA) based on linear mixed models (LMMs). However, LMM-based SMA usually yields a large number of false discoveries and cannot be directly applied to non-Gaussian phenotypes such as count data.
Results
We present a novel Bayesian method to find SNPs associated with non-Gaussian phenotypes. To that end, we use generalized linear mixed models (GLMMs) and, thus, call our method Bayesian GLMMs for GWAS (BG2). To deal with the high dimensionality of GWAS analysis, we propose novel nonlocal priors specifically tailored for GLMMs. In addition, we develop related fast approximate Bayesian computations. BG2 uses a two-step procedure: first, BG2 screens for candidate SNPs; second, BG2 performs model selection that considers all screened candidate SNPs as possible regressors. A simulation study shows favorable performance of BG2 when compared to GLMM-based SMA. We illustrate the usefulness and flexibility of BG2 with three case studies on cocaine dependence (binary data), alcohol consumption (count data), and number of root-like structures in a model plant (count data)."
149,Scellpam: an R package/C++ library to perform parallel partitioning around medoids on scRNAseq data sets,"Background
 Partitioning around medoids (PAM) is one of the most widely used and successful clustering method in many fields. One of its key advantages is that it only requires a distance or a dissimilarity between the individuals, and the fact that cluster centers are actual points in the data set means they can be taken as reliable representatives of their classes. However, its wider application is hampered by the large amount of memory needed to store the distance matrix (quadratic on the number of individuals) and also by the high computational cost of computing such distance matrix and, less importantly, by the cost of the clustering algorithm itself.
Results
 Therefore, new software has been provided that addresses these issues. This software, provided under GPL license and usable as either an R package or a C++ library, calculates in parallel the distance matrix for different distances/dissimilarities (
\(L_1\)
, 
\(L_2\)
, Pearson, cosine and weighted Euclidean) and also implements a parallel fast version of PAM (FASTPAM1) using any data type to reduce memory usage. Moreover, the parallel implementation uses all the cores available in modern computers which greatly reduces the execution time. Besides its general application, the software is especially useful for processing data of single cell experiments. It has been tested in problems including clustering of single cell experiments with up to 289,000 cells with the expression of about 29,000 genes per cell.
Conclusions
 Comparisons with other current packages in terms of execution time have been made. The method greatly outperforms the available R packages for distance matrix calculation and also improves the packages that implement the PAM itself. The software is available as an R package at 
https://CRAN.R-project.org/package=scellpam
 and as C++ libraries at 
https://github.com/JdMDE/jmatlib
 and 
https://github.com/JdMDE/ppamlib
 The package is useful for single cell RNA-seq studies but it is also applicable in other contexts where clustering of large data sets is required."
150,A systematic comparison of human mitochondrial genome assembly tools,"Background
Mitochondria are the cell organelles that produce most of the chemical energy required to power the cell's biochemical reactions. Despite being a part of a eukaryotic host cell, the mitochondria contain a separate genome whose origin is linked with the endosymbiosis of a prokaryotic cell by the host cell and encode independent genomic information throughout their genomes. Mitochondrial genomes accommodate essential genes and are regularly utilized in biotechnology and phylogenetics. Various assemblers capable of generating complete mitochondrial genomes are being continuously developed. These tools often use whole-genome sequencing data as an input containing reads from the mitochondrial genome. Till now, no published work has explored the systematic comparison of all the available tools for assembling human mitochondrial genomes using short-read sequencing data. This evaluation is required to identify the best tool that can be well-optimized for small-scale projects or even national-level research.
Results
In this study, we have tested the mitochondrial genome assemblers for both simulated datasets and whole genome sequencing (WGS) datasets of humans. For the highest computational setting of 16 computational threads with the simulated dataset having 1000X read depth, MitoFlex took the least execution time of 69 s, and IOGA took the longest execution time of 1278 s. NOVOPlasty utilized the least computational memory of approximately 0.098 GB for the same setting, whereas IOGA utilized the highest computational memory of 11.858 GB. In the case of WGS datasets for humans, GetOrganelle and MitoFlex performed the best in capturing the SNPs information with a mean F1-score of 0.919 at the sequencing depth of 10X. MToolBox and NOVOPlasty performed consistently across all sequencing depths with a mean F1 score of 0.897 and 0.890, respectively.
Conclusions
Based on the overall performance metrics and consistency in assembly quality for all sequencing data, MToolBox performed the best. However, NOVOPlasty was the second fastest tool in execution time despite being single-threaded, and it utilized the least computational resources among all the assemblers when tested on simulated datasets. Therefore, NOVOPlasty may be more practical when there is a significant sample size and a lack of computational resources. Besides, as long-read sequencing gains popularity, mitochondrial genome assemblers must be developed to use long-read sequencing data."
151,"BiocMAP: a Bioconductor-friendly, GPU-accelerated pipeline for bisulfite-sequencing data","Background
Bisulfite sequencing is a powerful tool for profiling genomic methylation, an epigenetic modification critical in the understanding of cancer, psychiatric disorders, and many other conditions. Raw data generated by whole genome bisulfite sequencing (WGBS) requires several computational steps before it is ready for statistical analysis, and particular care is required to process data in a timely and memory-efficient manner. Alignment to a reference genome is one of the most computationally demanding steps in a WGBS workflow, taking several hours or even days with commonly used WGBS-specific alignment software. This naturally motivates the creation of computational workflows that can utilize GPU-based alignment software to greatly speed up the bottleneck step. In addition, WGBS produces raw data that is large and often unwieldy; a lack of memory-efficient representation of data by existing pipelines renders WGBS impractical or impossible to many researchers.
Results
We present BiocMAP, a Bioconductor-friendly methylation analysis pipeline consisting of two modules, to address the above concerns. The first module performs computationally-intensive read alignment using 
Arioc
, a GPU-accelerated short-read aligner. Since GPUs are not always available on the same computing environments where traditional CPU-based analyses are convenient, the second module may be run in a GPU-free environment. This module extracts and merges DNA methylation proportions—the fractions of methylated cytosines across all cells in a sample at a given genomic site. Bioconductor-based output objects in R utilize an on-disk data representation to drastically reduce required main memory and make WGBS projects computationally feasible to more researchers.
Conclusions
BiocMAP is implemented using Nextflow and available at 
http://research.libd.org/BiocMAP/
. To enable reproducible analysis across a variety of typical computing environments, BiocMAP can be containerized with Docker or Singularity, and executed locally or with the SLURM or SGE scheduling engines. By providing Bioconductor objects, BiocMAP’s output can be integrated with powerful analytical open source software for analyzing methylation data."
152,An approach for proteins and their encoding genes synonyms integration based on protein ontology,"Background
Biological research is generating high volumes of data distributed across various sources. The inconsistent naming of proteins and their encoding genes brings great challenges to protein data integration: proteins and their coding genes usually have multiple related names and notations, which are difficult to match absolutely; the nomenclature of genes and proteins is complex and varies from species to species; some less studied species have no nomenclature of genes and proteins; The annotation of the same protein/gene varies greatly in different databases. In summary, a comprehensive set of protein/gene synonyms is necessary for relevant studies.
Results
In this study, we propose an approach for protein and its encoding gene synonym integration based on protein ontology. The workflow of protein and gene synonym integration is composed of three modules: data acquisition, entity and attribute alignment, attribute integration and deduplication. Finally, the integrated synonym set of proteins and their coding genes contains over 128.59 million terminologies covering 560,275 proteins/genes and 13,781 species. As the semantic basis, the comprehensive synonym set was used to develop a data platform to provide one-stop data retrieval without considering the diversity of protein nomenclature and species.
Conclusion
The synonym set constructed here can serve as an important resource for biological named entity identification, text mining and information retrieval without name ambiguity, especially synonyms associated with well-defined species categories can help to study the evolutionary relationships between species at the molecular level. More importantly, the comprehensive synonyms set is the semantic basis for our subsequent studies on Protein–protein Interaction (PPI) knowledge graph."
153,Prediction of the effects of small molecules on the gut microbiome using machine learning method integrating with optimal molecular features,"Background
The human gut microbiome (HGM), consisting of trillions of microorganisms, is crucial to human health. Adverse drug use is one of the most important causes of HGM disorder. Thus, it is necessary to identify drugs or compounds with anti-commensal effects on HGM in the early drug discovery stage. This study proposes a novel anti-commensal effects classification using a machine learning method and optimal molecular features. To improve the prediction performance, we explored combinations of six fingerprints and three descriptors to filter the best characterization as molecular features.
Results
The final consensus model based on optimal features yielded the F1-score of 0.725 ± 0.014, ACC of 82.9 ± 0.7%, and AUC of 0.791 ± 0.009 for five-fold cross-validation. In addition, this novel model outperformed the prior studies by using the same algorithm. Furthermore, the important chemical descriptors and misclassified anti-commensal compounds are analyzed to better understand and interpret the model. Finally, seven structural alerts responsible for the chemical anti-commensal effect are identified, implying valuable information for drug design.
Conclusion
Our study would be a promising tool for screening anti-commensal compounds in the early stage of drug discovery and assessing the potential risks of these drugs in vivo."
154,Prediction of diabetes disease using an ensemble of machine learning multi-classifier models,"Background and objective
Diabetes is a life-threatening chronic disease with a growing global prevalence, necessitating early diagnosis and treatment to prevent severe complications. Machine learning has emerged as a promising approach for diabetes diagnosis, but challenges such as limited labeled data, frequent missing values, and dataset imbalance hinder the development of accurate prediction models. Therefore, a novel framework is required to address these challenges and improve performance.
Methods
In this study, we propose an innovative pipeline-based multi-classification framework to predict diabetes in three classes: diabetic, non-diabetic, and prediabetes, using the imbalanced Iraqi Patient Dataset of Diabetes. Our framework incorporates various pre-processing techniques, including duplicate sample removal, attribute conversion, missing value imputation, data normalization and standardization, feature selection, and k-fold cross-validation. Furthermore, we implement multiple machine learning models, such as k-NN, SVM, DT, RF, AdaBoost, and GNB, and introduce a weighted ensemble approach based on the Area Under the Receiver Operating Characteristic Curve (AUC) to address dataset imbalance. Performance optimization is achieved through grid search and Bayesian optimization for hyper-parameter tuning.
Results
Our proposed model outperforms other machine learning models, including k-NN, SVM, DT, RF, AdaBoost, and GNB, in predicting diabetes. The model achieves high average accuracy, precision, recall, F1-score, and AUC values of 0.9887, 0.9861, 0.9792, 0.9851, and 0.999, respectively.
Conclusion
Our pipeline-based multi-classification framework demonstrates promising results in accurately predicting diabetes using an imbalanced dataset of Iraqi diabetic patients. The proposed framework addresses the challenges associated with limited labeled data, missing values, and dataset imbalance, leading to improved prediction performance. This study highlights the potential of machine learning techniques in diabetes diagnosis and management, and the proposed framework can serve as a valuable tool for accurate prediction and improved patient care. Further research can build upon our work to refine and optimize the framework and explore its applicability in diverse datasets and populations."
155,"Rin
maker
: a fast, versatile and reliable tool to determine residue interaction networks in proteins","Background
Residue Interaction Networks (RINs) map the crystallographic description of a protein into a graph, where amino acids are represented as nodes and non-covalent bonds as edges. Determination and visualization of a protein as a RIN provides insights on the topological properties (and hence their related biological functions) of large proteins without dealing with the full complexity of the three-dimensional description, and hence it represents an invaluable tool of modern bioinformatics.
Results
We present RIN
maker
, a fast, flexible, and powerful tool for determining and visualizing RINs that include all standard non-covalent interactions. RIN
maker
 is offered as a cross-platform and open source software that can be used either as a command-line tool or through a web application or a web API service. We benchmark its efficiency against the main alternatives and provide explicit tests to show its performance and its correctness.
Conclusions
RIN
maker
 is designed to be fully customizable, from a simple and handy support for experimental research to a sophisticated computational tool that can be embedded into a large computational pipeline. Hence, it paves the way to bridge the gap between data-driven/machine learning approaches and numerical simulations of simple, physically motivated, models."
156,HMCDA: a novel method based on the heterogeneous graph neural network and metapath for circRNA-disease associations prediction,"Circular RNA (CircRNA) is a type of non-coding RNAs in which both ends are covalently linked. Researchers have demonstrated that many circRNAs can act as biomarkers of diseases. However, traditional experimental methods for circRNA-disease associations identification are labor-intensive. In this work, we propose a novel method based on the heterogeneous graph neural network and metapaths for circRNA-disease associations prediction termed as HMCDA. First, a heterogeneous graph consisting of circRNA-disease associations, circRNA-miRNA associations, miRNA-disease associations and disease-disease associations are constructed. Then, six metapaths are defined and generated according to the biomedical pathways. Afterwards, the entity content transformation, intra-metapath and inter-metapath aggregation are implemented to learn the embeddings of circRNA and disease entities. Finally, the learned embeddings are used to predict novel circRNA-disase associations. In particular, the result of extensive experiments demonstrates that HMCDA outperforms four state-of-the-art models in fivefold cross validation. In addition, our case study indicates that HMCDA has the ability to identify novel circRNA-disease associations."
157,SubMDTA: drug target affinity prediction based on substructure extraction and multi-scale features,"Background
Drug–target affinity (DTA) prediction is a critical step in the field of drug discovery. In recent years, deep learning-based methods have emerged for DTA prediction. In order to solve the problem of fusion of substructure information of drug molecular graphs and utilize multi-scale information of protein, a self-supervised pre-training model based on substructure extraction and multi-scale features is proposed in this paper.
Results
For drug molecules, the model obtains substructure information through the method of probability matrix, and the contrastive learning method is implemented on the graph-level representation and subgraph-level representation to pre-train the graph encoder for downstream tasks. For targets, a BiLSTM method that integrates multi-scale features is used to capture long-distance relationships in the amino acid sequence. The experimental results showed that our model achieved better performance for DTA prediction.
Conclusions
The proposed model improves the performance of the DTA prediction, which provides a novel strategy based on substructure extraction and multi-scale features."
158,IHCP: interpretable hepatitis C prediction system based on black-box machine learning models,"Background
Hepatitis C is a prevalent disease that poses a high risk to the human liver. Early diagnosis of hepatitis C is crucial for treatment and prognosis. Therefore, developing an effective medical decision system is essential. In recent years, many computational methods have been proposed to identify hepatitis C patients. Although existing hepatitis prediction models have achieved good results in terms of accuracy, most of them are black-box models and cannot gain the trust of doctors and patients in clinical practice. As a result, this study aims to use various Machine Learning (ML) models to predict whether a patient has hepatitis C, while also using explainable models to elucidate the prediction process of the ML models, thus making the prediction process more transparent.
Result
We conducted a study on the prediction of hepatitis C based on serological testing and provided comprehensive explanations for the prediction process. Throughout the experiment, we modeled the benchmark dataset, and evaluated model performance using fivefold cross-validation and independent testing experiments. After evaluating three types of black-box machine learning models, Random Forest (RF), Support Vector Machine (SVM), and AdaBoost, we adopted Bayesian-optimized RF as the classification algorithm. In terms of model interpretation, in addition to using common SHapley Additive exPlanations (SHAP) to provide global explanations for the model, we also utilized the Local Interpretable Model-Agnostic Explanations with stability (LIME_stabilitly) to provide local explanations for the model.
Conclusion
Both the fivefold cross-validation and independent testing show that our proposed method significantly outperforms the state-of-the-art method. IHCP maintains excellent model interpretability while obtaining excellent predictive performance. This helps uncover potential predictive patterns of the model and enables clinicians to better understand the model's decision-making process."
159,Automatically transferring supervised targets method for segmenting lung lesion regions with CT imaging,"Background
To present an approach that autonomously identifies and selects a self-selective optimal target for the purpose of enhancing learning efficiency to segment infected regions of the lung from chest computed tomography images. We designed a semi-supervised dual-branch framework for training, where the training set consisted of limited expert-annotated data and a large amount of coarsely annotated data that was automatically segmented based on Hu values, which were used to train both strong and weak branches. In addition, we employed the Lovasz scoring method to automatically switch the supervision target in the weak branch and select the optimal target as the supervision object for training. This method can use noisy labels for rapid localization during the early stages of training, and gradually use more accurate targets for supervised training as the training progresses. This approach can utilize a large number of samples that do not require manual annotation, and with the iterations of training, the supervised targets containing noise become closer and closer to the fine-annotated data, which significantly improves the accuracy of the final model.
Results
The proposed dual-branch deep learning network based on semi-supervision together with cost-effective samples achieved 83.56 ± 12.10 and 82.67 ± 8.04 on our internal and external test benchmarks measured by the mean Dice similarity coefficient (DSC). Through experimental comparison, the DSC value of the proposed algorithm was improved by 13.54% and 2.02% on the internal benchmark and 13.37% and 2.13% on the external benchmark compared with U-Net without extra sample assistance and the mean-teacher frontier algorithm, respectively.
Conclusion
The cost-effective pseudolabeled samples assisted the training of DL models and achieved much better results compared with traditional DL models with manually labeled samples only. Furthermore, our method also achieved the best performance compared with other up-to-date dual branch structures."
160,Empirical methods for the validation of time-to-event mathematical models taking into account uncertainty and variability: application to EGFR + lung adenocarcinoma,"Background
Over the past several decades, metrics have been defined to assess the quality of various types of models and to compare their performance depending on their capacity to explain the variance found in real-life data. However, available validation methods are mostly designed for statistical regressions rather than for mechanistic models. To our knowledge, in the latter case, there are no consensus standards, for instance for the validation of predictions against real-world data given the variability and uncertainty of the data. In this work, we focus on the prediction of time-to-event curves using as an application example a mechanistic model of non-small cell lung cancer. We designed four empirical methods to assess both model performance and reliability of predictions: two methods based on bootstrapped versions of parametric statistical tests: log-rank and combined weighted log-ranks (MaxCombo); and two methods based on bootstrapped prediction intervals, referred to here as raw coverage and the juncture metric. We also introduced the notion of observation time uncertainty to take into consideration the real life delay between the moment when an event happens, and the moment when it is observed and reported.
Results
We highlight the advantages and disadvantages of these methods according to their application context. We have shown that the context of use of the model has an impact on the model validation process. Thanks to the use of several validation metrics we have highlighted the limit of the model to predict the evolution of the disease in the whole population of mutations at the same time, and that it was more efficient with specific predictions in the target mutation populations. The choice and use of a single metric could have led to an erroneous validation of the model and its context of use.
Conclusions
With this work, we stress the importance of making judicious choices for a metric, and how using a combination of metrics could be more relevant, with the objective of validating a given model and its predictions within a specific context of use. We also show how the reliability of the results depends both on the metric and on the statistical comparisons, and that the conditions of application and the type of available information need to be taken into account to choose the best validation strategy."
161,Acorn: an R package for de novo variant analysis,"Background
The study of de novo variation is important for assessing biological characteristics of new variation and for studies related to human phenotypes. Software programs exist to call de novo variants and programs also exist to test the burden of these variants in genomic regions; however, I am unaware of a program that fits in between these two aspects of de novo variant assessment. This intermediate space is important for assessing the quality of de novo variants and to understand the characteristics of the callsets. For this reason, I developed an R package called acorn.
Results
Acorn is an R package that examines various features of de novo variants including subsetting the data by individual(s), variant type, or genomic region; calculating features including variant change counts, variant lengths, and presence/absence at CpG sites; and characteristics of parental age in relation to de novo variant counts.
Conclusions
Acorn is an R package that fills a critical gap in assessing de novo variants and will be of benefit to many investigators studying de novo variation."
162,AUD-DSS: a decision support system for early detection of patients with alcohol use disorder,"Background
Alcohol use disorder (AUD) causes significant morbidity, mortality, and injuries. According to reports, approximately 5% of all registered deaths in Denmark could be due to AUD. The problem is compounded by the late identification of patients with AUD, a situation that can cause enormous problems, from psychological to physical to economic problems. Many individuals suffering from AUD never undergo specialist treatment during their addiction due to obstacles such as taboo and the poor performance of current screening tools. Therefore, there is a lack of rapid intervention. This can be mitigated by the early detection of patients with AUD. A clinical decision support system (DSS) powered by machine learning (ML) methods can be used to diagnose patients’ AUD status earlier.
Methods
This study proposes an effective AUD prediction model (AUDPM), which can be used in a DSS. The proposed model consists of four distinct components: (1) imputation to address missing values using the k-nearest neighbours approach, (2) recursive feature elimination with cross validation to select the most relevant subset of features, (3) a hybrid synthetic minority oversampling technique-edited nearest neighbour approach to remove noise and balance the distribution of the training data, and (4) an ML model for the early detection of patients with AUD.
Two data sources, including a questionnaire and electronic health records of 2571 patients, were collected from Odense University Hospital in the Region of Southern Denmark for the AUD-Dataset. Then, the AUD-Dataset was used to build ML models. The results of different ML models, such as support vector machine, K-nearest neighbour, decision tree, random forest, and extreme gradient boosting, were compared. Finally, a combination of all these models in an ensemble learning approach was selected for the AUDPM.
Results
The results revealed that the proposed ensemble AUDPM outperformed other single models and our previous study results, achieving 0.96, 0.94, 0.95, and 0.97 precision, recall, F1-score, and accuracy, respectively. In addition, we designed and developed an AUD-DSS prototype.
Conclusion
It was shown that our proposed AUDPM achieved high classification performance. In addition, we identified clinical factors related to the early detection of patients with AUD. The designed AUD-DSS is intended to be integrated into the existing Danish health care system to provide novel information to clinical staff if a patient shows signs of harmful alcohol use; in other words, it gives staff a good reason for having a conversation with patients for whom a conversation is relevant."
163,Empirical evaluation of language modeling to ascertain cancer outcomes from clinical text reports,"Background
Longitudinal data on key cancer outcomes for clinical research, such as response to treatment and disease progression, are not captured in standard cancer registry reporting. Manual extraction of such outcomes from unstructured electronic health records is a slow, resource-intensive process. Natural language processing (NLP) methods can accelerate outcome annotation, but they require substantial labeled data. Transfer learning based on language modeling, particularly using the Transformer architecture, has achieved improvements in NLP performance. However, there has been no systematic evaluation of NLP model training strategies on the extraction of cancer outcomes from unstructured text.
Results
We evaluated the performance of nine NLP models at the two tasks of identifying cancer response and cancer progression within imaging reports at a single academic center among patients with non-small cell lung cancer. We trained the classification models under different conditions, including training sample size, classification architecture, and language model pre-training. The training involved a labeled dataset of 14,218 imaging reports for 1112 patients with lung cancer. A subset of models was based on a pre-trained language model, DFCI-ImagingBERT, created by further pre-training a BERT-based model using an unlabeled dataset of 662,579 reports from 27,483 patients with cancer from our center. A classifier based on our DFCI-ImagingBERT, trained on more than 200 patients, achieved the best results in most experiments; however, these results were marginally better than simpler “bag of words” or convolutional neural network models.
Conclusion
When developing AI models to extract outcomes from imaging reports for clinical cancer research, if computational resources are plentiful but labeled training data are limited, large language models can be used for zero- or few-shot learning to achieve reasonable performance. When computational resources are more limited but labeled training data are readily available, even simple machine learning architectures can achieve good performance for such tasks."
164,Galba: genome annotation with miniprot and AUGUSTUS,"Background
The Earth Biogenome Project has rapidly increased the number of available eukaryotic genomes, but most released genomes continue to lack annotation of protein-coding genes. In addition, no transcriptome data is available for some genomes.
Results
Various gene annotation tools have been developed but each has its limitations. Here, we introduce GALBA, a fully automated pipeline that utilizes miniprot, a rapid protein-to-genome aligner, in combination with AUGUSTUS to predict genes with high accuracy. Accuracy results indicate that GALBA is particularly strong in the annotation of large vertebrate genomes. We also present use cases in insects, vertebrates, and a land plant. GALBA is fully open source and available as a docker image for easy execution with Singularity in high-performance computing environments.
Conclusions
Our pipeline addresses the critical need for accurate gene annotation in newly sequenced genomes, and we believe that GALBA will greatly facilitate genome annotation for diverse organisms."
165,scSNPdemux: a sensitive demultiplexing pipeline using single nucleotide polymorphisms for improved pooled single-cell RNA sequencing analysis,"Background
Here we present scSNPdemux, a sample demultiplexing pipeline for single-cell RNA sequencing data using natural genetic variations in humans. The pipeline requires alignment files from Cell Ranger (10× Genomics), a population SNP database and genotyped single nucleotide polymorphisms (SNPs) per sample. The tool works on sparse genotyping data in VCF format for sample identification.
Results
The pipeline was tested on both single-cell and single-nuclei based RNA sequencing datasets and showed superior demultiplexing performance over the lipid-based CellPlex and Multi-seq sample multiplexing technique which incurs additional single cell library preparation steps. Specifically, our pipeline demonstrated superior sensitivity and specificity in cell-identity assignment over CellPlex, especially on immune cell types with low RNA content.
Conclusions
We designed a streamlined pipeline for single-cell sample demultiplexing, aiming to overcome common problems in multiplexing samples using single cell libraries which might affect data quality and can be costly."
166,EDST: a decision stump based ensemble algorithm for synergistic drug combination prediction,"Introduction
There are countless possibilities for drug combinations, which makes it expensive and time-consuming to rely solely on clinical trials to determine the effects of each possible drug combination. In order to screen out the most effective drug combinations more quickly, scholars began to apply machine learning to drug combination prediction. However, most of them are of low interpretability. Consequently, even though they can sometimes produce high prediction accuracy, experts in the medical and biological fields can still not fully rely on their judgments because of the lack of knowledge about the decision-making process.
Related work
Decision trees and their ensemble algorithms are considered to be suitable methods for pharmaceutical applications due to their excellent performance and good interpretability. We review existing decision trees or decision tree ensemble algorithms in the medical field and point out their shortcomings.
Method
This study proposes a decision stump (DS)-based solution to extract interpretable knowledge from data sets. In this method, a set of DSs is first generated to selectively form a decision tree (DST). Different from the traditional decision tree, our algorithm not only enables a partial exchange of information between base classifiers by introducing a stump exchange method but also uses a modified Gini index to evaluate stump performance so that the generation of each node is evaluated by a global view to maintain high generalization ability. Furthermore, these trees are combined to construct an ensemble of DST (EDST).
Experiment
The two-drug combination data sets are collected from two cell lines with three classes (additive, antagonistic and synergistic effects) to test our method. Experimental results show that both our DST and EDST perform better than other methods. Besides, the rules generated by our methods are more compact and more accurate than other rule-based algorithms. Finally, we also analyze the extracted knowledge by the model in the field of bioinformatics.
Conclusion
The novel decision tree ensemble model can effectively predict the effect of drug combination datasets and easily obtain the decision-making process."
167,A knowledge graph approach to predict and interpret disease-causing gene interactions,"Background
Understanding the impact of gene interactions on disease phenotypes is increasingly recognised as a crucial aspect of genetic disease research. This trend is reflected by the growing amount of clinical research on oligogenic diseases, where disease manifestations are influenced by combinations of variants on a few specific genes. Although statistical machine-learning methods have been developed to identify relevant genetic variant or gene combinations associated with oligogenic diseases, they rely on abstract features and black-box models, posing challenges to interpretability for medical experts and impeding their ability to comprehend and validate predictions. In this work, we present a novel, interpretable predictive approach based on a knowledge graph that not only provides accurate predictions of disease-causing gene interactions but also offers explanations for these results.
Results
We introduce BOCK, a knowledge graph constructed to explore disease-causing genetic interactions, integrating curated information on oligogenic diseases from clinical cases with relevant biomedical networks and ontologies. Using this graph, we developed a novel predictive framework based on heterogenous paths connecting gene pairs. This method trains an interpretable decision set model that not only accurately predicts pathogenic gene interactions, but also unveils the patterns associated with these diseases. A unique aspect of our approach is its ability to offer, along with each positive prediction, explanations in the form of subgraphs, revealing the specific entities and relationships that led to each pathogenic prediction.
Conclusion
Our method, built with interpretability in mind, leverages heterogenous path information in knowledge graphs to predict pathogenic gene interactions and generate meaningful explanations. This not only broadens our understanding of the molecular mechanisms underlying oligogenic diseases, but also presents a novel application of knowledge graphs in creating more transparent and insightful predictors for genetic research."
168,MCL-DTI: using drug multimodal information and bi-directional cross-attention learning method for predicting drug–target interaction,"Background
Prediction of drug–target interaction (DTI) is an essential step for drug discovery and drug reposition. Traditional methods are mostly time-consuming and labor-intensive, and deep learning-based methods address these limitations and are applied to engineering. Most of the current deep learning methods employ representation learning of unimodal information such as SMILES sequences, molecular graphs, or molecular images of drugs. In addition, most methods focus on feature extraction from drug and target alone without fusion learning from drug–target interacting parties, which may lead to insufficient feature representation.
Motivation
In order to capture more comprehensive drug features, we utilize both molecular image and chemical features of drugs. The image of the drug mainly has the structural information and spatial features of the drug, while the chemical information includes its functions and properties, which can complement each other, making drug representation more effective and complete. Meanwhile, to enhance the interactive feature learning of drug and target, we introduce a bidirectional multi-head attention mechanism to improve the performance of DTI.
Results
To enhance feature learning between drugs and targets, we propose a novel model based on deep learning for DTI task called MCL-DTI which uses multimodal information of drug and learn the representation of drug–target interaction for drug–target prediction. In order to further explore a more comprehensive representation of drug features, this paper first exploits two multimodal information of drugs, molecular image and chemical text, to represent the drug. We also introduce to use bi-rectional multi-head corss attention (MCA) method to learn the interrelationships between drugs and targets. Thus, we build two decoders, which include an multi-head self attention (MSA) block and an MCA block, for cross-information learning. We use a decoder for the drug and target separately to obtain the interaction feature maps. Finally, we feed these feature maps generated by decoders into a fusion block for feature extraction and output the prediction results.
Conclusions
MCL-DTI achieves the best results in all the three datasets: Human, 
C. elegans
 and Davis, including the balanced datasets and an unbalanced dataset. The results on the drug–drug interaction (DDI) task show that MCL-DTI has a strong generalization capability and can be easily applied to other tasks."
169,Haplotype based testing for a better understanding of the selective architecture,"Background
The identification of genomic regions affected by selection is one of the most important goals in population genetics. If temporal data are available, allele frequency changes at SNP positions are often used for this purpose. Here we provide a new testing approach that uses haplotype frequencies instead of allele frequencies.
Results
Using simulated data, we show that compared to SNP based test, our approach has higher power, especially when the number of candidate haplotypes is small or moderate. To improve power when the number of haplotypes is large, we investigate methods to combine them with a moderate number of haplotype subsets. Haplotype frequencies can often be recovered with less noise than SNP frequencies, especially under pool sequencing, giving our test an additional advantage. Furthermore, spurious outlier SNPs may lead to false positives, a problem usually not encountered when working with haplotypes. Post hoc tests for the number of selected haplotypes and for differences between their selection coefficients are also provided for a better understanding of the underlying selection dynamics. An application on a real data set further illustrates the performance benefits.
Conclusions
Due to less multiple testing correction and noise reduction, haplotype based testing is able to outperform SNP based tests in terms of power in most scenarios."
170,Predicting weighted unobserved nodes in a regulatory network using answer set programming,"Background
The impact of a perturbation, over-expression, or repression of a key node on an organism, can be modelled based on a regulatory and/or metabolic network. Integration of these two networks could improve our global understanding of biological mechanisms triggered by a perturbation. This study focuses on improving the modelling of the regulatory network to facilitate a possible integration with the metabolic network. Previously proposed methods that study this problem fail to deal with a real-size regulatory network, computing predictions sensitive to perturbation and quantifying the predicted species behaviour more finely.
Results
To address previously mentioned limitations, we develop a new method based on Answer Set Programming, MajS. It takes a regulatory network and a discrete partial set of observations as input. MajS tests the consistency between the input data, proposes minimal repairs on the network to establish consistency, and finally computes weighted and signed predictions over the network species. We tested MajS by comparing the HIF-1 signalling pathway with two gene-expression datasets. Our results show that MajS can predict 100% of unobserved species. When comparing MajS with two similar (discrete and quantitative) tools, we observed that compared with the discrete tool, MajS proposes a better coverage of the unobserved species, is more sensitive to system perturbations, and proposes predictions closer to real data. Compared to the quantitative tool, MajS provides more refined discrete predictions that agree with the dynamic proposed by the quantitative tool.
Conclusions
MajS is a new method to test the consistency between a regulatory network and a dataset that provides computational predictions on unobserved network species. It provides fine-grained discrete predictions by outputting the weight of the predicted sign as a piece of additional information. MajS’ output, thanks to its weight, could easily be integrated 
with
 metabolic network modelling."
171,Sensitivity of CNN image analysis to multifaceted measurements of neurite growth,"Quantitative analysis of neurite growth and morphology is essential for understanding the determinants of neural development and regeneration, however, it is complicated by the labor-intensive process of measuring diverse parameters of neurite outgrowth. Consequently, automated approaches have been developed to study neurite morphology in a high-throughput and comprehensive manner. These approaches include computer-automated algorithms known as 'convolutional neural networks' (CNNs)—powerful models capable of learning complex tasks without the biases of hand-crafted models. Nevertheless, their complexity often relegates them to functioning as 'black boxes.' Therefore, research in the field of explainable AI is imperative to comprehend the relationship between CNN image analysis output and predefined morphological parameters of neurite growth in order to assess the applicability of these machine learning approaches. In this study, drawing inspiration from the field of automated feature selection, we investigate the correlation between quantified metrics of neurite morphology and the image analysis results from NeuriteNet—a CNN developed to analyze neurite growth. NeuriteNet accurately distinguishes images of neurite growth based on different treatment groups within two separate experimental systems. These systems differentiate between neurons cultured on different substrate conditions and neurons subjected to drug treatment inhibiting neurite outgrowth. By examining the model's function and patterns of activation underlying its classification decisions, we discover that NeuriteNet focuses on aspects of neuron morphology that represent quantifiable metrics distinguishing these groups. Additionally, it incorporates factors that are not encompassed by neuron morphology tracing analyses. NeuriteNet presents a novel tool ideally suited for screening morphological differences in heterogeneous neuron groups while also providing impetus for targeted follow-up studies."
172,A novel hybrid model to predict concomitant diseases for Hashimoto’s thyroiditis,"Hashimoto’s thyroiditis is an autoimmune disorder characterized by the destruction of thyroid cells through immune-mediated mechanisms involving cells and antibodies. The condition can trigger disturbances in metabolism, leading to the development of other autoimmune diseases, known as concomitant diseases. Multiple concomitant diseases may coexist in a single individual, making it challenging to diagnose and manage them effectively. This study aims to propose a novel hybrid algorithm that classifies concomitant diseases associated with Hashimoto’s thyroiditis based on sequences. The approach involves building distinct prediction models for each class and using the output of one model as input for the subsequent one, resulting in a dynamic decision-making process. Genes associated with concomitant diseases were collected alongside those related to Hashimoto’s thyroiditis, and their sequences were obtained from the NCBI site in fasta format. The hybrid algorithm was evaluated against common machine learning algorithms and their various combinations. The experimental results demonstrate that the proposed hybrid model outperforms existing classification methods in terms of performance metrics. The significance of this study lies in its two distinctive aspects. Firstly, it presents a new benchmarking dataset that has not been previously developed in this field, using diverse methods. Secondly, it proposes a more effective and efficient solution that accounts for the dynamic nature of the dataset. The hybrid approach holds promise in investigating the genetic heterogeneity of complex diseases such as Hashimoto’s thyroiditis and identifying new autoimmune disease genes. Additionally, the results of this study may aid in the development of genetic screening tools and laboratory experiments targeting Hashimoto’s thyroiditis genetic risk factors. New software, models, and techniques for computing, including systems biology, machine learning, and artificial intelligence, are used in our study."
173,iDESC: identifying differential expression in single-cell RNA sequencing data with multiple subjects,"Background
Single-cell RNA sequencing (scRNA-seq) technology has enabled assessment of transcriptome-wide changes at single-cell resolution. Due to the heterogeneity in environmental exposure and genetic background across subjects, subject effect contributes to the major source of variation in scRNA-seq data with multiple subjects, which severely confounds cell type specific differential expression (DE) analysis. Moreover, dropout events are prevalent in scRNA-seq data, leading to excessive number of zeroes in the data, which further aggravates the challenge in DE analysis.
Results
We developed iDESC to detect cell type specific DE genes between two groups of subjects in scRNA-seq data. iDESC uses a zero-inflated negative binomial mixed model to consider both subject effect and dropouts. The prevalence of dropout events (dropout rate) was demonstrated to be dependent on gene expression level, which is modeled by pooling information across genes. Subject effect is modeled as a random effect in the log-mean of the negative binomial component. We evaluated and compared the performance of iDESC with eleven existing DE analysis methods. Using simulated data, we demonstrated that iDESC had well-controlled type I error and higher power compared to the existing methods. Applications of those methods with well-controlled type I error to three real scRNA-seq datasets from the same tissue and disease showed that the results of iDESC achieved the best consistency between datasets and the best disease relevance.
Conclusions
iDESC was able to achieve more accurate and robust DE analysis results by separating subject effect from disease effect with consideration of dropouts to identify DE genes, suggesting the importance of considering subject effect and dropouts in the DE analysis of scRNA-seq data with multiple subjects."
174,RepBox: a toolbox for the identification of repetitive elements,"Background
Transposable elements (TEs) are short, mobile DNA elements that are known to play important roles in the genomes of many eukaryotic species. The identification and categorization of these elements is a critical task for many genomic studies, and the continued increase in the number of de novo assembled genomes demands new tools to improve the efficiency of this process. For this reason, we developed RepBox, a suite of Python scripts that combine several pre-existing family-specific TE detection methods into a single user-friendly pipeline.
Results
Based on comparisons of RepBox with the standard TE detection software RepeatModeler, we find that RepBox consistently classifies more elements and is also able to identify a more diverse array of TE families than the existing methods in plant genomes.
Conclusions
The performance of RepBox on two different plant genomes indicates that our toolbox represents a significant improvement over existing TE detection methods, and should facilitate future TE annotation efforts in additional species."
175,"BioLegato: a programmable, object-oriented graphic user interface","Background
Biologists are faced with an ever-changing array of complex software tools with steep learning curves, often run on High Performance Computing platforms. To resolve the tradeoff between analytical sophistication and usability, we have designed BioLegato, a programmable graphical user interface (GUI) for running external programs.
Results
BioLegato can run any program or pipeline that can be launched as a command. BioLegato reads specifications for each tool from files written in PCD, a simple language for specifying GUI components that set parameters for calling external programs. Thus, adding new tools to BioLegato can be done without changing the BioLegato Java code itself. The process is as simple as copying an existing PCD file and modifying it for the new program, which is more like filling in a form than writing code. PCD thus facilitates rapid development of new applications using existing programs as building blocks, and getting them to work together seamlessly.
Conclusion
BioLegato applies Object-Oriented concepts to the user experience by organizing applications based on discrete data types and the methods relevant to that data. PCD makes it easier for BioLegato applications to evolve with the succession of analytical tools for bioinformatics. BioLegato is applicable not only in biology, but in almost any field in which disparate software tools need to work as an integrated system."
176,An adaptive multi-modal hybrid model for classifying thyroid nodules by combining ultrasound and infrared thermal images,"Background
Two types of non-invasive, radiation-free, and inexpensive imaging technologies that are widely employed in medical applications are ultrasound (US) and infrared thermography (IRT). The ultrasound image obtained by ultrasound imaging primarily expresses the size, shape, contour boundary, echo, and other morphological information of the lesion, while the infrared thermal image obtained by infrared thermography imaging primarily describes its thermodynamic function information. Although distinguishing between benign and malignant thyroid nodules requires both morphological and functional information, present deep learning models are only based on US images, making it possible that some malignant nodules with insignificant morphological changes but significant functional changes will go undetected.
Results
Given the US and IRT images present thyroid nodules through distinct modalities, we proposed an Adaptive multi-modal Hybrid (AmmH) classification model that can leverage the amalgamation of these two image types to achieve superior classification performance. The AmmH approach involves the construction of a hybrid single-modal encoder module for each modal data, which facilitates the extraction of both local and global features by integrating a CNN module and a Transformer module. The extracted features from the two modalities are then weighted adaptively using an adaptive modality-weight generation network and fused using an adaptive cross-modal encoder module. The fused features are subsequently utilized for the classification of thyroid nodules through the use of MLP. On the collected dataset, our AmmH model respectively achieved 97.17% and 97.38% of F1 and F2 scores, which significantly outperformed the single-modal models. The results of four ablation experiments further show the superiority of our proposed method.
Conclusions
The proposed multi-modal model extracts features from various modal images, thereby enhancing the comprehensiveness of thyroid nodules descriptions. The adaptive modality-weight generation network enables adaptive attention to different modalities, facilitating the fusion of features using adaptive weights through the adaptive cross-modal encoder. Consequently, the model has demonstrated promising classification performance, indicating its potential as a non-invasive, radiation-free, and cost-effective screening tool for distinguishing between benign and malignant thyroid nodules. The source code is available at 
https://github.com/wuliZN2020/AmmH
."
177,rBahadur: efficient simulation of structured high-dimensional genotype data with applications to assortative mating,"Existing methods for generating synthetic genotype data are ill-suited for replicating the effects of assortative mating (AM). We propose 
rb
_
dplr
, a novel and computationally efficient algorithm for generating high-dimensional binary random variates that effectively recapitulates AM-induced genetic architectures using the Bahadur order-2 approximation of the multivariate Bernoulli distribution. The 
rBahadur
 R library is available through the Comprehensive R Archive Network at 
https://CRAN.R-project.org/package=rBahadur
."
178,BaPreS: a software tool for predicting bacteriocins using an optimal set of features,"Background
Antibiotic resistance is a major public health concern around the globe. As a result, researchers always look for new compounds to develop new antibiotic drugs for combating antibiotic-resistant bacteria. Bacteriocin becomes a promising antimicrobial agent to fight against antibiotic resistance, due to cases of both broad and narrow killing spectra. Sequence matching methods are widely used to identify bacteriocins by comparing them with the known bacteriocin sequences; however, these methods often fail to detect new bacteriocin sequences due to their high diversity. The ability to use a machine learning approach can help find new highly dissimilar bacteriocins for developing highly effective antibiotic drugs. The aim of this work is to develop a machine learning-based software tool called BaPreS (Bacteriocin Prediction Software) using an optimal set of features for detecting bacteriocin protein sequences with high accuracy. We extracted potential features from known bacteriocin and non-bacteriocin sequences by considering the physicochemical and structural properties of the protein sequences. Then we reduced the feature set using statistical justifications and recursive feature elimination technique. Finally, we built support vector machine (SVM) and random forest (RF) models using the selected features and utilized the best machine learning model to implement the software tool.
Results
We applied BaPreS to an established dataset and evaluated its prediction performance. Acquired results show that the software tool can achieve a prediction accuracy of 95.54% for testing protein sequences. This tool allows users to add new bacteriocin or non-bacteriocin sequences in the training dataset to further enhance the predictive power of the tool. We compared the prediction performance of the BaPreS with a popular sequence matching-based tool and a deep learning-based method, and our software tool outperformed both.
Conclusions
BaPreS is a bacteriocin prediction tool that can be used to discover new highly dissimilar bacteriocins for developing highly effective antibiotic drugs. This software tool can be used with Windows, Linux and macOS operating systems. The open-source software package and its user manual are available at 
https://github.com/suraiya14/BaPreS
."
179,ARCA: the interactive database for arbovirus reported cases in the Americas,"Background
Accurate case report data are essential to understand arbovirus dynamics, including spread and evolution of arboviruses such as Zika, dengue and chikungunya viruses. Giving the multi-country nature of arbovirus epidemics in the Americas, these data are not often accessible or are reported at different time scales (weekly, monthly) from different sources.
Results
We developed a publicly available and user-friendly database for arboviral case data in the Americas: ARCA. ARCA is a relational database that is hosted on the ARCA website. Users can interact with the database through the website by submitting queries through the website, which generates displays results and allows users to download these results in different, convenient file formats. Users can choose to view arboviral case data through a table which containscontaining the number of cases for a particular week, a plot, or through a map.
Conclusion
Our ARCA database is a useful tool for arboviral epidemiology research allowing for complex queries, data visualization, integration, and formatting."
180,An information-theoretic approach to single cell sequencing analysis,"Background
Single-cell sequencing (sc-Seq) experiments are producing increasingly large data sets. However, large data sets do not necessarily contain large amounts of information.
Results
Here, we formally quantify the information obtained from a sc-Seq experiment and show that it corresponds to an intuitive notion of gene expression heterogeneity. We demonstrate a natural relation between our notion of heterogeneity and that of cell type, decomposing heterogeneity into that component attributable to differential expression between cell types (inter-cluster heterogeneity) and that remaining (intra-cluster heterogeneity). We test our definition of heterogeneity as the objective function of a clustering algorithm, and show that it is a useful descriptor for gene expression patterns associated with different cell types.
Conclusions
Thus, our definition of gene heterogeneity leads to a biologically meaningful notion of cell type, as groups of cells that are statistically equivalent with respect to their patterns of gene expression. Our measure of heterogeneity, and its decomposition into inter- and intra-cluster, is non-parametric, intrinsic, unbiased, and requires no additional assumptions about expression patterns. Based on this theory, we develop an efficient method for the automatic unsupervised clustering of cells from sc-Seq data, and provide an R package implementation."
181,estimateR: an R package to estimate and monitor the effective reproductive number,"Background
Accurate estimation of the effective reproductive number (
\(R_e\)
) of epidemic outbreaks is of central relevance to public health policy and decision making. We present estimateR, an R package for the estimation of the reproductive number through time from delayed observations of infection events. Such delayed observations include confirmed cases, hospitalizations or deaths. The package implements the methodology of Huisman et al. but modularizes the 
\(R_e\)
 estimation procedure to allow easy implementation of new alternatives to the currently available methods. Users can tailor their analyses according to their particular use case by choosing among implemented options.
Results
The estimateR R package allows users to estimate the effective reproductive number of an epidemic outbreak based on observed cases, hospitalization, death or any other type of event documenting past infections, in a fast and timely fashion. We validated the implementation with a simulation study: estimateR yielded estimates comparable to alternative publicly available methods while being around two orders of magnitude faster. We then applied estimateR to empirical case-confirmation incidence data for COVID-19 in nine countries and for dengue fever in Brazil; in parallel, estimateR is already being applied (i) to SARS-CoV-2 measurements in wastewater data and (ii) to study influenza transmission based on wastewater and clinical data in other studies. In summary, this R package provides a fast and flexible implementation to estimate the effective reproductive number for various diseases and datasets.
Conclusions
The estimateR R package is a modular and extendable tool designed for outbreak surveillance and retrospective outbreak investigation. It extends the method developed for COVID-19 by Huisman et al. and makes it available for a variety of pathogens, outbreak scenarios, and observation types. Estimates obtained with estimateR can be interpreted directly or used to inform more complex epidemic models (e.g. for forecasting) on the value of 
\(R_e\)
."
182,"ggcoverage
: an R package to visualize and annotate genome coverage for various NGS data","Background
Visualizing genome coverage is of vital importance to inspect and interpret various next-generation sequencing (NGS) data. Besides genome coverage, genome annotations are also crucial in the visualization. While different NGS data require different annotations, how to visualize genome coverage and add the annotations appropriately and conveniently is challenging. Many tools have been developed to address this issue. However, existing tools are often inflexible, complicated, lack necessary preprocessing steps and annotations, and the figures generated support limited customization.
Results
Here, we introduce 
ggcoverage
, an R package to visualize and annotate genome coverage of multi-groups and multi-omics. The input files for 
ggcoverage
 can be in BAM, BigWig, BedGraph and TSV formats. For better usability, 
ggcoverage
 provides reliable and efficient ways to perform read normalization, consensus peaks generation and track data loading with state-of-the-art tools. 
ggcoverage
 provides various available annotations to adapt to different NGS data (e.g. WGS/WES, RNA-seq, ChIP-seq) and all the available annotations can be easily superimposed with ‘ + ’. 
ggcoverage
 can generate publication-quality plots and users can customize the plots with 
ggplot2
. In addition, 
ggcoverage
 supports the visualization and annotation of protein coverage.
Conclusions
ggcoverage
 provides a flexible, programmable, efficient and user-friendly way to visualize and annotate genome coverage of multi-groups and multi-omics. The 
ggcoverage
 package is available at 
https://github.com/showteeth/ggcoverage
 under the MIT license, and the vignettes are available at 
https://showteeth.github.io/ggcoverage/
."
183,Boosting variant-calling performance with multi-platform sequencing data using Clair3-MP,"Background
With the continuous advances in third-generation sequencing technology and the increasing affordability of next-generation sequencing technology, sequencing data from different sequencing technology platforms is becoming more common. While numerous benchmarking studies have been conducted to compare variant-calling performance across different platforms and approaches, little attention has been paid to the potential of leveraging the strengths of different platforms to optimize overall performance, especially integrating Oxford Nanopore and Illumina sequencing data.
Results
We investigated the impact of multi-platform data on the performance of variant calling through carefully designed experiments with a deep learning-based variant caller named Clair3-MP (Multi-Platform). Through our research, we not only demonstrated the capability of ONT-Illumina data for improved variant calling, but also identified the optimal scenarios for utilizing ONT-Illumina data. In addition, we revealed that the improvement in variant calling using ONT-Illumina data comes from an improvement in difficult genomic regions, such as the large low-complexity regions and segmental and collapse duplication regions. Moreover, Clair3-MP can incorporate reference genome stratification information to achieve a small but measurable improvement in variant calling. Clair3-MP is accessible as an open-source project at: 
https://github.com/HKU-BAL/Clair3-MP
.
Conclusions
These insights have important implications for researchers and practitioners alike, providing valuable guidance for improving the reliability and efficiency of genomic analysis in diverse applications."
184,Molecular complex detection in protein interaction networks through reinforcement learning,"Background
Proteins often assemble into higher-order complexes to perform their biological functions. Such protein–protein interactions (PPI) are often experimentally measured for pairs of proteins and summarized in a weighted PPI network, to which community detection algorithms can be applied to define the various higher-order protein complexes. Current methods include unsupervised and supervised approaches, often assuming that protein complexes manifest only as dense subgraphs. Utilizing supervised approaches, the focus is not on 
how
 to find them in a network, but only on learning 
which
 subgraphs correspond to complexes, currently solved using heuristics. However, learning to walk trajectories on a network to identify protein complexes leads naturally to a reinforcement learning (RL) approach, a strategy not extensively explored for community detection. Here, we develop and evaluate a reinforcement learning pipeline for community detection on weighted protein–protein interaction networks to detect new protein complexes. The algorithm is trained to calculate the value of different subgraphs encountered while walking on the network to reconstruct known complexes. A distributed prediction algorithm then scales the RL pipeline to search for novel protein complexes on large PPI networks.
Results
The reinforcement learning pipeline is applied to a human PPI network consisting of 8k proteins and 60k PPI, which results in 1,157 protein complexes. The method demonstrated competitive accuracy with improved speed compared to previous algorithms. We highlight protein complexes such as C4orf19, C18orf21, and KIAA1522 which are currently minimally characterized. Additionally, the results suggest TMC04 be a putative additional subunit of the KICSTOR complex and confirm the involvement of C15orf41 in a higher-order complex with HIRA, CDAN1, ASF1A, and by 3D structural modeling.
Conclusions
Reinforcement learning offers several distinct advantages for community detection, including scalability and knowledge of the walk trajectories defining those communities. Applied to currently available human protein interaction networks, this method had comparable accuracy with other algorithms and notable savings in computational time, and in turn, led to clear predictions of protein function and interactions for several uncharacterized human proteins."
185,VariantscanR: an R-package as a clinical tool for variant filtering of known phenotype-associated variants in domestic animals,"Background
Since the introduction of next-generation sequencing (NGS) techniques, whole-exome sequencing (WES) and whole-genome sequencing (WGS) have not only revolutionized research, but also diagnostics. The gradual switch from single gene testing to WES and WGS required a different set of skills, given the amount and type of data generated, while the demand for standardization remained. However, most of the tools currently available are solely applicable for human analysis because they require access to specific databases and/or simply do not support other species. Additionally, a complicating factor in clinical genetics in animals is that genetic diversity is often dangerously low due to the breeding history. Combined, there is a clear need for an easy-to-use, flexible tool that allows standardized data processing and preferably, monitoring of genetic diversity as well. To fill these gaps, we developed the R-package variantscanR that allows an easy and straightforward identification and prioritization of known phenotype-associated variants identified in dogs and other domestic animals.
Results
The R-package variantscanR enables the filtering of variant call format (VCF) files for the presence of known phenotype-associated variants and allows for the estimation of genetic diversity using multi-sample VCF files. Next to this, additional functions are available for the quality control and processing of user-defined input files to make the workflow as easy and straightforward as possible. This user-friendly approach enables the standardisation of complex data analysis in clinical settings.
Conclusion
We developed an R-package for the identification of known phenotype-associated variants and calculation of genetic diversity."
186,Netrank: network-based approach for biomarker discovery,"Background
Integrating multi-omics data is fast becoming a powerful approach for predicting disease progression and treatment outcomes. In light of that, we introduce a modified version of the NetRank algorithm, a network-based algorithm for biomarker discovery that incorporates the protein associations, co-expressions, and functions with its phenotypic association to differentiate different types of cancer. NetRank is introduced here as a robust feature selection method for biomarker selection in cancer prediction. We assess the robustness and suitability of the RNA gene expression data through scanning genomic data for 19 cancer types with more than 3000 patients from The Cancer Genome Atlas (TCGA).
Results
The results of evaluating different cancer type profiles from the TCGA data demonstrate the strength of our approach to identifying interpretable biomarker signatures for cancer outcome prediction. NetRank’s biomarkers segregate most cancer types with an area under the curve (AUC) above 90% using compact signatures.
Conclusion
In this paper we provide a fast and efficient implementation of NetRank, with a case study from The Cancer Genome Atlas, to assess the performance. We incorporated complete functionality for pre and post-processing for RNA-seq gene expression data with functions for building protein-protein interaction networks. The source code of NetRank is freely available (at github.com/Alfatlawi/Omics-NetRank) with an installable R library. We also deliver a comprehensive practical user manual with examples and data attached to this paper."
187,ROGUE: an R Shiny app for RNA sequencing analysis and biomarker discovery,"Background
The growing power and ever decreasing cost of RNA sequencing (RNA-Seq) technologies have resulted in an explosion of RNA-Seq data production. Comparing gene expression values within RNA-Seq datasets is relatively easy for many interdisciplinary biomedical researchers; however, user-friendly software applications increase the ability of biologists to efficiently explore available datasets.
Results
Here, we describe ROGUE (RNA-Seq Ontology Graphic User Environment, 
https://marisshiny.research.chop.edu/ROGUE/
), a user-friendly R Shiny application that allows a biologist to perform differentially expressed gene analysis, gene ontology and pathway enrichment analysis, potential biomarker identification, and advanced statistical analyses. We use ROGUE to identify potential biomarkers and show unique enriched pathways between various immune cells.
Conclusions
User-friendly tools for the analysis of next generation sequencing data, such as ROGUE, will allow biologists to efficiently explore their datasets, discover expression patterns, and advance their research by allowing them to develop and test hypotheses."
188,Evaluating imputation methods for single-cell RNA-seq data,"Background
Single-cell RNA sequencing (scRNA-seq) enables the high-throughput profiling of gene expression at the single-cell level. However, overwhelming dropouts within data may obscure meaningful biological signals. Various imputation methods have recently been developed to address this problem. Therefore, it is important to perform a systematic evaluation of different imputation algorithms.
Results
In this study, we evaluated 11 of the most recent imputation methods on 12 real biological datasets from immunological studies and 4 simulated datasets. The performance of these methods was compared, based on numerical recovery, cell clustering and marker gene analysis. Most of the methods brought some benefits on numerical recovery. To some extent, the performance of imputation methods varied among protocols. In the cell clustering analysis, no method performed consistently well across all datasets. Some methods performed poorly on real datasets but excellent on simulated datasets. Surprisingly and importantly, some methods had a negative effect on cell clustering. In marker gene analysis, some methods identified potentially novel cell subsets. However, not all of the marker genes were successfully imputed in gene expression, suggesting that imputation challenges remain.
Conclusions
In summary, different imputation methods showed different effects on different datasets, suggesting that imputation may have dataset specificity. Our study reveals the benefits and limitations of various imputation methods and provides a data-driven guidance for scRNA-seq data analysis."
189,StackTTCA: a stacking ensemble learning-based framework for accurate and high-throughput identification of tumor T cell antigens,"Background
The identification of tumor T cell antigens (TTCAs) is crucial for providing insights into their functional mechanisms and utilizing their potential in anticancer vaccines development. In this context, TTCAs are highly promising. Meanwhile, experimental technologies for discovering and characterizing new TTCAs are expensive and time-consuming. Although many machine learning (ML)-based models have been proposed for identifying new TTCAs, there is still a need to develop a robust model that can achieve higher rates of accuracy and precision.
Results
In this study, we propose a new stacking ensemble learning-based framework, termed StackTTCA, for accurate and large-scale identification of TTCAs. Firstly, we constructed 156 different baseline models by using 12 different feature encoding schemes and 13 popular ML algorithms. Secondly, these baseline models were trained and employed to create a new probabilistic feature vector. Finally, the optimal probabilistic feature vector was determined based the feature selection strategy and then used for the construction of our stacked model. Comparative benchmarking experiments indicated that StackTTCA clearly outperformed several ML classifiers and the existing methods in terms of the independent test, with an accuracy of 0.932 and Matthew's correlation coefficient of 0.866.
Conclusions
In summary, the proposed stacking ensemble learning-based framework of StackTTCA could help to precisely and rapidly identify true TTCAs for follow-up experimental verification. In addition, we developed an online web server (
http://2pmlab.camt.cmu.ac.th/StackTTCA
) to maximize user convenience for high-throughput screening of novel TTCAs."
190,"OGRE: calculate, visualize, and analyze overlap between genomic input regions and public annotations","Background
Modern genome sequencing leads to an ever-growing collection of genomic annotations. Combining these elements with a set of input regions (e.g. genes) would yield new insights in genomic associations, such as those involved in gene regulation. The required data are scattered across different databases making a manual approach tiresome, unpractical, and prone to error. Semi-automatic approaches require programming skills in data parsing, processing, overlap calculation, and visualization, which most biomedical researchers lack. Our aim was to develop an automated tool providing all necessary algorithms, benefiting both bioinformaticians and researchers without bioinformatic training.
Results
We developed overlapping annotated genomic regions (OGRE) as a comprehensive tool to associate and visualize input regions with genomic annotations. It does so by parsing regions of interest, mining publicly available annotations, and calculating possible overlaps between them. The user can thus identify location, type, and number of associated regulatory elements. Results are presented as easy to understand visualizations and result tables. We applied OGRE to recent studies and could show high reproducibility and potential new insights. To demonstrate OGRE’s performance in terms of running time and output, we have conducted a benchmark and compared its features with similar tools.
Conclusions
OGRE’s functions and built-in annotations can be applied as a downstream overlap association step, which is compatible with most genomic sequencing outputs, and can thus enrich pre-existing analyses pipelines. Compared to similar tools, OGRE shows competitive performance, offers additional features, and has been successfully applied to two recent studies. Overall, OGRE addresses the lack of tools for automatic analysis, local genomic overlap calculation, and visualization by providing an easy to use, end-to-end solution for both biologists and computational scientists."
191,The metabolomics workbench file status website: a metadata repository promoting FAIR principles of metabolomics data,"Background
An updated version of the mwtab Python package for programmatic access to the Metabolomics Workbench (MetabolomicsWB) data repository was released at the beginning of 2021. Along with updating the package to match the changes to MetabolomicsWB’s ‘mwTab’ file format specification and enhancing the package’s functionality, the included validation facilities were used to detect and catalog file inconsistencies and errors across all publicly available datasets in MetabolomicsWB.
Results
The MetabolomicsWB File Status website was developed to provide continuous validation of MetabolomicsWB data files and a useful interface to all found inconsistencies and errors. This list of detectable issues/errors include format parsing errors, format compliance issues, access problems via MetabolomicsWB’s REST interface, and other small inconsistencies that can hinder reusability. The website uses the mwtab Python package to pull down and validate each available analysis file and then generates an html report. The website is updated on a weekly basis. Moreover, the Python website design utilizes GitHub and GitHub.io, providing an easy to replicate template for implementing other metadata, virtual, and meta- repositories.
Conclusions
The MetabolomicsWB File Status website provides a metadata repository of validation metadata to promote the FAIR use of existing metabolomics datasets from the MetabolomicsWB data repository."
192,Selection of optimal quantile protein biomarkers based on cell-level immunohistochemistry data,"Background
Protein biomarkers of cancer progression and response to therapy are increasingly important for improving personalized medicine. Advanced quantitative pathology platforms enable measurement of protein expression in tissues at the single-cell level. However, this rich quantitative cell-by-cell biomarker information is most often not exploited. Instead, it is reduced to a single mean across the cells of interest or converted into a simple proportion of binary biomarker-positive or -negative cells.
Results
We investigated the utility of retaining all quantitative information at the single-cell level by considering the values of the quantile function (inverse of the cumulative distribution function) estimated from a sample of cell signal intensity levels in a tumor tissue. An algorithm was developed for selecting optimal cutoffs for dichotomizing cell signal intensity distribution quantiles as predictors of continuous, categorical or survival outcomes. The proposed algorithm was used to select optimal quantile biomarkers of breast cancer progression based on cancer cells’ cell signal intensity levels of nuclear protein Ki-67, Proliferating cell nuclear antigen, Programmed cell death 1 ligand 2, and Progesterone receptor. The performance of the resulting optimal quantile biomarkers was validated and compared to the standard cancer compartment mean signal intensity markers using an independent external validation cohort. For Ki-67, the optimal quantile biomarker was also compared to established biomarkers based on percentages of Ki67-positive cells. For proteins significantly associated with PFS in the external validation cohort, the optimal quantile biomarkers yielded either larger or similar effect size (hazard ratio for progression-free survival) as compared to cancer compartment mean signal intensity biomarkers.
Conclusion
The optimal quantile protein biomarkers yield generally improved prognostic value as compared to the standard protein expression markers. The proposed methodology has a broad application to single-cell data from genomics, transcriptomics, proteomics, or metabolomics studies at the single cell level."
193,Searching for protein variants with desired properties using deep generative models,"Background
Protein engineering aims to improve the functional properties of existing proteins to meet people’s needs. Current deep learning-based models have captured evolutionary, functional, and biochemical features contained in amino acid sequences. However, the existing generative models need to be improved when capturing the relationship between amino acid sites on longer sequences. At the same time, the distribution of protein sequences in the homologous family has a specific positional relationship in the latent space. We want to use this relationship to search for new variants directly from the vicinity of better-performing varieties.
Results
To improve the representation learning ability of the model for longer sequences and the similarity between the generated sequences and the original sequences, we propose a temporal variational autoencoder (T-VAE) model. T-VAE consists of an encoder and a decoder. The encoder expands the receptive field of neurons in the network structure by dilated causal convolution, thereby improving the encoding representation ability of longer sequences. The decoder decodes the sampled data into variants closely resembling the original sequence.
Conclusion
Compared to other models, the person correlation coefficient between the predicted values of protein fitness obtained by T-VAE and the truth values was higher, and the mean absolute deviation was lower. In addition, the T-VAE model has a better representation learning ability for longer sequences when comparing the encoding of protein sequences of different lengths. These results show that our model has more advantages in representation learning for longer sequences. To verify the model’s generative effect, we also calculate the sequence identity between the generated data and the input data. The sequence identity obtained by T-VAE improved by 12.9% compared to the baseline model."
194,Causal discovery approach with reinforcement learning for risk factors of type II diabetes mellitus,"Background
Statistical correlation analysis is currently the most typically used approach for investigating the risk factors of type 2 diabetes mellitus (T2DM). However, this approach does not readily reveal the causal relationships between risk factors and rarely describes the causal relationships visually.
Results
Considering the superiority of reinforcement learning in prediction, a causal discovery approach with reinforcement learning for T2DM risk factors is proposed herein. First, a reinforcement learning model is constructed for T2DM risk factors. Second, the process involved in the causal discovery method for T2DM risk factors is detailed. Finally, several experiments are designed based on diabetes datasets and used to verify the proposed approach.
Conclusions
The experimental results show that the proposed approach improves the accuracy of causality mining between T2DM risk factors and provides new evidence to researchers engaged in T2DM prevention and treatment research."
195,xenoGI 3: using the DTLOR model to reconstruct the evolution of gene families in clades of microbes,"To understand genome evolution in a group of microbes, we need to know the timing of events such as duplications, deletions and horizontal transfers. A common approach is to perform a gene-tree / species-tree reconciliation. While a number of software packages perform this type of analysis, none are geared toward a complete reconstruction for all families in an entire clade. Here we describe an update to the xenoGI software package which allows users to perform such an analysis using the newly developed DTLOR (duplication-transfer-loss-origin-rearrangement) reconciliation model starting from genome sequences as input."
196,Starvar: symptom-based tool for automatic ranking of variants using evidence from literature and genomes,"Background
Identifying variants associated with diseases is a challenging task in medical genetics research. Current studies that prioritize variants within individual genomes generally rely on known variants, evidence from literature and genomes, and patient symptoms and clinical signs. The functionalities of the existing tools, which rank variants based on given patient symptoms and clinical signs, are restricted to the coverage of ontologies such as the Human Phenotype Ontology (HPO). However, most clinicians do not limit themselves to HPO while describing patient symptoms/signs and their associated variants/genes. There is thus a need for an automated tool that can prioritize variants based on freely expressed patient symptoms and clinical signs.
Results
STARVar is a Symptom-based Tool for Automatic Ranking of Variants using evidence from literature and genomes. STARVar uses patient symptoms and clinical signs, either linked to HPO or expressed in free text format. It returns a ranked list of variants based on a combined score from two classifiers utilizing evidence from genomics and literature. STARVar improves over related tools on a set of synthetic patients. In addition, we demonstrated its distinct contribution to the domain on another synthetic dataset covering publicly available clinical genotype–phenotype associations by using symptoms and clinical signs expressed in free text format.
Conclusions
STARVar stands as a unique and efficient tool that has the advantage of ranking variants with flexibly expressed patient symptoms in free-form text. Therefore, STARVar can be easily integrated into bioinformatics workflows designed to analyze disease-associated genomes.
Availability
STARVar is freely available from 
https://github.com/bio-ontology-research-group/STARVar
."
197,Enhancing drug property prediction with dual-channel transfer learning based on molecular fragment,"Background
Accurate prediction of molecular property holds significance in contemporary drug discovery and medical research. Recent advances in AI-driven molecular property prediction have shown promising results. Due to the costly annotation of in vitro and in vivo experiments, transfer learning paradigm has been gaining momentum in extracting general self-supervised information to facilitate neural network learning. However, prior pretraining strategies have overlooked the necessity of explicitly incorporating domain knowledge, especially the molecular fragments, into model design, resulting in the under-exploration of the molecular semantic space.
Results
We propose an effective model with FRagment-based dual-channEL pretraining (
FREL
). Equipped with molecular fragments, 
FREL
 comprehensively employs masked autoencoder and contrastive learning to learn intra- and inter-molecule agreement, respectively. We further conduct extensive experiments on ten public datasets to demonstrate its superiority over state-of-the-art models. Further investigations and interpretations manifest the underlying relationship between molecular representations and molecular properties.
Conclusions
Our proposed model FREL achieves state-of-the-art performance on the benchmark datasets, emphasizing the importance of incorporating molecular fragments into model design. The expressiveness of learned molecular representations is also investigated by visualization and correlation analysis. Case studies indicate that the learned molecular representations better capture the drug property variation and fragment semantics."
198,A pipeline for the retrieval and extraction of domain-specific information with application to COVID-19 immune signatures,"Background
The accelerating pace of biomedical publication has made it impractical to manually, systematically identify papers containing specific information and extract this information. This is especially challenging when the information itself resides beyond titles or abstracts. For emerging science, with a limited set of known papers of interest and an incomplete information model, this is of pressing concern. A timely example in retrospect is the identification of immune signatures (coherent sets of biomarkers) driving differential SARS-CoV-2 infection outcomes.
Implementation
We built a classifier to identify papers containing domain-specific information from the document embeddings of the title and abstract. To train this classifier with limited data, we developed an iterative process leveraging pre-trained SPECTER document embeddings, SVM classifiers and web-enabled expert review to iteratively augment the training set. This training set was then used to create a classifier to identify papers containing domain-specific information. Finally, information was extracted from these papers through a semi-automated system that directly solicited the paper authors to respond via a web-based form.
Results
We demonstrate a classifier that retrieves papers with human COVID-19 immune signatures with a positive predictive value of 86%. The type of immune signature (e.g., gene expression vs. other types of profiling) was also identified with a positive predictive value of 74%. Semi-automated queries to the corresponding authors of these publications requesting signature information achieved a 31% response rate.
Conclusions
Our results demonstrate the efficacy of using a SVM classifier with document embeddings of the title and abstract, to retrieve papers with domain-specific information, even when that information is rarely present in the abstract. Targeted author engagement based on classifier predictions offers a promising pathway to build a semi-structured representation of such information. Through this approach, partially automated literature mining can help rapidly create semi-structured knowledge repositories for automatic analysis of emerging health threats."
199,"CamPype: an open-source workflow for automated bacterial whole-genome sequencing analysis focused on 
Campylobacter","Background
The rapid expansion of Whole-Genome Sequencing has revolutionized the fields of clinical and food microbiology. However, its implementation as a routine laboratory technique remains challenging due to the growth of data at a faster rate than can be effectively analyzed and critical gaps in bioinformatics knowledge.
Results
To address both issues, CamPype was developed as a new bioinformatics workflow for the genomics analysis of sequencing data of bacteria, especially 
Campylobacter
, which is the main cause of gastroenteritis worldwide making a negative impact on the economy of the public health systems. CamPype allows fully customization of stages to run and tools to use, including read quality control filtering, read contamination, reads extension and assembly, bacterial typing, genome annotation, searching for antibiotic resistance genes, virulence genes and plasmids, pangenome construction and identification of nucleotide variants. All results are processed and resumed in an interactive HTML report for best data visualization and interpretation.
Conclusions
The minimal user intervention of CamPype makes of this workflow an attractive resource for microbiology laboratories with no expertise in bioinformatics as a first line method for bacterial typing and epidemiological analyses, that would help to reduce the costs of disease outbreaks, or for comparative genomic analyses. CamPype is publicly available at 
https://github.com/JoseBarbero/CamPype
."
200,Leveraging pre-trained language models for mining microbiome-disease relationships,"Background
The growing recognition of the microbiome’s impact on human health and well-being has prompted extensive research into discovering the links between microbiome dysbiosis and disease (healthy) states. However, this valuable information is scattered in unstructured form within biomedical literature. The structured extraction and qualification of microbe-disease interactions are important. In parallel, recent advancements in deep-learning-based natural language processing algorithms have revolutionized language-related tasks such as ours. This study aims to leverage state-of-the-art deep-learning language models to extract microbe-disease relationships from biomedical literature.
Results
In this study, we first evaluate multiple pre-trained large language models within a zero-shot or few-shot learning context. In this setting, the models performed poorly out of the box, emphasizing the need for domain-specific fine-tuning of these language models. Subsequently, we fine-tune multiple language models (specifically, GPT-3, BioGPT, BioMedLM, BERT, BioMegatron, PubMedBERT, BioClinicalBERT, and BioLinkBERT) using labeled training data and evaluate their performance. Our experimental results demonstrate the state-of-the-art performance of these fine-tuned models ( specifically GPT-3, BioMedLM, and BioLinkBERT), achieving an average F1 score, precision, and recall of over 
\(>0.8\)
 compared to the previous best of  0.74.
Conclusion
Overall, this study establishes that pre-trained language models excel as transfer learners when fine-tuned with domain and problem-specific data, enabling them to achieve state-of-the-art results even with limited training data for extracting microbiome-disease interactions from scientific publications."
201,ForestSubtype: a cancer subtype identifying approach based on high-dimensional genomic data and a parallel random forest,"Background
Cancer subtype classification is helpful for personalized cancer treatment. Although, some approaches have been developed to classifying caner subtype based on high dimensional gene expression data, it is difficult to obtain satisfactory classification results. Meanwhile, some cancers have been well studied and classified to some subtypes, which are adopt by most researchers. Hence, this priori knowledge is significant for further identifying new meaningful subtypes.
Results
In this paper, we present a combined parallel random forest and autoencoder approach for cancer subtype identification based on high dimensional gene expression data, ForestSubtype. ForestSubtype first adopts the parallel RF and the priori knowledge of cancer subtype to train a module and extract significant candidate features. Second, ForestSubtype uses a random forest as the base module and ten parallel random forests to compute each feature weight and rank them separately. Then, the intersection of the features with the larger weights output by the ten parallel random forests is taken as our subsequent candidate features. Third, ForestSubtype uses an autoencoder to condenses the selected features into a two-dimensional data. Fourth, ForestSubtype utilizes k-means++ to obtain new cancer subtype identification results. In this paper, the breast cancer gene expression data obtained from The Cancer Genome Atlas are used for training and validation, and an independent breast cancer dataset from the Molecular Taxonomy of Breast Cancer International Consortium is used for testing. Additionally, we use two other cancer datasets for validating the generalizability of ForestSubtype. ForestSubtype outperforms the other two methods in terms of the distribution of clusters, internal and external metric results. The open-source code is available at 
https://github.com/lffyd/ForestSubtype
.
Conclusions
Our work shows that the combination of high-dimensional gene expression data and parallel random forests and autoencoder, guided by a priori knowledge, can identify new subtypes more effectively than existing methods of cancer subtype classification."
202,MitoHiFi: a python pipeline for mitochondrial genome assembly from PacBio high fidelity reads,"Background
 PacBio high fidelity (HiFi) sequencing reads are both long (15–20 kb) and highly accurate (> Q20). Because of these properties, they have revolutionised genome assembly leading to more accurate and contiguous genomes. In eukaryotes the mitochondrial genome is sequenced alongside the nuclear genome often at very high coverage. A dedicated tool for mitochondrial genome assembly using HiFi reads is still missing.
Results
 MitoHiFi was developed within the Darwin Tree of Life Project to assemble mitochondrial genomes from the HiFi reads generated for target species. The input for MitoHiFi is either the raw reads or the assembled contigs, and the tool outputs a mitochondrial genome sequence fasta file along with annotation of protein and RNA genes. Variants arising from heteroplasmy are assembled independently, and nuclear insertions of mitochondrial sequences are identified and not used in organellar genome assembly. MitoHiFi has been used to assemble 374 mitochondrial genomes (368 Metazoa and 6 Fungi species) for the Darwin Tree of Life Project, the Vertebrate Genomes Project and the Aquatic Symbiosis Genome Project. Inspection of 60 mitochondrial genomes assembled with MitoHiFi for species that already have reference sequences in public databases showed the widespread presence of previously unreported repeats.
Conclusions
 MitoHiFi is able to assemble mitochondrial genomes from a wide phylogenetic range of taxa from Pacbio HiFi data. MitoHiFi is written in python and is freely available on GitHub (
https://github.com/marcelauliano/MitoHiFi
). MitoHiFi is available with its dependencies as a Docker container on GitHub (ghcr.io/marcelauliano/mitohifi:master)."
203,SNPeffect 5.0: large-scale structural phenotyping of protein coding variants extracted from next-generation sequencing data using AlphaFold models,"Background
Next-generation sequencing technologies yield large numbers of genetic alterations, of which a subset are missense variants that alter an amino acid in the protein product. These variants can have a potentially destabilizing effect leading to an increased risk of misfolding and aggregation. Multiple software tools exist to predict the effect of single-nucleotide variants on proteins, however, a pipeline integrating these tools while starting from an NGS data output list of variants is lacking.
Results
The previous version SNPeffect 4.0 (De Baets in Nucleic Acids Res 40(D1):D935–D939, 2011) provided an online database containing pre-calculated variant effects and low-throughput custom variant analysis. Here, we built an automated and parallelized pipeline that analyzes the impact of missense variants on the aggregation propensity and structural stability of proteins starting from the Variant Call Format as input. The pipeline incorporates the AlphaFold Protein Structure Database to achieve high coverage for structural stability analyses using the FoldX force field. The effect on aggregation-propensity is analyzed using the established predictors TANGO and WALTZ. The pipeline focuses solely on the human proteome and can be used to analyze proteome stability/damage in a given sample based on sequencing results.
Conclusion
We provide a bioinformatics pipeline that allows structural phenotyping from sequencing data using established stability and aggregation predictors including FoldX, TANGO, and WALTZ; and structural proteome coverage provided by the AlphaFold database. The pipeline and installation guide are freely available for academic users on 
https://github.com/vibbits/snpeffect
 and requires a computer cluster."
204,IS-Seq: a bioinformatics pipeline for integration sites analysis with comprehensive abundance quantification methods,"Background
Integration site (IS) analysis is a fundamental analytical platform for evaluating the safety and efficacy of viral vector based preclinical and clinical Gene Therapy (GT). A handful of groups have developed standardized bioinformatics pipelines to process IS sequencing data, to generate reports, and/or to perform comparative studies across different GT trials. Keeping up with the technological advances in the field of IS analysis, different computational pipelines have been published over the past decade. These pipelines focus on identifying IS from single-read sequencing or paired-end sequencing data either using read-based or using sonication fragment-based methods, but there is a lack of a bioinformatics tool that automatically includes unique molecular identifiers (UMI) for IS abundance estimations and allows comparing multiple quantification methods in one integrated pipeline.

Results
Here we present IS-Seq a bioinformatics pipeline that can process data from paired-end sequencing of both old restriction sites-based IS collection methods and new sonication-based IS retrieval systems while allowing the selection of different abundance estimation methods, including read-based, Fragment-based and UMI-based systems.

Conclusions
We validated the performance of IS-Seq by testing it against the most popular  analytical workflow available in the literature (INSPIIRED) and using different scenarios. Lastly, by performing extensive simulation studies and a comprehensive wet-lab assessment of our IS-Seq pipeline we could show that in clinically relevant scenarios, UMI quantification provides better accuracy than the currently most widely used sonication fragment counts as a method for IS abundance estimation."
205,P-TransUNet: an improved parallel network for medical image segmentation,"Deep learning-based medical image segmentation has made great progress over the past decades. Scholars have proposed many novel transformer-based segmentation networks to solve the problems of building long-range dependencies and global context connections in convolutional neural networks (CNNs). However, these methods usually replace the CNN-based blocks with improved transformer-based structures, which leads to the lack of local feature extraction ability, and these structures require a huge number of data for training. Moreover, those methods did not pay attention to edge information, which is essential in medical image segmentation. To address these problems, we proposed a new network structure, called P-TransUNet. This network structure combines the designed efficient P-Transformer and the fusion module, which extract distance-related long-range dependencies and local information respectively and produce the fused features. Besides, we introduced edge loss into training to focus the attention of the network on the edge of the lesion area to improve segmentation performance. Extensive experiments across four tasks of medical image segmentation demonstrated the effectiveness of P-TransUNet, and showed that our network outperforms other state-of-the-art methods."
206,"MTG-Link: leveraging barcode information from linked-reads to assemble specific 
loci","Background
Local assembly with short and long reads has proven to be very useful in many applications: reconstruction of the sequence of a 
locus
 of interest, gap-filling in draft assemblies, as well as alternative allele reconstruction of large Structural Variants. Whereas linked-read technologies have a great potential to assemble specific 
loci
 as they provide long-range information while maintaining the power and accuracy of short-read sequencing, there is a lack of local assembly tools for linked-read data.
Results
We present MTG-Link, a novel local assembly tool dedicated to linked-reads. The originality of the method lies in its read subsampling step which takes advantage of the barcode information contained in linked-reads mapped in flanking regions. We validated our approach on several datasets from different linked-read technologies. We show that MTG-Link is able to assemble successfully large sequences, up to dozens of Kb. We also demonstrate that the read subsampling step of MTG-Link considerably improves the local assembly of specific 
loci
 compared to other existing short-read local assembly tools. Furthermore, MTG-Link was able to fully characterize large insertion variants and deletion breakpoints in a human genome and to reconstruct dark regions in clinically-relevant human genes. It also improved the contiguity of a 1.3 Mb 
locus
 of biological interest in several individual genomes of the mimetic butterfly 
Heliconius numata
.
Conclusions
MTG-Link is an efficient local assembly tool designed for different linked-read sequencing technologies. MTG-Link source code is available at 
https://github.com/anne-gcd/MTG-Link
 and as a Bioconda package."
207,VolumePeeler: a novel FIJI plugin for geometric tissue peeling to improve visualization and quantification of 3D image stacks,"Motivation
Quantitative descriptions of multi-cellular structures from optical microscopy imaging are prime to understand the variety of three-dimensional (3D) shapes in living organisms. Experimental models of vertebrates, invertebrates and plants, such as zebrafish, killifish, 
Drosophila
 or 
Marchantia
, mainly comprise multilayer tissues, and even if microscopes can reach the needed depth, their geometry hinders the selection and subsequent analysis of the optical volumes of interest. Computational tools to “peel” tissues by removing specific layers and reducing 3D volume into planar images, can critically improve visualization and analysis.
Results
We developed VolumePeeler, a versatile FIJI plugin for virtual 3D “peeling” of image stacks. The plugin implements spherical and spline surface projections. We applied VolumePeeler to perform peeling in 3D images of spherical embryos, as well as non-spherical tissue layers. The produced images improve the 3D volume visualization and enable analysis and quantification of geometrically challenging microscopy datasets.
Availability
ImageJ/FIJI software, source code, examples, and tutorials are openly available in 
https://cimt.uchile.cl/mcerda"
208,Improvement of variables interpretability in kernel PCA,"Background
Kernel methods have been proven to be a powerful tool for the integration and analysis of high-throughput technologies generated data. Kernels offer a nonlinear version of any linear algorithm solely based on dot products. The kernelized version of principal component analysis is a valid nonlinear alternative to tackle the nonlinearity of biological sample spaces. This paper proposes a novel methodology to obtain a data-driven feature importance based on the 
kernel PCA
 representation of the data.
Results
The proposed method, kernel PCA Interpretable Gradient (KPCA-IG), provides a data-driven feature importance that is computationally fast and based solely on linear algebra calculations. It has been compared with existing methods on three benchmark datasets. The accuracy obtained using KPCA-IG selected features is equal to or greater than the other methods’ average. Also, the computational complexity required demonstrates the high efficiency of the method. An exhaustive literature search has been conducted on the selected genes from a publicly available Hepatocellular carcinoma dataset to validate the retained features from a biological point of view. The results once again remark on the appropriateness of the computed ranking.
Conclusions
The black-box nature of kernel PCA needs new methods to interpret the original features. Our proposed methodology KPCA-IG proved to be a valid alternative to select influential variables in high-dimensional high-throughput datasets, potentially unravelling new biological and medical biomarkers."
209,GeCoNet-Tool: a software package for gene co-expression network construction and analysis,"Background
Network analysis is a powerful tool for studying gene regulation and identifying biological processes associated with gene function. However, constructing gene co-expression networks can be a challenging task, particularly when dealing with a large number of missing values.
Results
We introduce GeCoNet-Tool, an integrated gene co-expression network construction and analysis tool. The tool comprises two main parts: network construction and network analysis. In the network construction part, GeCoNet-Tool offers users various options for processing gene co-expression data derived from diverse technologies. The output of the tool is an edge list with the option of weights associated with each link. In network analysis part, the user can produce a table that includes several network properties such as communities, cores, and centrality measures. With GeCoNet-Tool, users can explore and gain insights into the complex interactions between genes."
210,Prognostic analysis of uveal melanoma based on the characteristic genes of M2-type macrophages in the tumor microenvironment,"Uveal melanoma arises from stromal melanocytes and is the most prevalent primary intraocular tumor in adults. It poses a significant diagnostic and therapeutic challenge due to its high malignancy and early onset of metastases. In recent years, there has been a growing interest in the role of diverse immune cells in tumor cell development and metastasis. Using The Cancer Genome Atlas and the gene expression omnibus databases, and the CIBERSORT method, we investigated the topography of intra-tumor immune infiltration in uveal melanoma in this research. We evaluated the prognosis of uveal melanoma patients using the M2 macrophage immune cell infiltration score in conjunction with clinical tumor patient data. We built a prognostic model based on the distinctive genes of M2 macrophages and combined it with patients’ clinical data in the database; we ran a survival prognostic analysis to authenticate the model’s accuracy. The functional study revealed the importance of macrophage-associated genes in the development of uveal melanoma. Moreover, the reliability of our prediction model was verified by combining tumor mutational load, immune checkpoint, and drug sensitivity, respectively. Our study provides a reference for the follow-up study of uveal melanoma."
211,Supervised topological data analysis for MALDI mass spectrometry imaging applications,"Background
Matrix-assisted laser desorption/ionization mass spectrometry imaging (MALDI MSI) displays significant potential for applications in cancer research, especially in tumor typing and subtyping. Lung cancer is the primary cause of tumor-related deaths, where the most lethal entities are adenocarcinoma (ADC) and squamous cell carcinoma (SqCC). Distinguishing between these two common subtypes is crucial for therapy decisions and successful patient management.
Results
We propose a new algebraic topological framework, which obtains intrinsic information from MALDI data and transforms it to reflect topological persistence. Our framework offers two main advantages. Firstly, topological persistence aids in distinguishing the signal from noise. Secondly, it compresses the MALDI data, saving storage space and optimizes computational time for subsequent classification tasks. We present an algorithm that efficiently implements our topological framework, relying on a single tuning parameter. Afterwards, logistic regression and random forest classifiers are employed on the extracted persistence features, thereby accomplishing an automated tumor (sub-)typing process. To demonstrate the competitiveness of our proposed framework, we conduct experiments on a real-world MALDI dataset using cross-validation. Furthermore, we showcase the effectiveness of the single denoising parameter by evaluating its performance on synthetic MALDI images with varying levels of noise.
Conclusion
Our empirical experiments demonstrate that the proposed algebraic topological framework successfully captures and leverages the intrinsic spectral information from MALDI data, leading to competitive results in classifying lung cancer subtypes. Moreover, the framework’s ability to be fine-tuned for denoising highlights its versatility and potential for enhancing data analysis in MALDI applications."
212,VGAEDTI: drug-target interaction prediction based on variational inference and graph autoencoder,"Motivation
Accurate identification of Drug-Target Interactions (DTIs) plays a crucial role in many stages of drug development and drug repurposing. (i) Traditional methods do not consider the use of multi-source data and do not consider the complex relationship between data sources. (ii) How to better mine the hidden features of drug and target space from high-dimensional data, and better solve the accuracy and robustness of the model.
Results
To solve the above problems, a novel prediction model named VGAEDTI is proposed in this paper. We constructed a heterogeneous network with multiple sources of information using multiple types of drug and target dataIn order to obtain deeper features of drugs and targets, we use two different autoencoders. One is variational graph autoencoder (VGAE) which is used to infer feature representations from drug and target spaces. The second is graph autoencoder (GAE) propagating labels between known DTIs. Experimental results on two public datasets show that the prediction accuracy of VGAEDTI is better than that of six DTIs prediction methods. These results indicate that model can predict new DTIs and provide an effective tool for accelerating drug development and repurposing."
213,AMEND: active module identification using experimental data and network diffusion,"Background
Molecular interaction networks have become an important tool in providing context to the results of various omics experiments. For example, by integrating transcriptomic data and protein–protein interaction (PPI) networks, one can better understand how the altered expression of several genes are related with one another. The challenge then becomes how to determine, in the context of the interaction network, the subset(s) of genes that best captures the main mechanisms underlying the experimental conditions. Different algorithms have been developed to address this challenge, each with specific biological questions in mind. One emerging area of interest is to determine which genes are equivalently or inversely changed between different experiments. The equivalent change index (ECI) is a recently proposed metric that measures the extent to which a gene is equivalently or inversely regulated between two experiments. The goal of this work is to develop an algorithm that makes use of the ECI and powerful network analysis techniques to identify a connected subset of genes that are highly relevant to the experimental conditions.
Results
To address the above goal, we developed a method called Active Module identification using Experimental data and Network Diffusion (AMEND). The AMEND algorithm is designed to find a subset of connected genes in a PPI network that have large experimental values. It makes use of random walk with restart to create gene weights, and a heuristic solution to the Maximum-weight Connected Subgraph problem using these weights. This is performed iteratively until an optimal subnetwork (i.e., active module) is found. AMEND was compared to two current methods, NetCore and DOMINO, using two gene expression datasets.
Conclusion
The AMEND algorithm is an effective, fast, and easy-to-use method for identifying network-based active modules. It returned connected subnetworks with the largest median ECI by magnitude, capturing distinct but related functional groups of genes. Code is freely available at 
https://github.com/samboyd0/AMEND
."
214,MOKPE: drug–target interaction prediction via manifold optimization based kernel preserving embedding,"Background
In many applications of bioinformatics, data stem from distinct heterogeneous sources. One of the well-known examples is the identification of drug–target interactions (DTIs), which is of significant importance in drug discovery. In this paper, we propose a novel framework, manifold optimization based kernel preserving embedding (MOKPE), to efficiently solve the problem of modeling heterogeneous data. Our model projects heterogeneous drug and target data into a unified embedding space by preserving drug–target interactions and drug–drug, target–target similarities simultaneously.
Results
We performed ten replications of ten-fold cross validation on four different drug–target interaction network data sets for predicting DTIs for previously unseen drugs. The classification evaluation metrics showed better or comparable performance compared to previous similarity-based state-of-the-art methods. We also evaluated MOKPE on predicting unknown DTIs of a given network. Our implementation of the proposed algorithm in R together with the scripts that replicate the reported experiments is publicly available at 
https://github.com/ocbinatli/mokpe
."
215,DeeP4med: deep learning for P4 medicine to predict normal and cancer transcriptome in multiple human tissues,"Background
P4 medicine (predict, prevent, personalize, and participate) is a new approach to diagnosing and predicting diseases on a patient-by-patient basis. For the prevention and treatment of diseases, prediction plays a fundamental role. One of the intelligent strategies is the design of deep learning models that can predict the state of the disease using gene expression data.
Results
We create an autoencoder deep learning model called DeeP4med, including a Classifier and a Transferor that predicts cancer's gene expression (mRNA) matrix from its matched normal sample and vice versa. The range of the F1 score of the model, depending on tissue type in the Classifier, is from 0.935 to 0.999 and in Transferor from 0.944 to 0.999. The accuracy of DeeP4med for tissue and disease classification was 0.986 and 0.992, respectively, which performed better compared to seven classic machine learning models (Support Vector Classifier, Logistic Regression, Linear Discriminant Analysis, Naive Bayes, Decision Tree, Random Forest, K Nearest Neighbors).
Conclusions
Based on the idea of DeeP4med, by having the gene expression matrix of a normal tissue, we can predict its tumor gene expression matrix and, in this way, find effective genes in transforming a normal tissue into a tumor tissue. Results of Differentially Expressed Genes (DEGs) and enrichment analysis on the predicted matrices for 13 types of cancer showed a good correlation with the literature and biological databases. This led that by using the gene expression matrix, to train the model with features of each person in a normal and cancer state, this model could predict diagnosis based on gene expression data from healthy tissue and be used to identify possible therapeutic interventions for those patients."
216,The prognostic value and immune landscaps of m6A/m5C-related lncRNAs signature in the low grade glioma,"Background
N6-methyladenosine (m6A) and 5-methylcytosine (m5C) are the main RNA methylation modifications involved in the oncogenesis of cancer. However, it remains obscure whether m6A/m5C-related long non-coding RNAs (lncRNAs) affect the development and progression of low grade gliomas (LGG).
Methods
We summarized 926 LGG tumor samples with RNA-seq data and clinical information from The Cancer Genome Atlas and Chinese Glioma Genome Atlas. 105 normal brain samples with RNA-seq data from the Genotype Tissue Expression project were collected for control. We obtained a molecular classification cluster from the expression pattern of sreened lncRNAs. The least absolute shrinkage and selection operator Cox regression was employed to construct a m6A/m5C-related lncRNAs prognostic signature of LGG. In vitro experiments were employed to validate the biological functions of lncRNAs in our risk model.
Results
The expression pattern of 14 sreened highly correlated lncRNAs could cluster samples into two groups, in which various clinicopathological features and the tumor immune microenvironment were significantly distinct. The survival time of cluster 1 was significantly reduced compared with cluster 2. This prognostic signature is based on 8 m6A/m5C-related lncRNAs (GDNF-AS1, HOXA-AS3, LINC00346, LINC00664, LINC00665, MIR155HG, NEAT1, RHPN1-AS1). Patients in the high-risk group harbored shorter survival times. Immunity microenvironment analysis showed B cells, CD4 + T cells, macrophages, and myeloid-derived DC cells were significantly increased in the high-risk group. Patients in high-risk group had the worse overall survival time regardless of followed TMZ therapy or radiotherapy. All observed results from the TCGA-LGG cohort could be validated in CGGA cohort. Afterwards, LINC00664 was found to promote cell viability, invasion and migration ability of glioma cells in vitro.
Conclusion
Our study elucidated a prognostic prediction model of LGG by 8 m6A/m5C methylated lncRNAs and a critical lncRNA regulation function involved in LGG progression. High-risk patients have shorter survival times and a pro-tumor immune microenvironment."
217,Deep ensemble approach for pathogen classification in large-scale images using patch-based training and hyper-parameter optimization,"Pathogenic bacteria present a major threat to human health, causing various infections and illnesses, and in some cases, even death. The accurate identification of these bacteria is crucial, but it can be challenging due to the similarities between different species and genera. This is where automated classification using convolutional neural network (CNN) models can help, as it can provide more accurate, authentic, and standardized results.In this study, we aimed to create a larger and balanced dataset by image patching and applied different variations of CNN models, including training from scratch, fine-tuning, and weight adjustment, and data augmentation through random rotation, reflection, and translation. The results showed that the best results were achieved through augmentation and fine-tuning of deep models. We also modified existing architectures, such as InceptionV3 and MobileNetV2, to better capture complex features. The robustness of the proposed ensemble model was evaluated using two data splits (7:2:1 and 6:2:2) to see how performance changed as the training data was increased from 10 to 20%. In both cases, the model exhibited exceptional performance. For the 7:2:1 split, the model achieved an accuracy of 99.91%, F-Score of 98.95%, precision of 98.98%, recall of 98.96%, and MCC of 98.92%. For the 6:2:2 split, the model yielded an accuracy of 99.94%, F-Score of 99.28%, precision of 99.31%, recall of 98.96%, and MCC of 99.26%. This demonstrates that automatic classification using the ensemble model can be a valuable tool for diagnostic staff and microbiologists in accurately identifying pathogenic bacteria, which in turn can help control epidemics and minimize their social and economic impact."
218,Comparing methods for drug–gene interaction prediction on the biomedical literature knowledge graph: performance versus explainability,"This paper applies different link prediction methods on a knowledge graph generated from biomedical literature, with the aim to compare their ability to identify unknown drug-gene interactions and explain their predictions. Identifying novel drug–target interactions is a crucial step in drug discovery and repurposing. One approach to this problem is to predict missing links between drug and gene nodes, in a graph that contains relevant biomedical knowledge. Such a knowledge graph can be extracted from biomedical literature, using text mining tools. In this work, we compare state-of-the-art graph embedding approaches and contextual path analysis on the interaction prediction task. The comparison reveals a trade-off between predictive accuracy and explainability of predictions. Focusing on explainability, we train a decision tree on model predictions and show how it can aid the understanding of the prediction process. We further test the methods on a drug repurposing task and validate the predicted interactions against external databases, with very encouraging results."
219,Neuroimaging feature extraction using a neural network classifier for imaging genetics,"Background
Dealing with the high dimension of both neuroimaging data and genetic data is a difficult problem in the association of genetic data to neuroimaging. In this article, we tackle the latter problem with an eye toward developing solutions that are relevant for disease prediction. Supported by a vast literature on the predictive power of neural networks, our proposed solution uses neural networks to extract from neuroimaging data features that are relevant for predicting Alzheimer’s Disease (AD) for subsequent relation to genetics. The neuroimaging-genetic pipeline we propose is comprised of image processing, neuroimaging feature extraction and genetic association steps. We present a neural network classifier for extracting neuroimaging features that are related with the disease. The proposed method is data-driven and requires no expert advice or a priori selection of regions of interest. We further propose a multivariate regression with priors specified in the Bayesian framework that allows for group sparsity at multiple levels including SNPs and genes.
Results
We find the features extracted with our proposed method are better predictors of AD than features used previously in the literature suggesting that single nucleotide polymorphisms (SNPs) related to the features extracted by our proposed method are also more relevant for AD. Our neuroimaging-genetic pipeline lead to the identification of some overlapping and more importantly some different SNPs when compared to those identified with previously used features.
Conclusions
The pipeline we propose combines machine learning and statistical methods to benefit from the strong predictive performance of blackbox models to extract relevant features while preserving the interpretation provided by Bayesian models for genetic association. Finally, we argue in favour of using automatic feature extraction, such as the method we propose, in addition to ROI or voxelwise analysis to find potentially novel disease-relevant SNPs that may not be detected when using ROIs or voxels alone."
220,Comprehensive analysis of KLF2 as a prognostic biomarker associated with fibrosis and immune infiltration in advanced hepatocellular carcinoma,"Purpose
Most Hepatocellular carcinoma (HCC) patients are in advanced or metastatic stage at the time of diagnosis. Prognosis for advanced HCC patients is dismal. This study was based on our previous microarray results, and aimed to explore the promising diagnostic and prognostic markers for advanced HCC by focusing on the important function of KLF2.
Methods
The Cancer Genome Atlas (TCGA), Cancer Genome Consortium database (ICGC), and the Gene Expression Comprehensive Database (GEO) provided the raw data of this study research. The cBioPortal platform, CeDR Atlas platform, and the Human Protein Atlas (HPA) website were applied to analyze the mutational landscape and single-cell sequencing data of KLF2. Basing on the results of single-cell sequencing analyses, we further explored the molecular mechanism of KLF2 regulation in the fibrosis and immune infiltration of HCC.
Results
Decreased KLF2 expression was discovered to be mainly regulated by hypermethylation, and indicated a poor prognosis of HCC. Single-cell level expression analyses revealed KLF2 was highly expressed in immune cells and fibroblasts. The function enrichment analysis of KLF2 targets indicated the crucial association between KLF2 and tumor matrix. 33-genes related with cancer associated fibroblasts (CAFs) were collected to identify the significant association of KLF2 with fibrosis. And SPP1 was validated as a promising prognostic and diagnostic marker for advanced HCC patients. CXCR6 CD8
+
 T cells were noted as a predominant proportion in the immune microenvironment, and T cell receptor CD3D was discovered to be a potential therapeutic biomarker for HCC immunotherapy.
Conclusion
This study identified that KLF2 is an important factor promoting HCC progression by affecting the fibrosis and immune infiltration, highlighting its great potential as a novel prognostic biomarker for advanced HCC."
221,Multiomics data analyses to identify SLC25A17 as a novel biomarker to predict the prognosis and immune microenvironment in head and neck squamous cell carcinoma,"Objective
This study aims to explore the predictive value of SLC25A17 in the prognosis and tumor microenvironment (TME) of patients with head and neck squamous cell carcinoma (HNSCC) and to provide ideas for individual clinical treatment.
Methods
A pancancer analysis of the differential expression of SLC25A17 among different tumors was first conducted via the TIMER 2.0 database. Subsequently, the expression of SLC25A17 and related clinical information of HNSCC patients were obtained from the TCGA database, and patients were divided into two groups according to the median value of SLC25A17 expression. K‒M survival analysis was conducted to compare the overall survival (OS) and progression-free survival (PFS) between the groups. The Wilcoxon test was used to compare the distribution of SLC25A17 in different clinical characteristics, and univariate Cox and multivariate Cox analyses were performed to analyze independent prognostic factors to establish a predictive nomogram. Calibration curves were generated to verify the reliability of predicting 1-year, 3-year and 5-year survival rates and another cohort (GSE65858) was used for external validation. Gene set enrichment analysis was conducted to compare the enriched pathways, and the immune microenvironment was assessed using the CIBERSORT and estimate packages. Furthermore, the expression levels of SLC25A17 in immune cells were also analyzed with single-cell RNA-seq via the TISCH. Moreover, the immunotherapeutic response and chemotherapy drug sensitivity were compared between the two groups to guide precise treatment. The TIDE database was applied to predict the possibility of immune escape in the TCGA-HNSC cohort.
Results
Compared with normal samples, the expression of SLC25A17 was much higher in HNSCC tumor samples. For patients with high SLC25A17 expression, the OS and PFS were shorter than those with low SLC25A17 expression, indicating a worse prognosis. The expression of SLC25A17 varied in different clinical features. Univariate Cox and multivariate COX analyses showed that SLC25A17, age, and lymph node metastasis are independent prognostic risk factors for HNSCC, and the survival prediction model based on these factors had reliable predictive value. Patients in the low-expression group exhibited more immune cell infiltration, higher TME scores, higher IPS scores and lower TIDE scores than those in the high-expression groups, suggesting better immunotherapeutic response with lower SLC25A17 expression. Moreover, patients in the high-expression group were more sensitive to chemotherapy.
Conclusions
SLC25A17 can effectively predict the prognosis of HNSCC patients and could be a precise individual-targeted indicator for the treatment of HNSCC patients."
222,"topr
: an R package for viewing and annotating genetic association results","Background
The successful identification of genetic loci for complex traits in genome-wide association studies (GWAS) has resulted in thousands of GWAS summary statistics becoming publicly available for hundreds of complex traits from multiple cohorts and studies. Visualisation is an important aid for interpreting, comparing, validating, and obtaining an overview of large amounts of data. However, the current software is limited in its ability and flexibility to annotate and simultaneously display multiple GWAS results which is useful when interpreting and comparing association results. Therefore, I created the 
topr
 R package to facilitate visualisation, annotation, and comparisons of single or multiple GWAS results. It contains functions tailored for viewing and analysing GWAS results.
Results
topr
 provides a fast and elegant visual display of association results, along with the annotation of association peaks with their nearest gene. Association results from multiple analyses can be viewed simultaneously over the entire genome or in a more detailed regional view along with gene information. Users can perform the essential steps of visually exploring and annotating association results and generating elegant publication-ready plots.
Conclusions
topr
 is developed as a package for the R statistical computing environment, released under the GNU General Public License, and is freely available on the Comprehensive R Archive Network (
http://cran.r-project.org/package=topr
). The source code is available at GitHub (
https://github.com/totajuliusd/topr
). 
topr
 provides several advantages and advances over the current alternatives, particularly in its gene annotation functionality and customisable display of single- or multiple-association results. With 
topr
, I provide a flexible tool with multiple features to aid in the analysis and evaluation of GWAS association results."
223,Cancer survival prediction by learning comprehensive deep feature representation for multiple types of genetic data,"Background
Cancer is one of the leading death causes around the world. Accurate prediction of its survival time is significant, which can help clinicians make appropriate therapeutic schemes. Cancer data can be characterized by varied molecular features, clinical behaviors and morphological appearances. However, the cancer heterogeneity problem usually makes patient samples with different risks (i.e., short and long survival time) inseparable, thereby causing unsatisfactory prediction results. Clinical studies have shown that genetic data tends to contain more molecular biomarkers associated with cancer, and hence integrating multi-type genetic data may be a feasible way to deal with cancer heterogeneity. Although multi-type gene data have been used in the existing work, how to learn more effective features for cancer survival prediction has not been well studied.
Results
To this end, we propose a deep learning approach to reduce the negative impact of cancer heterogeneity and improve the cancer survival prediction effect. It represents each type of genetic data as the shared and specific features, which can capture the consensus and complementary information among all types of data. We collect mRNA expression, DNA methylation and microRNA expression data for four cancers to conduct experiments.
Conclusions
Experimental results demonstrate that our approach substantially outperforms established integrative methods and is effective for cancer survival prediction.
Availability and implementation
https://github.com/githyr/ComprehensiveSurvival
."
224,PATH-SURVEYOR: pathway level survival enquiry for immuno-oncology and drug repurposing,"Pathway-level survival analysis offers the opportunity to examine molecular pathways and immune signatures that influence patient outcomes. However, available survival analysis algorithms are limited in pathway-level function and lack a streamlined analytical process. Here we present a comprehensive pathway-level survival analysis suite, PATH-SURVEYOR, which includes a Shiny user interface with extensive features for systematic exploration of pathways and covariates in a Cox proportional-hazard model. Moreover, our framework offers an integrative strategy for performing Hazard Ratio ranked Gene Set Enrichment Analysis and pathway clustering. As an example, we applied our tool in a combined cohort of melanoma patients treated with checkpoint inhibition (ICI) and identified several immune populations and biomarkers predictive of ICI efficacy. We also analyzed gene expression data of pediatric acute myeloid leukemia (AML) and performed an inverse association of drug targets with the patient’s clinical endpoint. Our analysis derived several drug targets in high-risk KMT2A-fusion-positive patients, which were then validated in AML cell lines in the Genomics of Drug Sensitivity database. Altogether, the tool offers a comprehensive suite for pathway-level survival analysis and a user interface for exploring drug targets, molecular features, and immune populations at different resolutions."
225,Advances in monolingual and crosslingual automatic disability annotation in Spanish,"Background
Unlike diseases, automatic recognition of disabilities has not received the same attention in the area of medical NLP. Progress in this direction is hampered by obstacles like the lack of annotated corpus. Neural architectures learn to translate sequences from spontaneous representations into their corresponding standard representations given a set of samples. The aim of this paper is to present the last advances in monolingual (Spanish) and crosslingual (from English to Spanish and vice versa) automatic disability annotation. The task consists of identifying disability mentions in medical texts written in Spanish within a collection of abstracts from journal papers related to the biomedical domain.
Results
In order to carry out the task, we have combined deep learning models that use different embedding granularities for sequence to sequence tagging with a simple acronym and abbreviation detection module to boost the coverage.
Conclusions
Our monolingual experiments demonstrate that a good combination of different word embedding representations provide better results than single representations, significantly outperforming the state of the art in disability annotation in Spanish. Additionally, we have experimented crosslingual transfer (zero-shot) for disability annotation between English and Spanish with interesting results that might help overcoming the data scarcity bottleneck, specially significant for the disabilities."
226,Gene regulatory network inference based on a nonhomogeneous dynamic Bayesian network model with an improved Markov Monte Carlo sampling,"A nonhomogeneous dynamic Bayesian network model, which combines the dynamic Bayesian network and the multi-change point process, solves the limitations of the dynamic Bayesian network in modeling non-stationary gene expression data to a certain extent. However, certain problems persist, such as the low network reconstruction accuracy and poor model convergence. Therefore, we propose an MD-birth move based on the Manhattan distance of the data points to increase the rationality of the multi-change point process. The underlying concept of the MD-birth move is that the direction of movement of the change point is assumed to have a larger Manhattan distance between the variance and the mean of its left and right data points. Considering the data instability characteristics, we propose a Markov chain Monte Carlo sampling method based on node-dependent particle filtering in addition to the multi-change point process. The candidate parent nodes to be sampled, which are close to the real state, are pushed to the high probability area through the particle filter, and the candidate parent node set to be sampled that is far from the real state is pushed to the low probability area and then sampled. In terms of reconstructing the gene regulatory network, the model proposed in this paper (FC-DBN) has better network reconstruction accuracy and model convergence speed than other corresponding models on the Saccharomyces cerevisiae data and RAF data."
227,Fast and accurate genome-wide predictions and structural modeling of protein–protein interactions using Galaxy,"Background
Protein–protein interactions play a crucial role in almost all cellular processes. Identifying interacting proteins reveals insight into living organisms and yields novel drug targets for disease treatment. Here, we present a publicly available, automated pipeline to predict genome-wide protein–protein interactions and produce high-quality multimeric structural models.
Results
Application of our method to the Human and Yeast genomes yield protein–protein interaction networks similar in quality to common experimental methods. We identified and modeled Human proteins likely to interact with the papain-like protease of SARS-CoV2’s non-structural protein 3. We also produced models of SARS-CoV2’s spike protein (S) interacting with myelin-oligodendrocyte glycoprotein receptor and dipeptidyl peptidase-4.
Conclusions
The presented method is capable of confidently identifying interactions while providing high-quality multimeric structural models for experimental validation. The interactome modeling pipeline is available at usegalaxy.org and usegalaxy.eu."
228,Assumptions on decision making and environment can yield multiple steady states in microbial community models,"Background
Microbial community simulations using genome scale metabolic networks (GSMs) are relevant for many application areas, such as the analysis of the human microbiome. Such simulations rely on assumptions about the culturing environment, affecting if the culture may reach a metabolically stationary state with constant microbial concentrations. They also require assumptions on decision making by the microbes: metabolic strategies can be in the interest of individual community members or of the whole community. However, the impact of such common assumptions on community simulation results has not been investigated systematically.
Results
Here, we investigate four combinations of assumptions, elucidate how they are applied in literature, provide novel mathematical formulations for their simulation, and show how the resulting predictions differ qualitatively. Our results stress that different assumption combinations give qualitatively different predictions on microbial coexistence by differential substrate utilization. This fundamental mechanism is critically under explored in the steady state GSM literature with its strong focus on coexistence states due to crossfeeding (division of labor). Furthermore, investigating a realistic synthetic community, where the two involved strains exhibit no growth in isolation, but grow as a community, we predict multiple modes of cooperation, even without an explicit cooperation mechanism.
Conclusions
Steady state GSM modelling of microbial communities relies both on assumed decision making principles and environmental assumptions. In principle, dynamic flux balance analysis addresses both. In practice, our methods that address the steady state directly may be preferable, especially if the community is expected to display multiple steady states."
229,DeepASDPred: a CNN-LSTM-based deep learning method for Autism spectrum disorders risk RNA identification,"Background
Autism spectrum disorders (ASD) are a group of neurodevelopmental disorders characterized by difficulty communicating with society and others, behavioral difficulties, and a brain that processes information differently than normal. Genetics has a strong impact on ASD associated with early onset and distinctive signs. Currently, all known ASD risk genes are able to encode proteins, and some de novo mutations disrupting protein-coding genes have been demonstrated to cause ASD. Next-generation sequencing technology enables high-throughput identification of ASD risk RNAs. However, these efforts are time-consuming and expensive, so an efficient computational model for ASD risk gene prediction is necessary.
Results
In this study, we propose DeepASDPerd, a predictor for ASD risk RNA based on deep learning. Firstly, we use K-mer to feature encode the RNA transcript sequences, and then fuse them with corresponding gene expression values to construct a feature matrix. After combining chi-square test and logistic regression to select the best feature subset, we input them into a binary classification prediction model constructed by convolutional neural network and long short-term memory for training and classification. The results of the tenfold cross-validation proved our method outperformed the state-of-the-art methods. Dataset and source code are available at 
https://github.com/Onebear-X/DeepASDPred
 is freely available.
Conclusions
Our experimental results show that DeepASDPred has outstanding performance in identifying ASD risk RNA genes."
230,BeEM: fast and faithful conversion of mmCIF format structure files to PDB format,"Background
Although mmCIF is the current official format for deposition of protein and nucleic acid structures to the protein data bank (PDB) database, the legacy PDB format is still the primary supported format for many structural bioinformatics tools. Therefore, reliable software to convert mmCIF structure files to PDB files is needed. Unfortunately, existing conversion programs fail to correctly convert many mmCIF files, especially those with many atoms and/or long chain identifies.
Results
This study proposed BeEM, which converts any mmCIF format structure files to PDB format. BeEM conversion faithfully retains all atomic and chain information, including chain IDs with more than 2 characters, which are not supported by any existing mmCIF to PDB converters. The conversion speed of BeEM is at least ten times faster than existing converters such as MAXIT and Phenix. Part of the reason for the speed improvement is the avoidance of conversion between numerical values and text strings.
Conclusion
BeEM is a fast and accurate tool for mmCIF-to-PDB format conversion, which is a common procedure in structural biology. The source code is available under the BSD licence at 
https://github.com/kad-ecoli/BeEM/
."
231,A MATLAB-based app to improve LC–MS/MS data analysis for N-linked glycan peak identification,"Background
Glycosylation is an important modification to proteins that plays a significant role in biological processes. Glycan structures are characterized by liquid chromatography (LC) combined with mass spectrometry (MS), but data interpretation of LC/MS and MS/MS data can be time-consuming and arduous when analyzed manually. Most of glycan analysis requires dedicated glycobioinformatics tools to process MS data, identify glycan structure, and display the results. However, software tools currently available are either too costly or heavily focused on academic applications, limiting their use within the biopharmaceutical industry for implementing the standardized LC/MS glycan analysis in high-throughput manner. Additionally, few tools provide the capability to generate report-ready annotated MS/MS glycan spectra.
Results
Here, we present a MATLAB-based app, GlyKAn AZ, which can automate data processing, glycan identification, and customizable result displays in a streamlined workflow. MS1 and MS2 mass search algorithms along with glycan databases were developed to confirm the fluorescent labeled N-linked glycan species based on accurate mass. A user-friendly graphical user interface (GUI) streamlines the data analysis process, making it easy to implement the software tool in biopharmaceutical analytical laboratories. The databases provided with the app can be expanded through the Fragment Generator functionality which automatically identifies fragmentation patterns for new glycans. The GlyKAn AZ app can automatically annotate the MS/MS spectra, yet this data display feature remains flexible and customizable by users, saving analysts’ time in generating individual report-ready spectra figures. This app accepts both OrbiTrap and matrix-assisted laser desorption/ionization–time of flight (MALDI–TOF) MS data and was successfully validated by identifying all glycan species that were previously identified manually.
Conclusions
The GlyKAn AZ app was developed to expedite glycan analysis while maintaining a high level of accuracy in positive identifications. The app’s customizable user inputs, polished figures and tables, and unique calculated outputs set it apart from similar software and greatly improve the current manual analysis workflow. Overall, this app serves as a tool for streamlining glycan identification for both academic and industrial needs."
232,Covariance regression with random forests,"Capturing the conditional covariances or correlations among the elements of a multivariate response vector based on covariates is important to various fields including neuroscience, epidemiology and biomedicine. We propose a new method called Covariance Regression with Random Forests (CovRegRF) to estimate the covariance matrix of a multivariate response given a set of covariates, using a random forest framework. Random forest trees are built with a splitting rule specially designed to maximize the difference between the sample covariance matrix estimates of the child nodes. We also propose a significance test for the partial effect of a subset of covariates. We evaluate the performance of the proposed method and significance test through a simulation study which shows that the proposed method provides accurate covariance matrix estimates and that the Type-1 error is well controlled. An application of the proposed method to thyroid disease data is also presented. 
CovRegRF
 is implemented in a freely available R package on CRAN."
233,"Exploration of m
6
A methylation regulators as epigenetic targets for immunotherapy in advanced sepsis","Background
This study aims to deeply explore the relationship between m
6
A methylation modification and peripheral immune cells in patients with advanced sepsis and mine potential epigenetic therapeutic targets by analyzing the differential expression patterns of m
6
A-related genes in healthy subjects and advanced sepsis patients.
Methods
A single cell expression dataset of peripheral immune cells containing blood samples from 4 patients with advanced sepsis and 5 healthy subjects was obtained from the gene expression comprehensive database (GSE175453). Differential expression analysis and cluster analysis were performed on 21 m
6
A-related genes. The characteristic gene was identified based on random forest  algorithm, and the correlation between the characteristic gene METTL16 and 23 immune cells in patients with advanced sepsis was evaluated using single-sample gene set enrichment analysis.
Results
IGFBP1, IGFBP2, IGF2BP1, and WTAP were highly expressed in patients with advanced sepsis and m
6
A cluster B. IGFBP1, IGFBP2, and IGF2BP1 were positively correlated with Th17 helper T cells. The characteristic gene METTL16 exhibited a significant positive correlation with the proportion of various immune cells.
Conclusion
IGFBP1, IGFBP2, IGF2BP1, WTAP, and METTL16 may accelerate the development of advanced sepsis by regulating m
6
A methylation modification and promoting immune cell infiltration. The discovery of these characteristic genes related to advanced sepsis provides potential therapeutic targets for the diagnosis and treatment of sepsis."
234,The Poisson distribution model fits UMI-based single-cell RNA-sequencing data,"Background
Modeling of single cell RNA-sequencing (scRNA-seq) data remains challenging due to a high percentage of zeros and data heterogeneity, so improved modeling has strong potential to benefit many downstream data analyses. The existing zero-inflated or over-dispersed models are based on aggregations at either the gene or the cell level. However, they typically lose accuracy due to a too crude aggregation at those two levels.
Results
We avoid the crude approximations entailed by such aggregation through proposing an independent Poisson distribution (IPD) particularly at each individual entry in the scRNA-seq data matrix. This approach naturally and intuitively models the large number of zeros as matrix entries with a very small Poisson parameter. The critical challenge of cell clustering is approached via a novel data representation as Departures from a simple homogeneous IPD (DIPD) to capture the per-gene-per-cell intrinsic heterogeneity generated by cell clusters. Our experiments using real data and crafted experiments show that using DIPD as a data representation for scRNA-seq data can uncover novel cell subtypes that are missed or can only be found by careful parameter tuning using conventional methods.
Conclusions
This new method has multiple advantages, including (1) no need for prior feature selection or manual optimization of hyperparameters; (2) flexibility to combine with and improve upon other methods, such as Seurat. Another novel contribution is the use of crafted experiments as part of the validation of our newly developed DIPD-based clustering pipeline. This new clustering pipeline is implemented in the R (CRAN) package 
scpoisson
."
235,Endoplasmic reticulum stress-related gene model predicts prognosis and guides therapies in lung adenocarcinoma,"Background
The prognosis and survival of lung adenocarcinoma (LUAD) patients are still not promising despite recent breakthroughs in treatment. Endoplasmic reticulum stress (ERS) is a self-protective mechanism resulting from an imbalance in quality control of unfolded proteins when cells are stressed, which plays an active role in lung cancer development, but the relationship between ERS and the pathological characteristics and clinical prognosis of LUAD patients remains unclear.
Methods
LASSO and Cox regression were applied based on sequencing information to construct the model, which was validated to be robust. The risk scores of the patients were calculated using the formula provided by the model, and the patients were divided into high and low-risk groups according to the median cut-off of risk scores. Cox regression analysis identifies independent prognostic factors for these patients, and enrichment analysis of prognosis-related genes was also performed. The relationship between risk scores and tumor mutation burden (TMB), cancer stem cell index, and drug sensitivity was explored.
Results
We constructed a 13-gene prognostic model for LUAD patients. Patients in the high-risk group had worse overall survival, lower immune score and ESTIMATE score, higher TMB, higher cancer stem cell index, and higher sensitivity to conventional chemotherapeutic agents. In addition, we constructed a nomogram that predicts 5-year survival in LUAD patients, which helps clinicians to foresee the prognosis from a new perspective.
Conclusions
Our results highlight the association of ERS with LUAD and the potential use of ERS in guiding treatment."
236,"WormTensor: a clustering method for time-series whole-brain activity data from 
C. elegans","Background
In the field of neuroscience, neural modules and circuits that control biological functions have been found throughout entire neural networks. Correlations in neural activity can be used to identify such neural modules. Recent technological advances enable us to measure whole-brain neural activity with single-cell resolution in several species including 
\(Caenorhabditis\ elegans\)
. Because current neural activity data in 
C. elegans
 contain many missing data points, it is necessary to merge results from as many animals as possible to obtain more reliable functional modules.
Results
In this work, we developed a new time-series clustering method, 
WormTensor
, to identify functional modules using whole-brain activity data from 
C. elegans
. 
WormTensor
 uses a distance measure, modified shape-based distance to account for the lags and the mutual inhibition of cell–cell interactions and applies the tensor decomposition algorithm multi-view clustering based on matrix integration using the higher orthogonal iteration of tensors (HOOI) algorithm (
MC-MI-HOOI
), which can estimate both the weight to account for the reliability of data from each animal and the clusters that are common across animals.
Conclusion
We applied the method to 24 individual 
C. elegans
 and successfully found some known functional modules. Compared with a widely used consensus clustering method to aggregate multiple clustering results, 
WormTensor
 showed higher silhouette coefficients. Our simulation also showed that 
WormTensor
 is robust to contamination from noisy data. 
WormTensor
 is freely available as an 
R
/CRAN package 
https://cran.r-project.org/web/packages/WormTensor
."
237,"Child-Sum EATree-LSTMs
: enhanced attentive 
Child-Sum Tree-LSTMs
 for biomedical event extraction","Background
Tree-structured neural networks have shown promise in extracting lexical representations of sentence syntactic structures, particularly in the detection of event triggers using recursive neural networks.
Methods
In this study, we introduce an attention mechanism into 
Child-Sum Tree-LSTMs
 for the detection of biomedical event triggers. We incorporate previous researches on assigning attention weights to adjacent nodes and integrate this mechanism into 
Child-Sum Tree-LSTMs
 to improve the detection of event trigger words. We also address a limitation of shallow syntactic dependencies in 
Child-Sum Tree-LSTMs
 by integrating deep syntactic dependencies to enhance the effect of the attention mechanism.
Results
Our proposed model, which integrates an enhanced attention mechanism into Tree-LSTM, shows the best performance for the MLEE and BioNLP’09 datasets. Moreover, our model outperforms almost all complex event categories for the BioNLP’09/11/13 test set.
Conclusion
We evaluate the performance of our proposed model with the MLEE and BioNLP datasets and demonstrate the advantage of an enhanced attention mechanism in detecting biomedical event trigger words."
238,eSPRESSO: topological clustering of single-cell transcriptomics data to reveal informative genes for spatio–temporal architectures of cells,"Background
Bioinformatics capability to analyze spatio–temporal dynamics of gene expression is essential in understanding animal development. Animal cells are spatially organized as functional tissues where cellular gene expression data contain information that governs morphogenesis during the developmental process. Although several computational tissue reconstruction methods using transcriptomics data have been proposed, those methods have been ineffective in arranging cells in their correct positions in tissues or organs unless spatial information is explicitly provided.
Results
This study demonstrates stochastic self-organizing map clustering with Markov chain Monte Carlo calculations for optimizing informative genes effectively reconstruct any spatio–temporal topology of cells from their transcriptome profiles with only a coarse topological guideline. The method, eSPRESSO (enhanced SPatial REconstruction by Stochastic Self-Organizing Map), provides a powerful in silico spatio–temporal tissue reconstruction capability, as confirmed by using human embryonic heart and mouse embryo, brain, embryonic heart, and liver lobule with generally high reproducibility (average max. accuracy = 92.0%), while revealing topologically informative genes, or spatial discriminator genes. Furthermore, eSPRESSO was used for temporal analysis of human pancreatic organoids to infer rational developmental trajectories with several candidate ‘temporal’ discriminator genes responsible for various cell type differentiations.
Conclusions
eSPRESSO provides a novel strategy for analyzing mechanisms underlying the spatio–temporal formation of cellular organizations."
239,In-silico assessment of high-risk non-synonymous SNPs in ADAMTS3 gene associated with Hennekam syndrome and their impact on protein stability and function,"Hennekam Lymphangiectasia–Lymphedema Syndrome 3 (HKLLS3) is a rare genetical disorder caused by mutations in a few genes including ADAMTS3. It is characterized by lymphatic dysplasia, intestinal lymphangiectasia, severe lymphedema and distinctive facial appearance. Up till now, no extensive studies have been conducted to elucidate the mechanism of the disease caused by various mutations. As a preliminary investigation of HKLLS3, we sorted out the most deleterious nonsynonymous single nucleotide polymorphisms (nsSNPs) that might affect the structure and function of ADAMTS3 protein by using a variety of in silico tools. A total of 919 nsSNPs in the ADAMTS3 gene were identified. 50 nsSNPs were predicted to be deleterious by multiple computational tools. 5 nsSNPs (G298R, C567Y, A370T, C567R and G374S) were found to be the most dangerous and can be associated with the disease as predicted by different bioinformatics tools. Modelling of the protein shows it can be divided into segments 1, 2 and 3, which are connected by short loops. Segment 3 mainly consists of loops without substantial secondary structures. With prediction tools and molecular dynamics simulation, some SNPs were found to significantly destabilize the protein structure and disrupt the secondary structures, especially in segment 2. The deleterious effects of mutations in segment 1 are possibly not from destabilization but from other factors such as the change in phosphorylation as suggested by post-translational modification (PTM) studies. This is the first-ever study of ADAMTS3 gene polymorphism, and the predicted nsSNPs in ADAMST3, some of which have not been reported yet in patients, will serve for diagnostic purposes and further therapeutic implications in Hennekam syndrome, contributing to better diagnosis and treatment."
240,Statistical methods and resources for biomarker discovery using metabolomics,"Metabolomics is a dynamic tool for elucidating biochemical changes in human health and disease. Metabolic profiles provide a close insight into physiological states and are highly volatile to genetic and environmental perturbations. Variation in metabolic profiles can inform mechanisms of pathology, providing potential biomarkers for diagnosis and assessment of the risk of contracting a disease. With the advancement of high-throughput technologies, large-scale metabolomics data sources have become abundant. As such, careful statistical analysis of intricate metabolomics data is essential for deriving relevant and robust results that can be deployed in real-life clinical settings. Multiple tools have been developed for both data analysis and interpretations. In this review, we survey statistical approaches and corresponding statistical tools that are available for discovery of biomarkers using metabolomics."
241,RegCloser: a robust regression approach to closing genome gaps,"Background
Closing gaps in draft genomes leads to more complete and continuous genome assemblies. The ubiquitous genomic repeats are challenges to the existing gap-closing methods, based on either the k-mer representation by the de Bruijn graph or the overlap-layout-consensus paradigm. Besides, chimeric reads will cause erroneous k-mers in the former and false overlaps of reads in the latter.
Results
We propose a novel local assembly approach to gap closing, called RegCloser. It represents read coordinates and their overlaps respectively by parameters and observations in a linear regression model. The optimal overlap is searched only in the restricted range consistent with insert sizes. Under this linear regression framework, the local DNA assembly becomes a robust parameter estimation problem. We solved the problem by a customized robust regression procedure that resists the influence of false overlaps by optimizing a convex global Huber loss function. The global optimum is obtained by iteratively solving the sparse system of linear equations. On both simulated and real datasets, RegCloser outperformed other popular methods in accurately resolving the copy number of tandem repeats, and achieved superior completeness and contiguity. Applying RegCloser to a plateau zokor draft genome that had been improved by long reads further increased contig N50 to 3-fold long. We also tested the robust regression approach on layout generation of long reads.
Conclusions
RegCloser is a competitive gap-closing tool. The software is available at 
https://github.com/csh3/RegCloser
. The robust regression approach has a prospect to be incorporated into the layout module of long read assemblers."
242,SBMLKinetics: a tool for annotation-independent classification of reaction kinetics for SBML models,"Background


Reaction networks are widely used as mechanistic models in systems biology to reveal principles of biological systems. Reactions are governed by kinetic laws that describe reaction rates. Selecting the appropriate kinetic laws is difficult for many modelers. There exist tools that attempt to find the correct kinetic laws based on annotations. Here, I developed annotation-independent technologies that assist modelers by focusing on finding kinetic laws commonly used for similar reactions.


Results


Recommending kinetic laws and other analyses of reaction networks can be viewed as a classification problem. Existing approaches to determining similar reactions rely heavily on having good annotations, a condition that is often unsatisfied in model repositories such as BioModels. I developed an annotation-independent approach to find similar reactions via reaction classifications. I proposed a two-dimensional kinetics classification scheme (2DK) that analyzed reactions along the dimensions of kinetics type (K type) and reaction type (R type). I identified approximately ten mutually exclusive K types, including zeroth order, mass action, Michaelis–Menten, Hill kinetics, and others. R types were organized by the number of distinct reactants and the number of distinct products in reactions. I constructed a tool, SBMLKinetics, that inputted a collection of SBML models and then calculated reaction classifications as the probability of each 2DK class. The effectiveness of 2DK was evaluated on BioModels, and the scheme classified over 95% of the reactions.


Conclusions


2DK had many applications. It provided a 
data-driven
 annotation-independent approach to recommending kinetic laws by using type common for the kind of models in combination with the R type of the reactions. Alternatively, 2DK could also be used to alert users that a kinetic law was unusual for the K type and R type. Last, 2DK provided a way to analyze groups of models to compare their kinetic laws. I applied 2DK to BioModels to compare the kinetics of signaling networks with the kinetics of metabolic networks and found significant differences in K type distributions."
243,CastNet: a systems-level sequence evolution simulator,"Background
Simulating DNA evolution has been done through coevolution-agnostic probabilistic frameworks for the past 3 decades. The most common implementation is by using the converse of the probabilistic approach used to infer phylogenies which, in the simplest form, simulates a single sequence at a time. However, biological systems are multi-genic, and gene products can affect each other’s evolutionary paths through coevolution. These crucial evolutionary dynamics still remain to be simulated, and we believe that modelling them can lead to profound insights for comparative genomics.
Results
Here we present CastNet, a genome evolution simulator that assumes each genome is a collection of genes with constantly evolving regulatory interactions in between them. The regulatory interactions produce a phenotype in the form of gene expression profiles, upon which fitness is calculated. A genetic algorithm is then used to evolve a population of such entities through a user-defined phylogeny. Importantly, the regulatory mutations are a response to sequence mutations, thus making a 1–1 relationship between the rate of evolution of sequences and of regulatory parameters. This is, to our knowledge, the first time the evolution of sequences and regulation have been explicitly linked in a simulation, despite there being a multitude of sequence evolution simulators, and a handful of models to simulate Gene Regulatory Network (GRN) evolution. In our test runs, we see a coevolutionary signal among genes that are active in the GRN, and neutral evolution in genes that are not included in the network, showing that selective pressures imposed on the regulatory output of the genes are reflected in their sequences.
Conclusion
We believe that CastNet represents a substantial step for developing new tools to study genome evolution, and more broadly, coevolutionary webs and complex evolving systems. This simulator also provides a new framework to study molecular evolution where sequence coevolution has a leading role."
244,"kboolnet
: a toolkit for the verification, validation, and visualization of reaction-contingency (
rxncon
) models","Background
Computational models of cell signaling networks are extremely useful tools for the exploration of underlying system behavior and prediction of response to various perturbations. By representing signaling cascades as executable Boolean networks, the previously developed 
rxncon
 (“reaction-contingency”) formalism and associated Python package enable accurate and scalable modeling of signal transduction even in large (thousands of components) biological systems. The models are split into reactions, which generate states, and contingencies, that impinge on reactions; this avoids the so-called “combinatorial explosion” of system size. Boolean description of the biological system compensates for the poor availability of kinetic parameters which are necessary for quantitative models. Unfortunately, few tools are available to support 
rxncon
 model development, especially for large, intricate systems.

Results
We present the 
kboolnet
 toolkit (
https://github.com/Kufalab-UCSD/kboolnet
, complete documentation at 
https://github.com/Kufalab-UCSD/kboolnet/wiki
), an R package and a set of scripts that seamlessly integrate with the python-based 
rxncon
 software and collectively provide a complete workflow for the verification, validation, and visualization of 
rxncon
 models. The verification script 
VerifyModel.R
 checks for responsiveness to repeated stimulations as well as consistency of steady state behavior. The validation scripts 
TruthTable.R
, 
SensitivityAnalysis.R
, and 
ScoreNet.R
 provide various readouts for the comparison of model predictions to experimental data. In particular, 
ScoreNet.R
 compares model predictions to a cloud-stored 
MIDAS
-format experimental database to provide a numerical score for tracking model accuracy. Finally, the visualization scripts allow for graphical representations of model topology and behavior. The entire 
kboolnet
 toolkit is cloud-enabled, allowing for easy collaborative development; most scripts also allow for the extraction and analysis of individual user-defined “modules”.
Conclusion
The 
kboolnet
 toolkit provides a modular, cloud-enabled workflow for the development of 
rxncon
 models, as well as their verification, validation, and visualization. This will enable the creation of larger, more comprehensive, and more rigorous models of cell signaling using the 
rxncon
 formalism in the future."
245,Deep learning-based classification model for GPR151 activator activity prediction,"Background
GPR151 is a kind of protein belonging to G protein-coupled receptor family that is closely associated with a variety of physiological and pathological processes.The potential use of GPR151 as a therapeutic target for the management of metabolic disorders has been demonstrated in several studies, highlighting the demand to explore its activators further. Activity prediction serves as a vital preliminary step in drug discovery, which is both costly and time-consuming. Thus, the development of reliable activity classification model has become an essential way in the process of drug discovery, aiming to enhance the efficiency of virtual screening.
Results
We propose a learning-based method based on feature extractor and deep neural network to predict the activity of GPR151 activators. We first introduce a new molecular feature extraction algorithm which utilizes the idea of bag-of-words model in natural language to densify the sparse fingerprint vector. Mol2vec method is also used to extract diverse features. Then, we construct three classical feature selection algorithms and three types of deep learning model to enhance the representational capacity of molecules and predict activity label by five different classifiers. We conduct experiments using our own dataset of GPR151 activators. The results demonstrate high classification accuracy and stability, with the optimal model Mol2vec-CNN significantly improving performance across multiple classifiers. The svm classifier achieves the best accuracy of 0.92 and F1 score of 0.76 which indicates promising applications for our method in the field of activity prediction.
Conclusion
The results suggest that the experimental design of this study is appropriate and well-conceived. The deep learning-based feature extraction algorithm established in this study outperforms traditional feature selection algorithm for activity prediction. The model developed can be effectively utilized in the pre-screening stage of drug virtual screening."
246,3DVizSNP: a tool for rapidly visualizing missense mutations identified in high throughput experiments in iCn3D,"Background
High throughput experiments in cancer and other areas of genomic research identify large numbers of sequence variants that need to be evaluated for phenotypic impact. While many tools exist to score the likely impact of single nucleotide polymorphisms (SNPs) based on sequence alone, the three-dimensional structural environment is essential for understanding the biological impact of a nonsynonymous mutation.
Results
We present a program, 3DVizSNP, that enables the rapid visualization of nonsynonymous missense mutations extracted from a variant caller format file using the web-based iCn3D visualization platform. The program, written in Python, leverages REST APIs and can be run locally without installing any other software or databases, or from a webserver hosted by the National Cancer Institute. It automatically selects the appropriate experimental structure from the Protein Data Bank, if available, or the predicted structure from the AlphaFold database, enabling users to rapidly screen SNPs based on their local structural environment. 3DVizSNP leverages iCn3D annotations and its structural analysis functions to assess changes in structural contacts associated with mutations.
Conclusions
This tool enables researchers to efficiently make use of 3D structural information to prioritize mutations for further computational and experimental impact assessment. The program is available as a webserver at 
https://analysistools.cancer.gov/3dvizsnp
 or as a standalone python program at 
https://github.com/CBIIT-CGBB/3DVizSNP
."
247,"Detecting patterns of accessory genome coevolution in 
Staphylococcus aureus
 using data from thousands of genomes","Bacterial genomes exhibit widespread horizontal gene transfer, resulting in highly variable genome content that complicates the inference of genetic interactions. In this study, we develop a method for detecting coevolving genes from large datasets of bacterial genomes based on pairwise comparisons of closely related individuals, analogous to a pedigree study in eukaryotic populations. We apply our method to pairs of genes from the 
Staphylococcus aureus
 accessory genome of over 75,000 annotated gene families using a database of over 40,000 whole genomes. We find many pairs of genes that appear to be gained or lost in a coordinated manner, as well as pairs where the gain of one gene is associated with the loss of the other. These pairs form networks of rapidly coevolving genes, primarily consisting of genes involved in virulence, mechanisms of horizontal gene transfer, and antibiotic resistance, particularly the SCC
mec
 complex. While we focus on gene gain and loss, our method can also detect genes that tend to acquire substitutions in tandem, or genotype-phenotype or phenotype-phenotype coevolution. Finally, we present the R package 
DeCoTUR
 that allows for the computation of our method."
248,TEMPROT: protein function annotation using transformers embeddings and homology search,"Background
Although the development of sequencing technologies has provided a large number of protein sequences, the analysis of functions that each one plays is still difficult due to the efforts of laboratorial methods, making necessary the usage of computational methods to decrease this gap. As the main source of information available about proteins is their sequences, approaches that can use this information, such as classification based on the patterns of the amino acids and the inference based on sequence similarity using alignment tools, are able to predict a large collection of proteins. The methods available in the literature that use this type of feature can achieve good results, however, they present restrictions of protein length as input to their models. In this work, we present a new method, called TEMPROT, based on the fine-tuning and extraction of embeddings from an available architecture pre-trained on protein sequences. We also describe TEMPROT+, an ensemble between TEMPROT and BLASTp, a local alignment tool that analyzes sequence similarity, which improves the results of our former approach.
Results
The evaluation of our proposed classifiers with the literature approaches has been conducted on our dataset, which was derived from CAFA3 challenge database. Both TEMPROT and TEMPROT+ achieved competitive results on 
\(F_{\max }\)
, 
\(S_{\min }\)
, AuPRC and IAuPRC metrics on Biological Process (BP), Cellular Component (CC) and Molecular Function (MF) ontologies compared to state-of-the-art models, with the main results equal to 0.581, 0.692 and 0.662 of 
\(F_{\max }\)
 on BP, CC and MF, respectively.
Conclusions
The comparison with the literature showed that our model presented competitive results compared the state-of-the-art approaches considering the amino acid sequence pattern recognition and homology analysis. Our model also presented improvements related to the input size that the model can use to train compared to the literature methods."
249,Analysis of RNA-Seq data using self-supervised learning for vital status prediction of colorectal cancer patients,"Background
RNA sequencing (RNA-Seq) is a technique that utilises the capabilities of next-generation sequencing to study a cellular transcriptome i.e., to determine the amount of RNA at a given time for a given biological sample. The advancement of RNA-Seq technology has resulted in a large volume of gene expression data for analysis.
Results
Our computational model (built on top of TabNet) is first pretrained on an unlabelled dataset of multiple types of adenomas and adenocarcinomas and later fine-tuned on the labelled dataset, showing promising results in the context of the estimation of the vital status of colorectal cancer patients. We achieve a final cross-validated (ROC-AUC) Score of 0.88 by using multiple modalities of data.
Conclusion
The results of this study demonstrate that self-supervised learning methods pretrained on a vast corpus of unlabelled data outperform traditional supervised learning methods such as XGBoost, Neural Networks, and Decision Trees that have been prevalent in the tabular domain. The results of this study are further boosted by the inclusion of multiple modalities of data pertaining to the patients in question. We find that genes such as RBM3, GSPT1, MAD2L1, and others important to the computation model’s prediction task obtained through model interpretability corroborate with pathological evidence in current literature."
250,Rescuing biologically relevant consensus regions across replicated samples,"Background
Protein-DNA binding sites of ChIP-seq experiments are identified where the binding affinity is significant based on a given threshold. The choice of the threshold is a trade-off between conservative region identification and discarding weak, but true binding sites.
Results
We rescue weak binding sites using MSPC, which efficiently exploits replicates to lower the threshold required to identify a site while keeping a low false-positive rate, and we compare it to IDR, a widely used post-processing method for identifying highly reproducible peaks across replicates. We observe several master transcription regulators (e.g., SP1 and GATA3) and HDAC2-GATA1 regulatory networks on rescued regions in K562 cell line.
Conclusions
We argue the biological relevance of weak binding sites and the information they add when 
rescued
 by MSPC. An implementation of the proposed extended MSPC methodology and the scripts to reproduce the performed analysis are freely available at 
https://genometric.github.io/MSPC/
; MSPC is distributed as a command-line application and an R package available from Bioconductor (
https://doi.org/doi:10.18129/B9.bioc.rmspc
)."
251,Tidyproteomics: an open-source R package and data object for quantitative proteomics post analysis and visualization,"Background
The analysis of mass spectrometry-based quantitative proteomics data can be challenging given the variety of established analysis platforms, the differences in reporting formats, and a general lack of approachable standardized post-processing analyses such as sample group statistics, quantitative variation and even data filtering. We developed 
tidyproteomics
 to facilitate basic analysis, improve data interoperability and potentially ease the integration of new processing algorithms, mainly through the use of a simplified data-object.

Results
The R package 
tidyproteomics
 was developed as both a framework for standardizing quantitative proteomics data and a platform for analysis workflows, containing discrete functions that can be connected end-to-end, thus making it easier to define complex analyses by breaking them into small stepwise units. Additionally, as with any analysis workflow, choices made during analysis can have large impacts on the results and as such, 
tidyproteomics
 allows researchers to string each function together in any order, select from a variety of options and in some cases develop and incorporate custom algorithms.
Conclusions
Tidyproteomics
 aims to simplify data exploration from multiple platforms, provide control over individual functions and analysis order, and serve as a tool to assemble complex repeatable processing workflows in a logical flow. Datasets in 
tidyproteomics
 are easy to work with, have a structure that allows for biological annotations to be added, and come with a framework for developing additional analysis tools. The consistent data structure and accessible analysis and plotting tools also offers a way for researchers to save time on mundane data manipulation tasks."
252,"Comprehensive analyses of a CD8
+
 T cell infiltration related gene signature with regard to the prediction of prognosis and immunotherapy response in lung squamous cell carcinoma","Lung squamous cell carcinoma (LUSC) is associated with a worse prognosis than other histological subtypes of non-small cell lung cancer. Due to the vital role of CD8
+
 T cells in anti-tumor immunity, the characterization of CD8
+
 T cell infiltration-related (CTLIR) gene signature in LUSC is worthy of in-depth exploration. In our study, tumor tissues of LUSC patients from Renmin Hospital of Wuhan University were stained by multiplex immunohistochemistry to evaluate the density of infiltrated CD8
+
 T cells and explore the correlation with immunotherapy response. We found that the proportion of LUSC patients who responded to immunotherapy was higher in the high density of CD8
+
 T cell infiltration group than in the low density of CD8
+
 T cell infiltration group. Subsequently, we collected bulk RNA-sequencing data from The Cancer Genome Atlas (TCGA) database. The abundance of infiltrating immune cells in LUSC patients was analyzed by using CIBERSORT algorithm, and weighted correlation network analysis was performed to identify the co-expressed gene modules related to CD8
+
 T cells. We then developed a prognostic gene signature based on CD8
+
 T cell co-expressed genes and calculated the CTLIR risk score, which stratified LUSC patients into high-risk and low-risk groups. With univariate and multivariate analyses, the gene signature was identified as an independent prognostic factor in LUSC patients. The overall survival of LUSC patients in the high-risk group was significantly shorter than that of the low-risk group in the TCGA cohort, which was validated in Gene Expression Omnibus datasets. We analyzed immune cell infiltration in the tumor microenviroment and found fewer CD8
+
 T cells and more regulatory T cell infiltration in the high-risk group, which is characterized as an immunosuppressive phenotype. Furthermore, the LUSC patients in the high-risk group were predicted to have a better response to immunotherapy than those in the low-risk group when treated with PD-1 and CTLA4 inhibitors. In conclusion, we performed a comprehensive molecular analysis of the CTLIR gene signature in LUSC and constructed a risk model for LUSC patients to predict prognosis and immunotherapy response."
253,RegiSTORM: channel registration for multi-color stochastic optical reconstruction microscopy,"Background
Stochastic optical reconstruction microscopy (STORM), a super-resolution microscopy technique based on single-molecule localizations, has become popular to characterize sub-diffraction limit targets. However, due to lengthy image acquisition, STORM recordings are prone to sample drift. Existing cross-correlation or fiducial marker-based algorithms allow correcting the drift within each channel, but misalignment between channels remains due to interchannel drift accumulating during sequential channel acquisition. This is a major drawback in multi-color STORM, a technique of utmost importance for the characterization of various biological interactions.
Results
We developed RegiSTORM, a software for reducing channel misalignment by accurately registering STORM channels utilizing fiducial markers in the sample. RegiSTORM identifies fiducials from the STORM localization data based on their non-blinking nature and uses them as landmarks for channel registration. We first demonstrated accurate registration on recordings of fiducials only, as evidenced by significantly reduced target registration error with all the tested channel combinations. Next, we validated the performance in a more practically relevant setup on cells multi-stained for tubulin. Finally, we showed that RegiSTORM successfully registers two-color STORM recordings of cargo-loaded lipid nanoparticles without fiducials, demonstrating the broader applicability of this software.
Conclusions
The developed RegiSTORM software was demonstrated to be able to accurately register multiple STORM channels and is freely available as open-source (MIT license) at 
https://github.com/oystein676/RegiSTORM.git
 and 
https://doi.org/10.5281/zenodo.5509861
 (archived), and runs as a standalone executable (Windows) or via Python (Mac OS, Linux)."
254,Biotite: new tools for a versatile Python bioinformatics library,"Background
Biotite is a program library for sequence and structural bioinformatics written for the Python programming language. It implements widely used computational methods into a consistent and accessible package. This allows for easy combination of various data analysis, modeling and simulation methods.
Results
This article presents major functionalities introduced into Biotite since its original publication. The fields of application are shown using concrete examples. We show that the computational performance of Biotite for bioinformatics tasks is comparable to individual, special purpose software systems specifically developed for the respective single task.
Conclusions
The results show that Biotite can be used as program library to either answer specific bioinformatics questions and simultaneously allow the user to write entire, self-contained software applications with sufficient performance for general application."
255,Detecting gene breakpoints in noisy genome sequences using position-annotated colored de-Bruijn graphs,"Background
Identifying the locations of gene breakpoints between species of different taxonomic groups can provide useful insights into the underlying evolutionary processes. Given the exact locations of their genes, the breakpoints can be computed without much effort. However, often, existing gene annotations are erroneous, or only nucleotide sequences are available. Especially in mitochondrial genomes, high variations in gene orders are usually accompanied by a high degree of sequence inconsistencies. This makes accurately locating breakpoints in mitogenomic nucleotide sequences a challenging task.
Results
This contribution presents a novel method for detecting gene breakpoints in the nucleotide sequences of complete mitochondrial genomes, taking into account possible high substitution rates. The method is implemented in the software package 
DeBBI
. 
DeBBI
 allows to analyze transposition- and inversion-based breakpoints independently and uses a parallel program design, allowing to make use of modern multi-processor systems. Extensive tests on synthetic data sets, covering a broad range of sequence dissimilarities and different numbers of introduced breakpoints, demonstrate 
DeBBI
 ’s ability to produce accurate results. Case studies using species of various taxonomic groups further show 
DeBBI
 ’s applicability to real-life data. While (some) multiple sequence alignment tools can also be used for the task at hand, we demonstrate that especially gene breaks between short, poorly conserved tRNA genes can be detected more frequently with the proposed approach.
Conclusion
The proposed method constructs a position-annotated de-Bruijn graph of the input sequences. Using a heuristic algorithm, this graph is searched for particular structures, called bulges, which may be associated with the breakpoint locations. Despite the large size of these structures, the algorithm only requires a small number of graph traversal steps."
256,Association filtering and generative adversarial networks for predicting lncRNA-associated disease,"Background
Long non-coding RNA (lncRNA) closely associates with numerous biological processes, and with many diseases. Therefore, lncRNA-disease association prediction helps obtain relevant biological information and understand pathogenesis, and thus better diagnose preventable diseases.
Results
Herein, we offer the LDAF_GAN method for predicting lncRNA-associated disease based on association filtering and generative adversarial networks. Experimentation used two types of data: lncRNA-disease associated data without lncRNA sequence features, and fused lncRNA sequence features. LDAF_GAN uses a generator and discriminator, and differs from the original GAN by the addition of a filtering operation and negative sampling. Filtering allows the generator output to filter out unassociated diseases before being fed into the discriminator. Thus, the results generated by the model focuses only on lncRNAs associated with disease. Negative sampling takes a portion of disease terms with 0 from the association matrix as negative samples, which are assumed to be unassociated with lncRNA. A regular term is added to the loss function to avoid producing a vector with all values of 1, which can fool the discriminator. Thus, the model requires that generated positive samples are close to 1, and negative samples are close to 0. The model achieved a superior fitting effect; LDAF_GAN had superior performance in predicting fivefold cross-validations on the two datasets with AUC values of 0.9265 and 0.9278, respectively. In the case study, LDAF_GAN predicted disease association for six lncRNAs-H19, MALAT1, XIST, ZFAS1, UCA1, and ZEB1-AS1-and with the top ten predictions of 100%, 80%, 90%, 90%, 100%, and 90%, respectively, which were reported by previous studies.
Conclusion
LDAF_GAN efficiently predicts the potential association of existing lncRNAs and the potential association of new lncRNAs with diseases. The results of fivefold cross-validation, tenfold cross-validation, and case studies suggest that the model has great predictive potential for lncRNA-disease association prediction."
257,End-to-end protein–ligand complex structure generation with diffusion-based generative models,"Background
Three-dimensional structures of protein–ligand complexes provide valuable insights into their interactions and are crucial for molecular biological studies and drug design. However, their high-dimensional and multimodal nature hinders end-to-end modeling, and earlier approaches depend inherently on existing protein structures. To overcome these limitations and expand the range of complexes that can be accurately modeled, it is necessary to develop efficient end-to-end methods.
Results
We introduce an equivariant diffusion-based generative model that learns the joint distribution of ligand and protein conformations conditioned on the molecular graph of a ligand and the sequence representation of a protein extracted from a pre-trained protein language model. Benchmark results show that this protein structure-free model is capable of generating diverse structures of protein–ligand complexes, including those with correct binding poses. Further analyses indicate that the proposed end-to-end approach is particularly effective when the ligand-bound protein structure is not available.
Conclusion
The present results demonstrate the effectiveness and generative capability of our end-to-end complex structure modeling framework with diffusion-based generative models. We suppose that this framework will lead to better modeling of protein–ligand complexes, and we expect further improvements and wide applications."
258,LAPIS is a fast web API for massive open virus sequencing data,"Background
Recent epidemic outbreaks such as the SARS-CoV-2 pandemic and the mpox outbreak in 2022 have demonstrated the value of genomic sequencing data for tracking the origin and spread of pathogens. Laboratories around the globe generated new sequences at unprecedented speed and volume and bioinformaticians developed new tools and dashboards to analyze this wealth of data. However, a major challenge that remains is the lack of simple and efficient approaches for accessing and processing sequencing data.
Results
The Lightweight API for Sequences (LAPIS) facilitates rapid retrieval and analysis of genomic sequencing data through a REST API. It supports complex mutation- and metadata-based queries and can perform aggregation operations on massive datasets. LAPIS is optimized for typical questions relevant to genomic epidemiology. Using a newly-developed in-memory database engine, it has a high speed and throughput: between 25 January and 4 February 2023, the SARS-CoV-2 instance of LAPIS, which contains 14.5 million sequences, processed over 20 million requests with a mean response time of 411 ms and a median response time of 1 ms. LAPIS is the core engine behind our dashboards on genspectrum.org and we currently maintain public LAPIS instances for SARS-CoV-2 and mpox.
Conclusions
Powered by an optimized database engine and available through a web API, LAPIS enhances the accessibility of genomic sequencing data. It is designed to serve as a common backend for dashboards and analyses with the potential to be integrated into common database platforms such as GenBank."
259,Beyond the state of the art of reverse vaccinology: predicting vaccine efficacy with the universal immune system simulator for influenza,"When it was first introduced in 2000, reverse vaccinology was defined as an in silico approach that begins with the pathogen's genomic sequence. It concludes with a list of potential proteins with a possible, but not necessarily, list of peptide candidates that need to be experimentally confirmed for vaccine production. During the subsequent years, reverse vaccinology has dramatically changed: now it consists of a large number of bioinformatics tools and processes, namely subtractive proteomics, computational vaccinology, immunoinformatics, and in silico related procedures. However, the state of the art of reverse vaccinology still misses the ability to predict the efficacy of the proposed vaccine formulation. Here, we describe how to fill the gap by introducing an advanced immune system simulator that tests the efficacy of a vaccine formulation against the disease for which it has been designed. As a working example, we entirely apply this advanced reverse vaccinology approach to design and predict the efficacy of a potential vaccine formulation against influenza H5N1. Climate change and melting glaciers are critical due to reactivating frozen viruses and emerging new pandemics. H5N1 is one of the potential strains present in icy lakes that can raise a pandemic. Investigating structural antigen protein is the most profitable therapeutic pipeline to generate an effective vaccine against H5N1. In particular, we designed a multi-epitope vaccine based on predicted epitopes of hemagglutinin and neuraminidase proteins that potentially trigger B-cells, CD4, and CD8 T-cell immune responses. Antigenicity and toxicity of all predicted CTL, Helper T-lymphocytes, and B-cells epitopes were evaluated, and both antigenic and non-allergenic epitopes were selected. From the perspective of advanced reverse vaccinology, the Universal Immune System Simulator, an in silico trial computational framework, was applied to estimate vaccine efficacy using a cohort of 100 digital patients."
260,Nfinder: automatic inference of cell neighborhood in 2D and 3D using nuclear markers,"Background
In tissues and organisms, the coordination of neighboring cells is essential to maintain their properties and functions. Therefore, knowing which cells are adjacent is crucial to understand biological processes that involve physical interactions among them, e.g. cell migration and proliferation. In addition, some signaling pathways, such as Notch or extrinsic apoptosis, are highly dependent on cell–cell communication. While this is straightforward to obtain from membrane images, nuclei labelling is much more ubiquitous for technical reasons. However, there are no automatic and robust methods to find neighboring cells based only on nuclear markers.
Results
In this work, we describe Nfinder, a method to assess the cell’s local neighborhood from images with nuclei labeling. To achieve this goal, we approximate the cell–cell interaction graph by the Delaunay triangulation of nuclei centroids. Then, links are filtered by automatic thresholding in cell–cell distance (pairwise interaction) and the maximum angle that a pair of cells subtends with shared neighbors (non-pairwise interaction). We systematically characterized the detection performance by applying Nfinder to publicly available datasets from 
Drosophila melanogaster
, 
Tribolium castaneum
, 
Arabidopsis thaliana
 and 
C. elegans
. In each case, the result of the algorithm was compared to a cell neighbor graph generated by manually annotating the original dataset. On average, our method detected 95% of true neighbors, with only 6% of false discoveries. Remarkably, our findings indicate that taking into account non-pairwise interactions might increase the Positive Predictive Value up to + 11.5%.
Conclusion
Nfinder is the first robust and automatic method for estimating neighboring cells in 2D and 3D based only on nuclear markers and without any free parameters. Using this tool, we found that taking non-pairwise interactions into account improves the detection performance significantly. We believe that using our method might improve the effectiveness of other workflows to study cell–cell interactions from microscopy images. Finally, we also provide a reference implementation in Python and an easy-to-use napari plugin."
261,KATZNCP: a miRNA–disease association prediction model integrating KATZ algorithm and network consistency projection,"Background
Clinical studies have shown that miRNAs are closely related to human health. The study of potential associations between miRNAs and diseases will contribute to a profound understanding of the mechanism of disease development, as well as human disease prevention and treatment. MiRNA–disease associations predicted by computational methods are the best complement to biological experiments.
Results
In this research, a federated computational model KATZNCP was proposed on the basis of the KATZ algorithm and network consistency projection to infer the potential miRNA–disease associations. In KATZNCP, a heterogeneous network was initially constructed by integrating the known miRNA–disease association, integrated miRNA similarities, and integrated disease similarities; then, the KATZ algorithm was implemented in the heterogeneous network to obtain the estimated miRNA–disease prediction scores. Finally, the precise scores were obtained by the network consistency projection method as the final prediction results. KATZNCP achieved the reliable predictive performance in leave-one-out cross-validation (LOOCV) with an AUC value of 0.9325, which was better than the state-of-the-art comparable algorithms. Furthermore, case studies of lung neoplasms and esophageal neoplasms demonstrated the excellent predictive performance of KATZNCP.
Conclusion
A new computational model KATZNCP was proposed for predicting potential miRNA–drug associations based on KATZ and network consistency projections, which can effectively predict the potential miRNA–disease interactions. Therefore, KATZNCP can be used to provide guidance for future experiments."
262,A mixed-effects stochastic model reveals clonal dominance in gene therapy safety studies,"Background
Mathematical models of haematopoiesis can provide insights on abnormal cell expansions (clonal dominance), and in turn can guide safety monitoring in gene therapy clinical applications. Clonal tracking is a recent high-throughput technology that can be used to quantify cells arising from a single haematopoietic stem cell ancestor after a gene therapy treatment. Thus, clonal tracking data can be used to calibrate the stochastic differential equations describing clonal population dynamics and hierarchical relationships in vivo.
Results
In this work we propose a random-effects stochastic framework that allows to investigate the presence of events of clonal dominance from high-dimensional clonal tracking data. Our framework is based on the combination between stochastic reaction networks and mixed-effects generalized linear models. Starting from the Kramers–Moyal approximated Master equation, the dynamics of cells duplication, death and differentiation at clonal level, can be described by a local linear approximation. The parameters of this formulation, which are inferred using a maximum likelihood approach, are assumed to be shared across the clones and are not sufficient to describe situation in which clones exhibit heterogeneity in their fitness that can lead to clonal dominance. In order to overcome this limitation, we extend the base model by introducing random-effects for the clonal parameters. This extended formulation is calibrated to the clonal data using a tailor-made expectation-maximization algorithm. We also provide the companion 
 package 
RestoreNet
, publicly available for download at 
https://cran.r-project.org/package=RestoreNet
.
Conclusions
Simulation studies show that our proposed method outperforms the state-of-the-art. The application of our method in two in-vivo studies unveils the dynamics of clonal dominance. Our tool can provide statistical support to biologists in gene therapy safety analyses."
263,An analysis of entity normalization evaluation biases in specialized domains,"Background
Entity normalization is an important information extraction task which has recently gained attention, particularly in the clinical/biomedical and life science domains. On several datasets, state-of-the-art methods perform rather well on popular benchmarks. Yet, we argue that the task is far from resolved.
Results
We have selected two gold standard corpora and two state-of-the-art methods to highlight some evaluation biases. We present non-exhaustive initial findings on the existence of evaluation problems of the entity normalization task.

Conclusions
Our analysis suggests better evaluation practices to support the methodological research in this field."
264,Comut-viz: efficiently creating and browsing comutation plots online,"Background
Comutation plot is a widely used visualization method to deliver a global view of the mutation landscape of large-scale genomic studies. Current tools for creating comutation plot are either offline packages that require coding or online web servers with varied features. When a package is used, it often requires repetitive runs of code to adjust a single feature that might only be a few clicks in a web app. But web apps mostly have limited capacity for customization and cannot handle very large genomic files.
Results
To improve on existing tools, we identified features that are most frequently adjusted in creating a plot and incorporate them in Comut-viz that interactively filters and visualizes mutation data as downloadable plots. It includes colored labels for numeric metadata, a preloaded palette for changing colors and two input boxes for adjusting width and height. It accepts standard mutation annotation format (MAF) files as input and can handle large MAF files with more than 200 k rows. As a front-end only app, Comut-viz guarantees privacy of user data and no latency in the analysis.
Conclusions
Comut-viz is a highly responsive and extensible web app to make comutation plots. It provides customization for frequently adjusted features and accepts large genomic files as input. It is suitable for genomic studies with more than a thousand samples."
265,An N6-methyladenosine regulation- and mRNAsi-related prognostic index reveals the distinct immune microenvironment and immunotherapy responses in lower-grade glioma,"Background
N6-methyladenosine (m6A) modification is involved in tumorigenesis and progression as well as closely correlated with stem cell differentiation and pluripotency. Moreover, tumor progression includes the acquisition of stemness characteristics and accumulating loss of differentiation phenotype. Therefore, we integrated m6A modification and stemness indicator mRNAsi to classify patients and predict prognosis for LGG.
Methods
We performed consensus clustering, weighted gene co-expression network analysis, and least absolute shrinkage and selection operator Cox regression analysis to identify an m6A regulation- and mRNAsi-related prognostic index (MRMRPI). Based on this prognostic index, we also explored the differences in immune microenvironments between high- and low-risk populations. Next, immunotherapy responses were also predicted. Moreover, single-cell RNA sequencing data was further used to verify the expression of these genes in MRMRPI. At last, the tumor-promoting and tumor-associated macrophage polarization roles of TIMP1 in LGG were validated by in vitro experiments.
Results
Ten genes (DGCR10, CYP2E1, CSMD3, HOXB3, CABP4, AVIL, PTCRA, TIMP1, CLEC18A, and SAMD9) were identified to construct the MRMRPI, which was able to successfully classify patients into high- and low-risk group. Significant differences in prognosis, immune microenvironment, and immunotherapy responses were found between distinct groups. A nomogram integrating the MRMRPI and other prognostic factors were also developed to accurately predict prognosis. Moreover, in vitro experiments illustrated that inhibition of TIMP1 could inhibit the proliferation, migration, and invasion of LGG cells and also inhibit the polarization of tumor-associated macrophages.
Conclusion
These findings provide novel insights into understanding the interactions of m6A methylation regulation and tumor stemness on LGG development and contribute to guiding more precise immunotherapy strategies."
266,A diabetes prediction model based on Boruta feature selection and ensemble learning,"Background and objective
As a common chronic disease, diabetes is called the “second killer” among modern diseases. Currently, there is no medical cure for diabetes. We can only rely on medication for auxiliary treatment. However, many diabetic patients still die each year. In addition, a considerable number of people do not pay attention to their physical health or opt out of treatment due to lack of money, which eventually leads to various complications. Therefore, diagnosing diabetes at an early stage and intervening early is necessary; thus, developing an early detection method for diabetes is essential.
Methods
In this study, a diabetes prediction model based on Boruta feature selection and ensemble learning is proposed. The model contains the use of Boruta feature selection, the extraction of salient features from datasets, the use of the K-Means++ algorithm for unsupervised clustering of data and stacking of an ensemble learning method for classification. It has been validated on a diabetes dataset.
Results
The experiments were performed on the PIMA Indian diabetes dataset. The model was evaluated by accuracy, precision and F1 index. The obtained results show that the accuracy rate of the model reaches 98% and achieves good results.
Conclusion
Compared with other diabetes prediction models, this model achieved better results, and the obtained results indicate that this model is superior to other models in diabetes prediction and has better performance."
267,The therapeutic and prognostic role of cuproptosis-related genes in triple negative breast cancer,"Background
This study aimed to observe the potential impact of known cuproptosis-related genes (CRGs) on triple negative breast cancer (TNBC) development, as well as their associated molecular mechanisms, immune infiltration mechanisms and potential therapeutic agents.
Results
Based on the Cox Proportional Hazard Model, 11 CRGs may be especially important in TNBC development and progression (considered as the Key-TNBC-CRGs). The expression of several Key-TNBC-CRGs (e.g., 
ATP7A, PIK3CA, LIAS,
 and 
LIPT
) are associated with common mutations. The SCNA variation of 11 Key-TNBC-CRGs are related to differences immune infiltration profiles. In particular, depletion of 
ATP7A
, 
ATP7B
, 
CLS
, 
LIAS
, and 
SCL31A1
 and while high amplification of 
NLRP3
 and 
LIPT2
 are correlated with decreased immune infiltration. In our Cox proportional hazards regression model, there is a significant difference in the overall survival between high-risk and low-risk groups. The HR in the high-risk group is 3.891 versus the low-risk group. And this model has a satisfactory performance in Prediction of 5–15-year survival, in particular in the 10-year survival (AUC = 0.836). Finally, we discovered some potential drugs for TNBC treatment based on the strategy of targeting 11 Key-TNBC-CRGs, such as Dasatinib combined with ABT-737, Erastin or Methotrexate, and Docetaxel/Ispinesib combination.
Conclusion
In conclusion, CRGs may play important roles in TNBC development, and they can impact tumor immune microenvironment and patient survival. The Key-TNBC-CRGs interact mutually and can be influenced by common BC-related mutations. Additionally, we established a 11-gene risk model with a robust performance in prediction of 5–15-year survival. As well, some new drugs are proposed potentially effective in TNBC based on the CRG strategy."
268,Risk prediction for dermatomyositis-associated hepatocellular carcinoma,"Objective
To explore dermatomyositis signature genes as potential biomarkers of hepatocellular carcinoma and their associated molecular regulatory mechanisms.
Methods
Based on the mRNA-Seq data of dermatomyositis and hepatocellular carcinoma in public databases, five dermatomyositis signature genes were screened by LASSO regression analysis and support vector machine (SVM) algorithm, and their biological functions in dermatomyositis with hepatocellular carcinoma were investigated, and a nomogram risk prediction model for hepatocellular carcinoma was constructed and its predictive efficiency was initially evaluated. The immune profile in hepatocellular carcinoma was examined based on the CIBERSORT and ssGSEA algorithms, and the correlation between five dermatomyositis signature genes and tumor immune cell infiltration and immune checkpoints in hepatocellular carcinoma was investigated.
Results
The expression levels of five dermatomyositis signature genes were significantly altered in hepatocellular carcinoma and showed good diagnostic efficacy for hepatocellular carcinoma, suggesting that they may be potential predictive targets for hepatocellular carcinoma, and the risk prediction model based on five dermatomyositis signature genes showed good risk prediction efficacy for hepatocellular carcinoma and has good potential for clinical application. In addition, we also found that the upregulation of SPP1 expression may activate the PI3K/ART signaling pathway through integrin-mediated activation, which in turn regulates the development and progression of hepatocellular carcinoma.
Conclusion
LY6E, IFITM1, GADD45A, MT1M, and SPP1 are potential predictive targets for new-onset hepatocellular carcinoma in patients with dermatomyositis, and the upregulation of SPP1 expression may activate the PI3K/ART signaling pathway through the mediation of integrins to promote the development and progression of hepatocellular carcinoma."
269,Accelerating genomic workflows using NVIDIA Parabricks,"Background
As genome sequencing becomes better integrated into scientific research, government policy, and personalized medicine, the primary challenge for researchers is shifting from generating raw data to analyzing these vast datasets. Although much work has been done to reduce compute times using various configurations of traditional CPU computing infrastructures, Graphics Processing Units (GPUs) offer opportunities to accelerate genomic workflows by orders of magnitude. Here we benchmark one GPU-accelerated software suite called NVIDIA Parabricks on Amazon Web Services (AWS), Google Cloud Platform (GCP), and an NVIDIA DGX cluster. We benchmarked six variant calling pipelines, including two germline callers (HaplotypeCaller and DeepVariant) and four somatic callers (Mutect2, Muse, LoFreq, SomaticSniper).
Results
We achieved up to 65 × acceleration with germline variant callers, bringing HaplotypeCaller runtimes down from 36 h to 33 min on AWS, 35 min on GCP, and 24 min on the NVIDIA DGX. Somatic callers exhibited more variation between the number of GPUs and computing platforms. On cloud platforms, GPU-accelerated germline callers resulted in cost savings compared with CPU runs, whereas some somatic callers were more expensive than CPU runs because their GPU acceleration was not sufficient to overcome the increased GPU cost.
Conclusions
Germline variant callers scaled well with the number of GPUs across platforms, whereas somatic variant callers exhibited more variation in the number of GPUs with the fastest runtimes, suggesting that, at least with the version of Parabricks used here, these workflows are less GPU optimized and require benchmarking on the platform of choice before being deployed at production scales. Our study demonstrates that GPUs can be used to greatly accelerate genomic workflows, thus bringing closer to grasp urgent societal advances in the areas of biosurveillance and personalized medicine."
270,CircSSNN: circRNA-binding site prediction via sequence self-attention neural networks with pre-normalization,"Background
Circular RNAs (circRNAs) play a significant role in some diseases by acting as transcription templates. Therefore, analyzing the interaction mechanism between circRNA and RNA-binding proteins (RBPs) has far-reaching implications for the prevention and treatment of diseases. Existing models for circRNA-RBP identification usually adopt convolution neural network (CNN), recurrent neural network (RNN), or their variants as feature extractors. Most of them have drawbacks such as poor parallelism, insufficient stability, and inability to capture long-term dependencies.
Methods
In this paper, we propose a new method completely using the self-attention mechanism to capture deep semantic features of RNA sequences. On this basis, we construct a CircSSNN model for the cirRNA-RBP identification. The proposed model constructs a feature scheme by fusing circRNA sequence representations with statistical distributions, static local contexts, and dynamic global contexts. With a stable and efficient network architecture, the distance between any two positions in a sequence is reduced to a constant, so CircSSNN can quickly capture the long-term dependencies and extract the deep semantic features.
Results
Experiments on 37 circRNA datasets show that the proposed model has overall advantages in stability, parallelism, and prediction performance. Keeping the network structure and hyperparameters unchanged, we directly apply the CircSSNN to linRNA datasets. The favorable results show that CircSSNN can be transformed simply and efficiently without task-oriented tuning.
Conclusions
In conclusion, CircSSNN can serve as an appealing circRNA-RBP identification tool with good identification performance, excellent scalability, and wide application scope without the need for task-oriented fine-tuning of parameters, which is expected to reduce the professional threshold required for hyperparameter tuning in bioinformatics analysis."
271,CRISPR-GRANT: a cross-platform graphical analysis tool for high-throughput CRISPR-based genome editing evaluation,"Backgroud
CRISPR/Cas is an efficient genome editing system that has been widely used for functional genetic studies and exhibits high potential in biomedical translational applications. Indel analysis has thus become one of the most common practices in the lab to evaluate DNA editing events generated by CRISPR/Cas. Several indel analysis tools have been reported, however, it is often required that users have certain bioinformatics training and basic command-line processing capability.
Results
Here, we developed CRISPR-GRANT, a stand-alone graphical CRISPR indel analysis tool, which could be easily installed for multi-platforms, including Linux, Windows, and macOS. CRISPR-GRANT offered a straightforward GUI by simple click-and-run for genome editing analysis of single or pooled amplicons and one-step analysis for whole-genome sequencing without the need of data pre-processing, making it ideal for novice lab scientists. Moreover, it also exhibited shorter run-time compared with tools currently available.
Conclusion
Therefore, CRISPR-GRANT is a valuable addition to the current CRISPR toolkits that significantly lower the barrier for wet-lab researchers to conduct indel analysis from large NGS datasets. CRISPR-GRANT binaries are freely available for Linux (above Ubuntu 16.04), macOS (above High Sierra 10.13) and Windows (above Windows 7) at 
https://github.com/fuhuancheng/CRISPR-GRANT
. CRISPR-GRANT source code is licensed under the GPLv3 license and free to download and use."
272,Genofunc: genome annotation and identification of genome features for automated pipelining analysis of virus whole genome sequences,"Background
Viral genomics and epidemiology have been increasingly important tools for analysing the spread of key pathogens affecting daily lives of individuals worldwide. With the rapidly expanding scale of pathogen genome sequencing efforts for epidemics and outbreaks efficient workflows in extracting genomic information are becoming increasingly important for answering key research questions.
Results
Here we present Genofunc, a toolkit offering a range of command line orientated functions for processing of raw virus genome sequences into aligned and annotated data ready for analysis. The tool contains functions such as genome annotation, feature extraction etc. for processing of large genomic datasets both manual or as part of pipeline such as Snakemake or Nextflow ready for down-stream phylogenetic analysis. Originally designed for a large-scale HIV sequencing project, Genofunc has been benchmarked against annotated sequence gene coordinates from the Los Alamos HIV database as validation with downstream phylogenetic analysis result comparable to past literature as case study.
Conclusion
Genofunc is implemented fully in Python and licensed under the MIT license. Source code and documentation is available at: 
https://github.com/xiaoyu518/genofunc
."
273,scSemiAAE: a semi-supervised clustering model for single-cell RNA-seq data,"Background
Single-cell RNA sequencing (scRNA-seq) strives to capture cellular diversity with higher resolution than bulk RNA sequencing. Clustering analysis is critical to transcriptome research as it allows for further identification and discovery of new cell types. Unsupervised clustering cannot integrate prior knowledge where relevant information is widely available. Purely unsupervised clustering algorithms may not yield biologically interpretable clusters when confronted with the high dimensionality of scRNA-seq data and frequent dropout events, which makes identification of cell types more challenging.
Results
We propose scSemiAAE, a semi-supervised clustering model for scRNA sequence analysis using deep generative neural networks. Specifically, scSemiAAE carefully designs a ZINB adversarial autoencoder-based architecture that inherently integrates adversarial training and semi-supervised modules in the latent space. In a series of experiments on scRNA-seq datasets spanning thousands to tens of thousands of cells, scSemiAAE can significantly improve clustering performance compared to dozens of unsupervised and semi-supervised algorithms, promoting clustering and interpretability of downstream analyses.
Conclusion
scSemiAAE is a Python-based algorithm implemented on the VSCode platform that provides efficient visualization, clustering, and cell type assignment for scRNA-seq data. The tool is available from 
https://github.com/WHang98/scSemiAAE
."
274,A T-cell-related signature for prognostic stratification and immunotherapy response in hepatocellular carcinoma based on transcriptomics and single-cell sequencing,"Background
Hepatocellular carcinoma (HCC) is the fifth most frequently diagnosed malignancy and the third leading cause of cancer death globally. T cells are significantly correlated with the progression, therapy and prognosis of cancer. Limited systematic studies regarding the role of T-cell-related markers in HCC have been performed.
Methods
T-cell markers were identified with single-cell RNA sequencing (scRNA-seq) data from the GEO database. A prognostic signature was developed with the LASSO algorithm in the TCGA cohort and verified in the GSE14520 cohort. Another three eligible immunotherapy datasets, GSE91061, PRJEB25780 and IMigor210, were used to verify the role of the risk score in the immunotherapy response.
Results
With 181 T-cell markers identified by scRNA-seq analysis, a 13 T-cell-related gene-based prognostic signature (TRPS) was developed for prognostic prediction, which divided HCC patients into high-risk and low-risk groups according to overall survival, with AUCs of 1 year, 3 years, and 5 years of 0.807, 0.752, and 0.708, respectively. TRPS had the highest C-index compared with the other 10 established prognostic signatures, suggesting a better performance of TRPS in predicting the prognosis of HCC. More importantly, the TRPS risk score was closely correlated with the TIDE score and immunophenoscore. The high-risk score patients had a higher percentage of SD/PD, and CR/PR occurred more frequently in patients with low TRPS-related risk scores in the IMigor210, PRJEB25780 and GSE91061 cohorts. We also constructed a nomogram based on the TRPS, which had high potential for clinical application.
Conclusion
Our study proposed a novel TRPS for HCC patients, and the TRPS could effectively indicate the prognosis of HCC. It also served as a predictor for immunotherapy."
275,Drug mechanism enrichment analysis improves prioritization of therapeutics for repurposing,"Background
There is a pressing need for improved methods to identify effective therapeutics for diseases. Many computational approaches have been developed to repurpose existing drugs to meet this need. However, these tools often output long lists of candidate drugs that are difficult to interpret, and individual drug candidates may suffer from unknown off-target effects. We reasoned that an approach which aggregates information from multiple drugs that share a common mechanism of action (MOA) would increase on-target signal compared to evaluating drugs on an individual basis. In this study, we present drug mechanism enrichment analysis (DMEA), an adaptation of gene set enrichment analysis (GSEA), which groups drugs with shared MOAs to improve the prioritization of drug repurposing candidates.
Results
First, we tested DMEA on simulated data and showed that it can sensitively and robustly identify an enriched drug MOA. Next, we used DMEA on three types of rank-ordered drug lists: (1) perturbagen signatures based on gene expression data, (2) drug sensitivity scores based on high-throughput cancer cell line screening, and (3) molecular classification scores of intrinsic and acquired drug resistance. In each case, DMEA detected the expected MOA as well as other relevant MOAs. Furthermore, the rankings of MOAs generated by DMEA were better than the original single-drug rankings in all tested data sets. Finally, in a drug discovery experiment, we identified potential senescence-inducing and senolytic drug MOAs for primary human mammary epithelial cells and then experimentally validated the senolytic effects of EGFR inhibitors.
Conclusions
DMEA is a versatile bioinformatic tool that can improve the prioritization of candidates for drug repurposing. By grouping drugs with a shared MOA, DMEA increases on-target signal and reduces off-target effects compared to analysis of individual drugs. DMEA is publicly available as both a web application and an R package at 
https://belindabgarana.github.io/DMEA
."
276,Empowering biologists to decode omics data: the Genekitr R package and web server,"Background
A variety of high-throughput analyses, such as transcriptome, proteome, and metabolome analysis, have been developed, producing unprecedented amounts of omics data. These studies generate large gene lists, of which the biological significance shall be deeply understood. However, manually interpreting these lists is difficult, especially for non-bioinformatics-savvy scientists.
Results
We developed an R package and a corresponding web server—Genekitr, to assist biologists in exploring large gene sets. Genekitr comprises four modules: gene information retrieval, ID (identifier) conversion, enrichment analysis and publication-ready plotting. Currently, the information retrieval module can retrieve information on up to 23 attributes for genes of 317 organisms. The ID conversion module assists in ID-mapping of genes, probes, proteins, and aliases. The enrichment analysis module organizes 315 gene set libraries in different biological contexts by over-representation analysis and gene set enrichment analysis. The plotting module performs customizable and high-quality illustrations that can be used directly in presentations or publications.
Conclusions
This web server tool will make bioinformatics more accessible to scientists who might not have programming expertise, allowing them to perform bioinformatics tasks without coding."
277,SVcnn: an accurate deep learning-based method for detecting structural variation based on long-read data,"Background
Structural variations (SVs) refer to variations in an organism’s chromosome structure that exceed a length of 50 base pairs. They play a significant role in genetic diseases and evolutionary mechanisms. While long-read sequencing technology has led to the development of numerous SV caller methods, their performance results have been suboptimal. Researchers have observed that current SV callers often miss true SVs and generate many false SVs, especially in repetitive regions and areas with multi-allelic SVs. These errors are due to the messy alignments of long-read data, which are affected by their high error rate. Therefore, there is a need for a more accurate SV caller method.
Result
We propose a new method-SVcnn, a more accurate deep learning-based method for detecting SVs by using long-read sequencing data. We run SVcnn and other SV callers in three real datasets and find that SVcnn improves the F1-score by 2–8% compared with the second-best method when the read depth is greater than 5×. More importantly, SVcnn has better performance for detecting multi-allelic SVs.
Conclusions
SVcnn is an accurate deep learning-based method to detect SVs. The program is available at 
https://github.com/nwpuzhengyan/SVcnn
."
278,Reducing Boolean networks with backward equivalence,"Background
Boolean Networks (BNs) are a popular dynamical model in biology where the state of each component is represented by a variable taking binary values that express, for instance, activation/deactivation or high/low concentrations. Unfortunately, these models suffer from the state space explosion, i.e., there are exponentially many states in the number of BN variables, which hampers their analysis.
Results
We present Boolean Backward Equivalence (BBE), a novel reduction technique for BNs which collapses system variables that, if initialized with same value, maintain matching values in all states. A large-scale validation on 86 models from two online model repositories reveals that BBE is effective, since it is able to reduce more than 90% of the models. Furthermore, on such models we also show that BBE brings notable analysis speed-ups, both in terms of state space generation and steady-state analysis. In several cases, BBE allowed the analysis of models that were originally intractable due to the complexity. On two selected case studies, we show how one can tune the reduction power of BBE using model-specific information to preserve all dynamics of interest, and selectively exclude behavior that does not have biological relevance.
Conclusions
BBE complements existing reduction methods, preserving properties that other reduction methods fail to reproduce, and vice versa. BBE drops all and only the dynamics, including attractors, originating from states where BBE-equivalent variables have been initialized with different activation values The remaining part of the dynamics is preserved exactly, including the length of the preserved attractors, and their reachability from given initial conditions, without adding any spurious behaviours. Given that BBE is a model-to-model reduction technique, it can be combined with further reduction methods for BNs."
279,Identifying driver pathways based on a parameter-free model and a partheno-genetic algorithm,"Background
Tremendous amounts of omics data accumulated have made it possible to identify cancer driver pathways through computational methods, which is believed to be able to offer critical information in such downstream research as ascertaining cancer pathogenesis, developing anti-cancer drugs, and so on. It is a challenging problem to identify cancer driver pathways by integrating multiple omics data.
Results
In this study, a parameter-free identification model SMCMN, incorporating both pathway features and gene associations in Protein–Protein Interaction (PPI) network, is proposed. A novel measurement of mutual exclusivity is devised to exclude some gene sets with “inclusion” relationship. By introducing gene clustering based operators, a partheno-genetic algorithm CPGA is put forward for solving the SMCMN model. Experiments were implemented on three real cancer datasets to compare the identification performance of models and methods. The comparisons of models demonstrate that the SMCMN model does eliminate the “inclusion” relationship, and produces gene sets with better enrichment performance compared with the classical model MWSM in most cases.
Conclusions
The gene sets recognized by the proposed CPGA-SMCMN method possess more genes engaging in known cancer related pathways, as well as stronger connectivity in PPI network. All of which have been demonstrated through extensive contrast experiments among the CPGA-SMCMN method and six state-of-the-art ones."
280,Bayesian compositional regression with microbiome features via variational inference,"The microbiome plays a key role in the health of the human body. Interest often lies in finding features of the microbiome, alongside other covariates, which are associated with a phenotype of interest. One important property of microbiome data, which is often overlooked, is its compositionality as it can only provide information about the relative abundance of its constituting components. Typically, these proportions vary by several orders of magnitude in datasets of high dimensions. To address these challenges we develop a Bayesian hierarchical linear log-contrast model which is estimated by mean field Monte-Carlo co-ordinate ascent variational inference (CAVI-MC) and easily scales to high dimensional data. We use novel priors which account for the large differences in scale and constrained parameter space associated with the compositional covariates. A reversible jump Monte Carlo Markov chain guided by the data through univariate approximations of the variational posterior probability of inclusion, with proposal parameters informed by approximating variational densities via auxiliary parameters, is used to estimate intractable marginal expectations. We demonstrate that our proposed Bayesian method performs favourably against existing frequentist state of the art compositional data analysis methods. We then apply the CAVI-MC to the analysis of real data exploring the relationship of the gut microbiome to body mass index."
281,GraphSNP: an interactive distance viewer for investigating outbreaks and transmission networks using a graph approach,"Background
Cluster and transmission analysis utilising pairwise SNP distance are increasingly used in genomic epidemiological studies. However, current methods are often challenging to install and use, and lack interactive functionalities for easy data exploration.
Results
GraphSNP is an interactive visualisation tool running in a web browser that allows users to rapidly generate pairwise SNP distance networks, investigate SNP distance distributions, identify clusters of related organisms, and reconstruct transmission routes. The functionality of GraphSNP is demonstrated using examples from recent multi-drug resistant bacterial outbreaks in healthcare settings.
Conclusions
GraphSNP is freely available at 
https://github.com/nalarbp/graphsnp
. An online version of GraphSNP, including demonstration datasets, input templates, and quick start guide is available for use at 
https://graphsnp.fordelab.com
."
282,DePolymerase Predictor (DePP): a machine learning tool for the targeted identification of phage depolymerases,"Biofilm production plays a clinically significant role in the pathogenicity of many bacteria, limiting our ability to apply antimicrobial agents and contributing in particular to the pathogenesis of chronic infections. Bacteriophage depolymerases, leveraged by these viruses to circumvent biofilm mediated resistance, represent a potentially powerful weapon in the fight against antibiotic resistant bacteria. Such enzymes are able to degrade the extracellular matrix that is integral to the formation of all biofilms and as such would allow complementary therapies or disinfection procedures to be successfully applied. In this manuscript, we describe the development and application of a machine learning based approach towards the identification of phage depolymerases. We demonstrate that on the basis of a relatively limited number of experimentally proven enzymes and using an amino acid derived feature vector that the development of a powerful model with an accuracy on the order of 90% is possible, showing the value of such approaches in protein functional annotation and the discovery of novel therapeutic agents."
283,On the correspondence between the transcriptomic response of a compound and its effects on its targets,"Better understanding the transcriptomic response produced by a compound perturbing its targets can shed light on the underlying biological processes regulated by the compound. However, establishing the relationship between the induced transcriptomic response and the target of a compound is non-trivial, partly because targets are rarely differentially expressed. Therefore, connecting both modalities requires orthogonal information (e.g., pathway or functional information). Here, we present a comprehensive study aimed at exploring this relationship by leveraging thousands of transcriptomic experiments and target data for over 2000 compounds. Firstly, we confirm that compound-target information does not correlate as expected with the transcriptomic signatures induced by a compound. However, we reveal how the concordance between both modalities increases by connecting pathway and target information. Additionally, we investigate whether compounds that target the same proteins induce a similar transcriptomic response and conversely, whether compounds with similar transcriptomic responses share the same target proteins. While our findings suggest that this is generally not the case, we did observe that compounds with similar transcriptomic profiles are more likely to share at least one protein target and common therapeutic applications. Finally, we demonstrate how to exploit the relationship between both modalities for mechanism of action deconvolution by presenting a case scenario involving a few compound pairs with high similarity."
284,"Accuracy of a machine learning method based on structural and locational information from AlphaFold2 for predicting the pathogenicity of 
TARDBP
 and 
FUS
 gene variants in ALS","Background
In the sporadic form of amyotrophic lateral sclerosis (ALS), the pathogenicity of rare variants in the causative genes characterizing the familial form remains largely unknown. To predict the pathogenicity of such variants, in silico analysis is commonly used. In some ALS causative genes, the pathogenic variants are concentrated in specific regions, and the resulting alterations in protein structure are thought to significantly affect pathogenicity. However, existing methods have not taken this issue into account. To address this, we have developed a technique termed MOVA (method for evaluating the pathogenicity of missense variants using AlphaFold2), which applies positional information for structural variants predicted by AlphaFold2. Here we examined the utility of MOVA for analysis of several causative genes of ALS.
Methods
We analyzed variants of 12 ALS-related genes (
TARDBP
, 
FUS
, 
SETX
, 
TBK1
, 
OPTN
, 
SOD1
, 
VCP
, 
SQSTM1
, 
ANG
, 
UBQLN2
, 
DCTN1
, and 
CCNF
) and classified them as pathogenic or neutral. For each gene, the features of the variants, consisting of their positions in the 3D structure predicted by AlphaFold2, pLDDT score, and BLOSUM62 were trained into a random forest and evaluated by the stratified fivefold cross validation method. We compared how accurately MOVA predicted mutant pathogenicity with other in silico prediction methods and evaluated the prediction accuracy at 
TARDBP
 and 
FUS
 hotspots. We also examined which of the MOVA features had the greatest impact on pathogenicity discrimination.
Results
MOVA yielded useful results (AUC ≥ 0.70) for 
TARDBP
, 
FUS
, 
SOD1
, 
VCP
, and 
UBQLN2
 of 12 ALS causative genes. In addition, when comparing the prediction accuracy with other in silico prediction methods, MOVA obtained the best results among those compared for 
TARDBP
, 
VCP
, 
UBQLN2
, and 
CCNF
. MOVA demonstrated superior predictive accuracy for the pathogenicity of mutations at hotspots of 
TARDBP
 and 
FUS
. Moreover, higher accuracy was achieved by combining MOVA with REVEL or CADD. Among the features of MOVA, the x, y, and z coordinates performed the best and were highly correlated with MOVA.
Conclusions
MOVA is useful for predicting the virulence of rare variants in which they are concentrated at specific structural sites, and for use in combination with other prediction methods."
285,CircPrime: a web-based platform for design of specific circular RNA primers,"Background
Circular RNAs (circRNAs) are covalently closed-loop RNAs with critical regulatory roles in cells. Tens of thousands of circRNAs have been unveiled due to the recent advances in high throughput RNA sequencing technologies and bioinformatic tools development. At the same time, polymerase chain reaction (PCR) cross-validation for circRNAs predicted by bioinformatic tools remains an essential part of any circRNA study before publication.
Results
Here, we present the CircPrime web-based platform, providing a user-friendly solution for DNA primer design and thermocycling conditions for circRNA identification with routine PCR methods.
Conclusions
User-friendly CircPrime web platform (
http://circprime.elgene.net/
) works with outputs of the most popular bioinformatic predictors of circRNAs to design specific circular RNA primers. CircPrime works with circRNA coordinates and any reference genome from the National Center for Biotechnology Information database)."
286,"HISS: Snakemake-based workflows for performing SMRT-RenSeq assembly, AgRenSeq and dRenSeq for the discovery of novel plant disease resistance genes","Background
In the ten years since the initial publication of the RenSeq protocol, the method has proved to be a powerful tool for studying disease resistance in plants and providing target genes for breeding programmes. Since the initial publication of the methodology, it has continued to be developed as new technologies have become available and the increased availability of computing power has made new bioinformatic approaches possible. Most recently, this has included the development of a 
k
-mer based association genetics approach, the use of PacBio HiFi data, and graphical genotyping with diagnostic RenSeq. However, there is not yet a unified workflow available and researchers must instead configure approaches from various sources themselves. This makes reproducibility and version control a challenge and limits the ability to perform these analyses to those with bioinformatics expertise.

Results
Here we present HISS, consisting of three workflows which take a user from raw RenSeq reads to the identification of candidates for disease resistance genes. These workflows conduct the assembly of enriched HiFi reads from an accession with the resistance phenotype of interest. A panel of accessions both possessing and lacking the resistance are then used in an association genetics approach (AgRenSeq) to identify contigs positively associated with the resistance phenotype. Candidate genes are then identified on these contigs and assessed for their presence or absence in the panel with a graphical genotyping approach that uses dRenSeq. These workflows are implemented via Snakemake, a python-based workflow manager. Software dependencies are either shipped with the release or handled with conda. All code is freely available and is distributed under the GNU GPL-3.0 license.
Conclusions
HISS provides a user-friendly, portable, and easily customised approach for identifying novel disease resistance genes in plants. It is easily installed with all dependencies handled internally or shipped with the release and represents a significant improvement in the ease of use of these bioinformatics analyses."
287,Identification of essential proteins based on edge features and the fusion of multiple-source biological information,"Background
A major current focus in the analysis of protein–protein interaction (PPI) data is how to identify essential proteins. As massive PPI data are available, this warrants the design of efficient computing methods for identifying essential proteins. Previous studies have achieved considerable performance. However, as a consequence of the features of high noise and structural complexity in PPIs, it is still a challenge to further upgrade the performance of the identification methods.
Methods
This paper proposes an identification method, named CTF, which identifies essential proteins based on edge features including 
h
-quasi-cliques and 
uv
-triangle graphs and the fusion of multiple-source information. We first design an edge-weight function, named EWCT, for computing the topological scores of proteins based on quasi-cliques and triangle graphs. Then, we generate an edge-weighted PPI network using EWCT and dynamic PPI data. Finally, we compute the essentiality of proteins by the fusion of topological scores and three scores of biological information.
Results
We evaluated the performance of the CTF method by comparison with 16 other methods, such as MON, PeC, TEGS, and LBCC, the experiment results on three datasets of 
Saccharomyces cerevisiae
 show that CTF outperforms the state-of-the-art methods. Moreover, our method indicates that the fusion of other biological information is beneficial to improve the accuracy of identification."
288,Drug repurposing and prediction of multiple interaction types via graph embedding,"Background
Finding drugs that can interact with a specific target to induce a desired therapeutic outcome is key deliverable in drug discovery for targeted treatment. Therefore, both identifying new drug–target links, as well as delineating the type of drug interaction, are important in drug repurposing studies.

Results
A computational drug repurposing approach was proposed to predict novel drug–target interactions (DTIs), as well as to predict the type of interaction induced. The methodology is based on mining a heterogeneous graph that integrates drug–drug and protein–protein similarity networks, together with verified drug-disease and protein-disease associations. In order to extract appropriate features, the three-layer heterogeneous graph was mapped to low dimensional vectors using node embedding principles. The DTI prediction problem was formulated as a multi-label, multi-class classification task, aiming to determine drug modes of action. DTIs were defined by concatenating pairs of drug and target vectors extracted from graph embedding, which were used as input to classification via gradient boosted trees, where a model is trained to predict the type of interaction. After validating the prediction ability of DT2Vec+, a comprehensive analysis of all unknown DTIs was conducted to predict the degree and type of interaction. Finally, the model was applied to propose potential approved drugs to target cancer-specific biomarkers.
Conclusion
DT2Vec+ showed promising results in predicting type of DTI, which was achieved via integrating and mapping triplet drug–target–disease association graphs into low-dimensional dense vectors. To our knowledge, this is the first approach that addresses prediction between drugs and targets across six interaction types."
289,SeqWiz: a modularized toolkit for next-generation protein sequence database management and analysis,"Background
Current proteomic technologies are fast-evolving to uncover the complex features of sequence processes, variations and modifications. Thus, protein sequence database and the corresponding softwares should also be improved to solve this issue.
Results
We developed a state-of-the-art toolkit (SeqWiz) for constructing next-generation sequence databases and performing proteomic-centric sequence analyses. First, we proposed two derived data formats: SQPD (a well-structured and high-performance local sequence database based on SQLite), and SET (an associated list of selected entries based on JSON). The SQPD format follows the basic standards of the emerging PEFF format, which also aims to facilitate the search of complex proteoform. The SET format is designed for generating subsets with with high-efficiency. These formats are shown to greatly outperform the conventional FASTA or PEFF formats in time and resource consumption. Then, we mainly focused on the UniProt knowledgebase and developed a collection of open-source tools and basic modules for retrieving species-specific databases, formats conversion, sequence generation, sequence filter, and sequence analysis. These tools are implemented by using the Python language and licensed under the GNU General Public Licence V3. The source codes and distributions are freely available at GitHub (
https://github.com/fountao/protwiz/tree/main/seqwiz
).
Conclusions
SeqWiz is designed to be a collection of modularized tools, which is friendly to both end-users for preparing easy-to-use sequence databases as well as bioinformaticians for performing downstream sequence analysis. Besides the novel formats, it also provides compatible functions for handling the traditional text based FASTA or PEFF formats. We believe that SeqWiz will promote the implementing of complementary proteomics for data renewal and proteoform analysis to achieve precision proteomics. Additionally, it can also drive the improvement of proteomic standardization and the development of next-generation proteomic softwares."
290,CysPresso: a classification model utilizing deep learning protein representations to predict recombinant expression of cysteine-dense peptides,"Background
Cysteine-dense peptides (CDPs) are an attractive pharmaceutical scaffold that display extreme biochemical properties, low immunogenicity, and the ability to bind targets with high affinity and selectivity. While many CDPs have potential and confirmed therapeutic uses, synthesis of CDPs is a challenge. Recent advances have made the recombinant expression of CDPs a viable alternative to chemical synthesis. Moreover, identifying CDPs that can be expressed in mammalian cells is crucial in predicting their compatibility with gene therapy and mRNA therapy. Currently, we lack the ability to identify CDPs that will express recombinantly in mammalian cells without labour intensive experimentation. To address this, we developed CysPresso, a novel machine learning model that predicts recombinant expression of CDPs based on primary sequence.
Results
We tested various protein representations generated by deep learning algorithms (SeqVec, proteInfer, AlphaFold2) for their suitability in predicting CDP expression and found that AlphaFold2 representations possessed the best predictive features. We then optimized the model by concatenation of AlphaFold2 representations, time series transformation with random convolutional kernels, and dataset partitioning.
Conclusion
Our novel model, CysPresso, is the first to successfully predict recombinant CDP expression in mammalian cells and is particularly well suited for predicting recombinant expression of knottin peptides. When preprocessing the deep learning protein representation for supervised machine learning, we found that random convolutional kernel transformation preserves more pertinent information relevant for predicting expressibility than embedding averaging. Our study showcases the applicability of deep learning-based protein representations, such as those provided by AlphaFold2, in tasks beyond structure prediction."
291,geneHapR: an R package for gene haplotypic statistics and visualization,"Background
Together with application of next-generation sequencing technologies and increased accumulation of genomic variation data in different organism species, an opportunity for effectively identification of superior alleles of functional genes to facilitate marker-assisted selection is emerging, and the clarification of haplotypes of functional genes is becoming an essential target in recent study works.
Results
In this paper, we describe an R package ‘geneHapR’ developed for haplotypes identification, statistics and visualization analysis of candidate genes. This package could integrate genotype data, genomic annotating information and phenotypic variation data to clarify genotype variations, evolutionary-ship, and morphological effects among haplotypes through variants visualization, network construction and phenotypic comparison. ‘geneHapR’ also provides functions for Linkage Disequilibrium block analysis and visualizing of haplotypes geo-distribution.
Conclusions
The R package ‘geneHapR’ provided an easy-to-use tool for haplotype identification, statistic and visualization for candidate gene and will provide useful clues for gene functional dissection and molecular-assistant pyramiding of beneficial alleles of functional locus in future breeding programs."
292,A systematic review of biologically-informed deep learning models for cancer: fundamental trends for encoding and interpreting oncology data,"Background
There is an increasing interest in the use of Deep Learning (DL) based methods as a supporting analytical framework in oncology. However, most direct applications of DL will deliver models with limited transparency and explainability, which constrain their deployment in biomedical settings.
Methods
This systematic review discusses DL models used to support inference in cancer biology with a particular emphasis on multi-omics analysis. It focuses on how existing models address the need for better dialogue with prior knowledge, biological plausibility and interpretability, fundamental properties in the biomedical domain. For this, we retrieved and analyzed 42 studies focusing on emerging architectural and methodological advances, the encoding of biological domain knowledge and the integration of explainability methods.
Results
We discuss the recent evolutionary arch of DL models in the direction of integrating prior biological relational and network knowledge to support better generalisation (e.g. pathways or Protein-Protein-Interaction networks) and interpretability. This represents a fundamental functional shift towards models which can integrate mechanistic and statistical inference aspects. We introduce a concept of 
bio-centric interpretability
 and according to its taxonomy, we discuss representational methodologies for the integration of domain prior knowledge in such models.
Conclusions
The paper provides a critical outlook into contemporary methods for explainability and interpretability used in DL for cancer. The analysis points in the direction of a convergence between encoding prior knowledge and improved interpretability. We introduce 
bio-centric interpretability
 which is an important step towards formalisation of 
biological interpretability
 of DL models and developing methods that are less problem- or application-specific."
293,Improving variant calling using population data and deep learning,"Large-scale population variant data is often used to filter and aid interpretation of variant calls in a single sample. These approaches do not incorporate population information directly into the process of variant calling, and are often limited to filtering which trades recall for precision. In this study, we develop population-aware DeepVariant models with a new channel encoding allele frequencies from the 1000 Genomes Project. This model reduces variant calling errors, improving both precision and recall in single samples, and reduces rare homozygous and pathogenic clinvar calls cohort-wide. We assess the use of population-specific or diverse reference panels, finding the greatest accuracy with diverse panels, suggesting that large, diverse panels are preferable to individual populations, even when the population matches sample ancestry. Finally, we show that this benefit generalizes to samples with different ancestry from the training data even when the ancestry is also excluded from the reference panel."
294,Nonnegative matrix factorization analysis and multiple machine learning methods identified IL17C and ACOXL as novel diagnostic biomarkers for atherosclerosis,"Background
Atherosclerosis is the common pathological basis for many cardiovascular and cerebrovascular diseases. The purpose of this study is to identify the diagnostic biomarkers related to atherosclerosis through machine learning algorithm.
Methods
Clinicopathological parameters and transcriptomics data were obtained from 4 datasets (GSE21545, GSE20129, GSE43292, GSE100927). A nonnegative matrix factorization algorithm was used to classify arteriosclerosis patients in GSE21545 dataset. Then, we identified prognosis-related differentially expressed genes (DEGs) between the subtypes. Multiple machine learning methods to detect pivotal markers. Discrimination, calibration and clinical usefulness of the predicting model were assessed using area under curve, calibration plot and decision curve analysis respectively. The expression level of the feature genes was validated in GSE20129, GSE43292, GSE100927.
Results
2 molecular subtypes of atherosclerosis was identified, and 223 prognosis-related DEGs between the 2 subtypes were identified. These genes are not only related to epithelial cell proliferation, mitochondrial dysfunction, but also to immune related pathways. Least absolute shrinkage and selection operator, random forest, support vector machine- recursive feature elimination show that IL17C and ACOXL were identified as diagnostic markers of atherosclerosis. The prediction model displayed good discrimination and good calibration. Decision curve analysis showed that this model was clinically useful. Moreover, IL17C and ACOXL were verified in other 3 GEO datasets, and also have good predictive performance.
Conclusion
IL17C and ACOXL were diagnostic genes of atherosclerosis and associated with higher incidence of ischemic events."
295,A statistical approach for identifying primary substrates of ZSWIM8-mediated microRNA degradation in small-RNA sequencing data,"Background
One strategy for identifying targets of a regulatory factor is to perturb the factor and use high-throughput RNA sequencing to examine the consequences. However, distinguishing direct targets from secondary effects and experimental noise can be challenging when confounding signal is present in the background at varying levels.
Results
Here, we present a statistical modeling strategy to identify microRNAs that are primary substrates of target-directed miRNA degradation (TDMD) mediated by ZSWIM8. This method uses a bi-beta-uniform mixture (BBUM) model to separate primary from background signal components, leveraging the expectation that primary signal is restricted to upregulation and not downregulation upon loss of ZSWIM8. The BBUM model strategy retained the apparent sensitivity and specificity of the previous ad hoc approach but was more robust against outliers, achieved a more consistent stringency, and could be performed using a single cutoff of false discovery rate (FDR).
Conclusions
We developed the BBUM model, a robust statistical modeling strategy to account for background secondary signal in differential expression data. It performed well for identifying primary substrates of TDMD and should be useful for other applications in which the primary regulatory targets are only upregulated or only downregulated. The BBUM model, FDR-correction algorithm, and significance-testing methods are available as an R package at 
https://github.com/wyppeter/bbum
."
296,BGWAS: Bayesian variable selection in linear mixed models with nonlocal priors for genome-wide association studies,"Background
Genome-wide association studies (GWAS) seek to identify single nucleotide polymorphisms (SNPs) that cause observed phenotypes. However, with highly correlated SNPs, correlated observations, and the number of SNPs being two orders of magnitude larger than the number of observations, GWAS procedures often suffer from high false positive rates.
Results
We propose BGWAS, a novel Bayesian variable selection method based on nonlocal priors for linear mixed models specifically tailored for genome-wide association studies. Our proposed method BGWAS uses a novel nonlocal prior for linear mixed models (LMMs). BGWAS has two steps: screening and model selection. The screening step scans through all the SNPs fitting one LMM for each SNP and then uses Bayesian false discovery control to select a set of candidate SNPs. After that, a model selection step searches through the space of LMMs that may have any number of SNPs from the candidate set. A simulation study shows that, when compared to popular GWAS procedures, BGWAS greatly reduces false positives while maintaining the same ability to detect true positive SNPs. We show the utility and flexibility of BGWAS with two case studies: a case study on salt stress in plants, and a case study on alcohol use disorder.
Conclusions
BGWAS maintains and in some cases increases the recall of true SNPs while drastically lowering the number of false positives compared to popular SMA procedures."
297,A neural network model to screen feature genes for pancreatic cancer,"All the time, pancreatic cancer is a problem worldwide because of its high degree of malignancy and increased mortality. Neural network model analysis is an efficient and accurate machine learning method that can quickly and accurately predict disease feature genes. The aim of our research was to build a neural network model that would help screen out feature genes for pancreatic cancer diagnosis and prediction of prognosis. Our study confirmed that the neural network model is a reliable way to predict feature genes of pancreatic cancer, and immune cells infiltrating play an essential role in the development of pancreatic cancer, especially neutrophils. ANO1, AHNAK2, and ADAM9 were eventually identified as feature genes of pancreatic cancer, helping to diagnose and predict prognosis. Neural network model analysis provides us with a new idea for finding new intervention targets for pancreatic cancer."
298,Pan-cancer analysis of SYNGR2 with a focus on clinical implications and immune landscape in liver hepatocellular carcinoma,"Background
Synaptogyrin-2 (SYNGR2), as a member of synaptogyrin gene family, is overexpressed in several types of cancer. However, the role of SYNGR2 in pan-cancer is largely unexplored.
Methods
From the TCGA and GEO databases, we obtained bulk transcriptomes, and clinical information. We examined the expression patterns, prognostic values, and diagnostic value of SYNGR2 in pan-cancer, and investigated the relationship of SYNGR2 expression with tumor mutation burden (TMB), microsatellite instability (MSI), immune infiltration, and immune checkpoint (ICP) genes. The gene set enrichment analysis (GSEA) software was used to perform pathway analysis. Besides, we built a nomogram of liver hepatocellular carcinoma patients (LIHC) and validated its prediction accuracy.
Results
SYNGR2 was highly expressed in most cancers. The high expression of SYNGR2 significantly reduced the overall survival (OS), disease-specific survival (DSS), disease-free interval (DFI), and progression-free interval (PFI) in multiple types of cancer. Also, receiver operating characteristic (ROC) curve analysis demonstrated that SYNGR2 showed high accuracy in distinguishing cancerous tissues from normal ones. Moreover, SYNGR2 expression was correlated with TMB, MSI, immune scores, and immune cell infiltrations. We also analyzed the association of SYNGR2 with immunotherapy response in LIHC. Finally, a nomogram including SYNGR2 and pathologic T, N, M stage was built and exhibited good predictive power for the OS, DSS, and PFI of LIHC patients.
Conclusion
Overall, SYNGR2 is a critical oncogene in various tumors. SYNGR2 participates in the carcinogenic progression, and may contribute to the immune infiltration in tumor microenvironment. Our study suggests that SYNGR2 can serve as a predictor related to prognosis in pan-cancer, especially LIHC."
299,Integrated bioinformatics analysis for conducting a prognostic model and identifying immunotherapeutic targets in gastric cancer,"Background
Gastric cancer is the third leading cause of death from cancer worldwide and has a poor prognosis. Practical risk scores and prognostic models for gastric cancer are lacking. While immunotherapy has succeeded in some cancers, few gastric cancer patients benefit from immunotherapy. Immune genes and the tumor microenvironment (TME) are essential for cancer progression and immunotherapy response. However, the roles of immune genes and the tumor microenvironment in immunotherapy remain unclear. The study aimed to construct a prognostic prediction model and identify immunotherapeutic targets for gastric cancer (GC) patients by exploring immune genes and the tumor microenvironment.
Results
An immune-related risk score (IRRS) model, including APOH, RNASE2, F2R, DEFB126, CXCL6, and CXCL3 genes, was constructed for risk stratification. Patients in the low-risk group, which was characterized by elevated tumor mutation burden (TMB) have higher survival rate. The risk level was remarkably correlated with tumor-infiltrating immune cells (TIICs), the immune checkpoint molecule expression, and immunophenoscore (IPS). CXCL3 and CXCL6 were significantly upregulated in gastric cancer tissues compared with normal tissues using the UALCAN database and RT-qPCR. The nomogram showed good calibration and moderate discrimination in predicting overall survival (OS) at 1-, 3-, and 5- year for gastric cancer patients using risk-level and clinical characteristics.
Conclusion
Our findings provided a risk stratification and prognosis prediction tool for gastric cancer patients and further the research into immunotherapy in gastric cancer."
300,Deep learning approach for early prediction of COVID-19 mortality using chest X-ray and electronic health records,"Background
An artificial-intelligence (AI) model for predicting the prognosis or mortality of coronavirus disease 2019 (COVID-19) patients will allow efficient allocation of limited medical resources. We developed an early mortality prediction ensemble model for COVID-19 using AI models with initial chest X-ray and electronic health record (EHR) data.

Results
We used convolutional neural network (CNN) models (Inception-ResNet-V2 and EfficientNet) for chest X-ray analysis and multilayer perceptron (MLP), Extreme Gradient Boosting (XGBoost), and random forest (RF) models for EHR data analysis. The Gradient-weighted Class Activation Mapping and Shapley Additive Explanations (SHAP) methods were used to determine the effects of these features on COVID-19. We developed an ensemble model (Area under the receiver operating characteristic curve of 0.8698) using a soft voting method with weight differences for CNN, XGBoost, MLP, and RF models. To resolve the data imbalance, we conducted F1-score optimization by adjusting the cutoff values to optimize the model performance (F1 score of 0.77).
Conclusions
Our study is meaningful in that we developed an early mortality prediction model using only the initial chest X-ray and EHR data of COVID-19 patients. Early prediction of the clinical courses of patients is helpful for not only treatment but also bed management. Our results confirmed the performance improvement of the ensemble model achieved by combining AI models. Through the SHAP method, laboratory tests that indicate the factors affecting COVID-19 mortality were discovered, highlighting the importance of these tests in managing COVID-19 patients."
301,HLA-Clus: HLA class I clustering based on 3D structure,"Background
In a previous paper, we classified populated HLA class I alleles into supertypes and subtypes based on the similarity of 3D landscape of peptide binding grooves, using newly defined structure distance metric and hierarchical clustering approach. Compared to other approaches, our method achieves higher correlation with peptide binding specificity, intra-cluster similarity (cohesion), and robustness. Here we introduce HLA-Clus, a Python package for clustering HLA Class I alleles using the method we developed recently and describe additional features including a new nearest neighbor clustering method that facilitates clustering based on user-defined criteria.
Results
The HLA-Clus pipeline includes three stages: First, HLA Class I structural models are coarse grained and transformed into clouds of labeled points. Second, similarities between alleles are determined using a newly defined structure distance metric that accounts for spatial and physicochemical similarities. Finally, alleles are clustered via hierarchical or nearest-neighbor approaches. We also interfaced HLA-Clus with the peptide:HLA affinity predictor MHCnuggets. By using the nearest neighbor clustering method to select optimal allele-specific deep learning models in MHCnuggets, the average accuracy of peptide binding prediction of rare alleles was improved.
Conclusions
The HLA-Clus package offers a solution for characterizing the peptide binding specificities of a large number of HLA alleles. This method can be applied in HLA functional studies, such as the development of peptide affinity predictors, disease association studies, and HLA matching for grafting. HLA-Clus is freely available at our GitHub repository (
https://github.com/yshen25/HLA-Clus
)."
302,GKLOMLI: a link prediction model for inferring miRNA–lncRNA interactions by using Gaussian kernel-based method on network profile and linear optimization algorithm,"Background
The limited knowledge of miRNA–lncRNA interactions is considered as an obstruction of revealing the regulatory mechanism. Accumulating evidence on 
Human
 diseases indicates that the modulation of gene expression has a great relationship with the interactions between miRNAs and lncRNAs. However, such interaction validation via crosslinking-immunoprecipitation and high-throughput sequencing (CLIP-seq) experiments that inevitably costs too much money and time but with unsatisfactory results. Therefore, more and more computational prediction tools have been developed to offer many reliable candidates for a better design of further bio-experiments.

Methods
In this work, we proposed a novel link prediction model based on Gaussian kernel-based method and linear optimization algorithm for inferring miRNA–lncRNA interactions (GKLOMLI). Given an observed miRNA–lncRNA interaction network, the Gaussian kernel-based method was employed to output two similarity matrixes of miRNAs and lncRNAs. Based on the integrated matrix combined with similarity matrixes and the observed interaction network, a linear optimization-based link prediction model was trained for inferring miRNA–lncRNA interactions.
Results
To evaluate the performance of our proposed method, 
k
-fold cross-validation (CV) and leave-one-out CV were implemented, in which each CV experiment was carried out 100 times on a training set generated randomly. The high area under the curves (AUCs) at 0.8623 ± 0.0027 (2-fold CV), 0.9053 ± 0.0017 (5-fold CV), 0.9151 ± 0.0013 (10-fold CV), and 0.9236 (LOO-CV), illustrated the precision and reliability of our proposed method.
Conclusion
GKLOMLI with high performance is anticipated to be used to reveal underlying interactions between miRNA and their target lncRNAs, and deciphers the potential mechanisms of the complex diseases."
303,Model selection and robust inference of mutational signatures using Negative Binomial non-negative matrix factorization,"Background
The spectrum of mutations in a collection of cancer genomes can be described by a mixture of a few mutational signatures. The mutational signatures can be found using non-negative matrix factorization (NMF). To extract the mutational signatures we have to assume a distribution for the observed mutational counts and a number of mutational signatures. In most applications, the mutational counts are assumed to be Poisson distributed, and the rank is chosen by comparing the fit of several models with the same underlying distribution and different values for the rank using classical model selection procedures. However, the counts are often overdispersed, and thus the Negative Binomial distribution is more appropriate.
Results
We propose a Negative Binomial NMF with a patient specific dispersion parameter to capture the variation across patients and derive the corresponding update rules for parameter estimation. We also introduce a novel model selection procedure inspired by cross-validation to determine the number of signatures. Using simulations, we study the influence of the distributional assumption on our method together with other classical model selection procedures. We also present a simulation study with a method comparison where we show that state-of-the-art methods are highly overestimating the number of signatures when overdispersion is present. We apply our proposed analysis on a wide range of simulated data and on two real data sets from breast and prostate cancer patients. On the real data we describe a residual analysis to investigate and validate the model choice.
Conclusions
With our results on simulated and real data we show that our model selection procedure is more robust at determining the correct number of signatures under model misspecification. We also show that our model selection procedure is more accurate than the available methods in the literature for finding the true number of signatures. Lastly, the residual analysis clearly emphasizes the overdispersion in the mutational count data. The code for our model selection procedure and Negative Binomial NMF is available in the R package SigMoS and can be found at 
https://github.com/MartaPelizzola/SigMoS
."
304,Semi-supervised learning improves regulatory sequence prediction with unlabeled sequences,"Motivation
Genome-wide association studies have systematically identified thousands of single nucleotide polymorphisms (SNPs) associated with complex genetic diseases. However, the majority of those SNPs were found in non-coding genomic regions, preventing the understanding of the underlying causal mechanism. Predicting molecular processes based on the DNA sequence represents a promising approach to understand the role of those non-coding SNPs. Over the past years, deep learning was successfully applied to regulatory sequence prediction using supervised learning. Supervised learning required DNA sequences associated with functional data for training, whose amount is strongly limited by the finite size of the human genome. Conversely, the amount of mammalian DNA sequences is exponentially increasing due to ongoing large sequencing projects, but without functional data in most cases.
Results
To alleviate the limitations of supervised learning, we propose a paradigm shift with semi-supervised learning, which does not only exploit labeled sequences (e.g. human genome with ChIP-seq experiment), but also unlabeled sequences available in much larger amounts (e.g. from other species without ChIP-seq experiment, such as chimpanzee). Our approach is flexible and can be plugged into any neural architecture including shallow and deep networks, and shows strong predictive performance improvements compared to supervised learning in most cases (up to 
\(70\%\)
).
Availability and implementation
https://forgemia.inra.fr/raphael.mourad/deepgnn
."
305,Identification of cuproptosis-related lncRNAs to predict prognosis and immune infiltration characteristics in alimentary tract malignancies,"Background
Alimentary tract malignancies (ATM) caused nearly one-third of all tumor-related death. Cuproptosis is a newly identified cell death pattern. The role of cuproptosis-associated lncRNAs in ATM is unknown.
Method
Data from The Cancer Genome Atlas (TCGA) and Gene Expression Omnibus (GEO) databases were used to identify prognostic lncRNAs by Cox regression and LASSO. Then a predictive nomogram was constructed based on seven prognostic lncRNAs. In addition, the prognostic potential of the seven-lncRNA signature was verified via survival analysis, the receiver operating characteristic (ROC) curve, calibration curve, and clinicopathologic characteristics correlation analysis. Furthermore, we explored the associations between the signature risk score and immune landscape, and somatic gene mutation.
Results
We identified 1211 cuproptosis-related lncRNAs and seven survival-related lncRNAs. Patients were categorized into high-risk and low-risk groups with significantly different prognoses. ROC and calibration curve confirmed the good prediction capability of the risk model and nomogram. Somatic mutations between the two groups were compared. We also found that patients in the two groups responded differently to immune checkpoint inhibitors and immunotherapy.
Conclusion
The proposed novel seven lncRNAs nomogram could predict prognosis and guide treatment of ATM. Further research was required to validate the nomogram."
306,AttSec: protein secondary structure prediction by capturing local patterns from attention map,"Background
Protein secondary structures that link simple 1D sequences to complex 3D structures can be used as good features for describing the local properties of protein, but also can serve as key features for predicting the complex 3D structures of protein. Thus, it is very important to accurately predict the secondary structure of the protein, which contains a local structural property assigned by the pattern of hydrogen bonds formed between amino acids. In this study, we accurately predict protein secondary structure by capturing the local patterns of protein. For this objective, we present a novel prediction model, AttSec, based on transformer architecture. In particular, AttSec extracts self-attention maps corresponding to pairwise features between amino acid embeddings and passes them through 2D convolution blocks to capture local patterns. In addition, instead of using additional evolutionary information, it uses protein embedding as an input, which is generated by a language model.
Results
For the ProteinNet DSSP8 dataset, our model showed 11.8% better performance on the entire evaluation datasets compared with other no-evolutionary-information-based models. For the NetSurfP-2.0 DSSP8 dataset, it showed 1.2% better performance on average. There was an average performance improvement of 9.0% for the ProteinNet DSSP3 dataset and an average of 0.7% for the NetSurfP-2.0 DSSP3 dataset.
Conclusion
We accurately predict protein secondary structure by capturing the local patterns of protein. For this objective, we present a novel prediction model, AttSec, based on transformer architecture. Although there was no dramatic accuracy improvement compared with other models, the improvement on DSSP8 was greater than that on DSSP3. This result implies that using our proposed pairwise feature could have a remarkable effect for several challenging tasks that require finely subdivided classification. Github package URL is 
https://github.com/youjin-DDAI/AttSec
."
307,MBECS: Microbiome Batch Effects Correction Suite,"Despite the availability of batch effect correcting algorithms (BECA), no comprehensive tool that combines batch correction and evaluation of the results exists for microbiome datasets. This work outlines the Microbiome Batch Effects Correction Suite development that integrates several BECAs and evaluation metrics into a software package for the statistical computation framework R."
308,CAGECAT: The CompArative GEne Cluster Analysis Toolbox for rapid search and visualisation of homologous gene clusters,"Background
Co-localized sets of genes that encode specialized functions are common across microbial genomes and occur in genomes of larger eukaryotes as well. Important examples include Biosynthetic Gene Clusters (BGCs) that produce specialized metabolites with medicinal, agricultural, and industrial value (e.g. antimicrobials). Comparative analysis of BGCs can aid in the discovery of novel metabolites by highlighting distribution and identifying variants in public genomes. Unfortunately, gene-cluster-level homology detection remains inaccessible, time-consuming and difficult to interpret.
Results
The comparative gene cluster analysis toolbox (CAGECAT) is a rapid and user-friendly platform to mitigate difficulties in comparative analysis of whole gene clusters. The software provides homology searches and downstream analyses without the need for command-line or programming expertise. By leveraging remote BLAST databases, which always provide up-to-date results, CAGECAT can yield relevant matches that aid in the comparison, taxonomic distribution, or evolution of an unknown query. The service is extensible and interoperable and implements the cblaster and clinker pipelines to perform homology search, filtering, gene neighbourhood estimation, and dynamic visualisation of resulting variant BGCs. With the visualisation module, publication-quality figures can be customized directly from a web-browser, which greatly accelerates their interpretation via informative overlays to identify conserved genes in a BGC query.
Conclusion
Overall, CAGECAT is an extensible software that can be interfaced via a standard web-browser for whole region homology searches and comparison on continually updated genomes from NCBI. The public web server and installable docker image are open source and freely available without registration at: 
https://cagecat.bioinformatics.nl
."
309,ERStruct: a fast Python package for inferring the number of top principal components from whole genome sequencing data,"Background
Large-scale multi-ethnic DNA sequencing data is increasingly available owing to decreasing cost of modern sequencing technologies. Inference of the population structure with such sequencing data is fundamentally important. However, the ultra-dimensionality and complicated linkage disequilibrium patterns across the whole genome make it challenging to infer population structure using traditional principal component analysis based methods and software.
Results
We present the ERStruct Python Package, which enables the inference of population structure using whole-genome sequencing data. By leveraging parallel computing and GPU acceleration, our package achieves significant improvements in the speed of matrix operations for large-scale data. Additionally, our package features adaptive data splitting capabilities to facilitate computation on GPUs with limited memory.
Conclusion
Our Python package ERStruct is an efficient and user-friendly tool for estimating the number of top informative principal components that capture population structure from whole genome sequencing data."
310,Faster and more accurate pathogenic combination predictions with VarCoPP2.0,"Background
The prediction of potentially pathogenic variant combinations in patients remains a key task in the field of medical genetics for the understanding and detection of oligogenic/multilocus diseases. Models tailored towards such cases can help shorten the gap of missing diagnoses and can aid researchers in dealing with the high complexity of the derived data. The predictor VarCoPP (Variant Combinations Pathogenicity Predictor) that was published in 2019 and identified potentially pathogenic variant combinations in gene pairs (bilocus variant combinations), was the first important step in this direction. Despite its usefulness and applicability, several issues still remained that hindered a better performance, such as its False Positive (FP) rate, the quality of its training set and its complex architecture.
Results
We present VarCoPP2.0: the successor of VarCoPP that is a simplified, faster and more accurate predictive model identifying potentially pathogenic bilocus variant combinations. Results from cross-validation and on independent data sets reveal that VarCoPP2.0 has improved in terms of both sensitivity (95% in cross-validation and 98% during testing) and specificity (5% FP rate). At the same time, its running time shows a significant 150-fold decrease due to the selection of a simpler Balanced Random Forest model. Its positive training set now consists of variant combinations that are more confidently linked with evidence of pathogenicity, based on the confidence scores present in OLIDA, the Oligogenic Diseases Database (
https://olida.ibsquare.be
). The improvement of its performance is also attributed to a more careful selection of up-to-date features identified via an original wrapper method. We show that the combination of different variant and gene pair features together is important for predictions, highlighting the usefulness of integrating biological information at different levels.
Conclusions
Through its improved performance and faster execution time, VarCoPP2.0 enables a more accurate analysis of larger data sets linked to oligogenic diseases. Users can access the ORVAL platform (
https://orval.ibsquare.be
) to apply VarCoPP2.0 on their data."
311,A comparison of feature selection methodologies and learning algorithms in the development of a DNA methylation-based telomere length estimator,"Background
The field of epigenomics holds great promise in understanding and treating disease with advances in machine learning (ML) and artificial intelligence being vitally important in this pursuit. Increasingly, research now utilises DNA methylation measures at cytosine–guanine dinucleotides (CpG) to detect disease and estimate biological traits such as aging. Given the challenge of high dimensionality of DNA methylation data, feature-selection techniques are commonly employed to reduce dimensionality and identify the most important subset of features. In this study, our aim was to test and compare a range of feature-selection methods and ML algorithms in the development of a novel DNA methylation-based telomere length (TL) estimator. We utilised both nested cross-validation and two independent test sets for the comparisons.
Results
We found that principal component analysis in advance of elastic net regression led to the overall best performing estimator when evaluated using a nested cross-validation analysis and two independent test cohorts. This approach achieved a correlation between estimated and actual TL of 0.295 (83.4% CI [0.201, 0.384]) on the EXTEND test data set. Contrastingly, the baseline model of elastic net regression with no prior feature reduction stage performed less well in general—suggesting a prior feature-selection stage may have important utility. A previously developed TL estimator, DNAmTL, achieved a correlation of 0.216 (83.4% CI [0.118, 0.310]) on the EXTEND data. Additionally, we observed that different DNA methylation-based TL estimators, which have few common CpGs, are associated with many of the same biological entities.
Conclusions
The variance in performance across tested approaches shows that estimators are sensitive to data set heterogeneity and the development of an optimal DNA methylation-based estimator should benefit from the robust methodological approach used in this study. Moreover, our methodology which utilises a range of feature-selection approaches and ML algorithms could be applied to other biological markers and disease phenotypes, to examine their relationship with DNA methylation and predictive value."
312,Prediction of disease-related miRNAs by voting with multiple classifiers,"There is strong evidence to support that mutations and dysregulation of miRNAs are associated with a variety of diseases, including cancer. However, the experimental methods used to identify disease-related miRNAs are expensive and time-consuming. Effective computational approaches to identify disease-related miRNAs are in high demand and would aid in the detection of lncRNA biomarkers for disease diagnosis, treatment, and prevention. In this study, we develop an ensemble learning framework to reveal the potential associations between miRNAs and diseases (ELMDA). The ELMDA framework does not rely on the known associations when calculating miRNA and disease similarities and uses multi-classifiers voting to predict disease-related miRNAs. As a result, the average AUC of the ELMDA framework was 0.9229 for the HMDD v2.0 database in a fivefold cross-validation. All potential associations in the HMDD V2.0 database were predicted, and 90% of the top 50 results were verified with the updated HMDD V3.2 database. The ELMDA framework was implemented to investigate gastric neoplasms, prostate neoplasms and colon neoplasms, and 100%, 94%, and 90%, respectively, of the top 50 potential miRNAs were validated by the HMDD V3.2 database. Moreover, the ELMDA framework can predict isolated disease-related miRNAs. In conclusion, ELMDA appears to be a reliable method to uncover disease-associated miRNAs."
313,Screening prognostic markers for hepatocellular carcinoma based on pyroptosis-related lncRNA pairs,"Background
Pyroptosis is closely related to cancer prognosis. In this study, we tried to construct an individualized prognostic risk model for hepatocellular carcinoma (HCC) based on within-sample relative expression orderings (REOs) of pyroptosis-related lncRNAs (PRlncRNAs).
Methods
RNA-seq data of 343 HCC samples derived from The Cancer Genome Atlas (TCGA) database were analyzed. PRlncRNAs were detected based on differentially expressed lncRNAs between sample groups clustered by 40 reported pyroptosis-related genes (PRGs). Univariate Cox regression was used to screen out prognosis-related PRlncRNA pairs. Then, based on REOs of prognosis-related PRlncRNA pairs, a risk model for HCC was constructed by combining LASSO and stepwise multivariate Cox regression analysis. Finally, a prognosis-related competing endogenous RNA (ceRNA) network was built based on information about lncRNA–miRNA–mRNA interactions derived from the miRNet and TargetScan databases.
Results
Hierarchical clustering of HCC patients according to the 40 PRGs identified two groups with a significant survival difference (Kaplan–Meier log-rank, 
p
 = 0.026). Between the two groups, 104 differentially expressed lncRNAs were identified (|log
2
(FC)|> 1 and FDR < 5%). Among them, 83 PRlncRNA pairs showed significant associations between their REOs within HCC samples and overall survival (Univariate Cox regression, 
p
 < 0.005). An optimal 11-PRlncRNA-pair prognostic risk model was constructed for HCC. The areas under the curves (AUCs) of time-dependent receiver operating characteristic (ROC) curves of the risk model for 1-, 3-, and 5-year survival were 0.737, 0.705, and 0.797 in the validation set, respectively. Gene Set Enrichment Analysis showed that inflammation-related interleukin signaling pathways were upregulated in the predicted high-risk group (
p
 < 0.05). Tumor immune infiltration analysis revealed a higher abundance of regulatory T cells (Tregs) and M2 macrophages and a lower abundance of CD8 + T cells in the high-risk group, indicating that excessive pyroptosis might occur in high-risk patients. Finally, eleven lncRNA–miRNA–mRNA regulatory axes associated with pyroptosis were established.
Conclusion
Our risk model allowed us to determine the robustness of the REO-based PRlncRNA prognostic biomarkers in the stratification of HCC patients at high and low risk. The model is also helpful for understanding the molecular mechanisms between pyroptosis and HCC prognosis. High-risk patients may have excessive pyroptosis and thus be less sensitive to immune therapy."
314,"Automated recognition and analysis of body bending behavior in 
C. elegans","Background
Locomotion behaviors of 
Caenorhabditis elegans
 play an important role in drug activity screening, anti-aging research, and toxicological assessment. Previous studies have provided important insights into drug activity screening, anti-aging, and toxicological research by manually counting the number of body bends. However, manual counting is often low-throughput and takes a lot of time and manpower. And it is easy to cause artificial bias and error in counting results.
Results
In this paper, an algorithm is proposed for automatic counting and analysis of the body bending behavior of nematodes. First of all, the numerical coordinate regression method with convolutional neural network is used to obtain the head and tail coordinates. Next, curvature-based feature point extraction algorithm is used to calculate the feature points of the nematode centerline. Then the maximum distance between the peak point and the straight line between the pharynx and the tail is calculated. The number of body bends is counted according to the change in the maximum distance per frame.
Conclusion
Experiments are performed to prove the effectiveness of the proposed algorithm. The accuracy of head coordinate prediction is 0.993, and the accuracy of tail coordinate prediction is 0.990. The Pearson correlation coefficient between the results of the automatic count and manual count of the number of body bends is 0.998 and the mean absolute error is 1.931. Different strains of nematodes are selected to analyze differences in body bending behavior, demonstrating a relationship between nematode vitality and lifespan. The code is freely available at 
https://github.com/hthana/Body-Bend-Count
."
315,"POInT
browse
: orthology prediction and synteny exploration for paleopolyploid genomes","We describe POInT
browse
, a web portal that gives access to the orthology inferences made for polyploid genomes with POInT, the Polyploidy Orthology Inference Tool. Ancient, or paleo-, polyploidy events are widely distributed across the eukaryotic phylogeny, and the combination of duplicated and lost duplicated genes that these polyploidies produce can confound the identification of orthologous genes between genomes. POInT uses conserved synteny and phylogenetic models to infer orthologous genes between genomes with a shared polyploidy. It also gives confidence estimates for those orthology inferences. POInT
browse
 gives both graphical and query-based access to these inferences from 12 different polyploidy events, allowing users to visualize genomic regions produced by polyploidies and perform batch queries for each polyploidy event, downloading genes trees and coding sequences for orthologous genes meeting user-specified criteria. POInT
browse
 and the associated data are online at 
https://wgd.statgen.ncsu.edu
."
316,Magicmol: a light-weighted pipeline for drug-like molecule evolution and quick chemical space exploration,"The flourishment of machine learning and deep learning methods has boosted the development of cheminformatics, especially regarding the application of drug discovery and new material exploration. Lower time and space expenses make it possible for scientists to search the enormous chemical space. Recently, some work combined reinforcement learning strategies with recurrent neural network (RNN)-based models to optimize the property of generated small molecules, which notably improved a batch of critical factors for these candidates. However, a common problem among these RNN-based methods is that several generated molecules have difficulty in synthesizing despite owning higher desired properties such as binding affinity. However, RNN-based framework better reproduces the molecule distribution among the training set than other categories of models during molecule exploration tasks. Thus, to optimize the whole exploration process and make it contribute to the optimization of specified molecules, we devised a light-weighted pipeline called Magicmol; this pipeline has a re-mastered RNN network and utilize SELFIES presentation instead of SMILES. Our backbone model achieved extraordinary performance while reducing the training cost; moreover, we devised reward truncate strategies to eliminate the model collapse problem. Additionally, adopting SELFIES presentation made it possible to combine STONED-SELFIES as a post-processing procedure for specified molecule optimization and quick chemical space exploration."
317,ecpc: an R-package for generic co-data models for high-dimensional prediction,"Background
High-dimensional prediction considers data with more variables than samples. Generic research goals are to find the best predictor or to select variables. Results may be improved by exploiting prior information in the form of co-data, providing complementary data not on the samples, but on the variables. We consider adaptive ridge penalised generalised linear and Cox models, in which the variable-specific ridge penalties are adapted to the co-data to give a priori more weight to more important variables. The 
R
-package 
ecpc
 originally accommodated various and possibly multiple co-data sources, including categorical co-data, i.e. groups of variables, and continuous co-data. Continuous co-data, however, were handled by adaptive discretisation, potentially inefficiently modelling and losing information. As continuous co-data such as external 
p
 values or correlations often arise in practice, more generic co-data models are needed.
Results
Here, we present an extension to the method and software for generic co-data models, particularly for continuous co-data. At the basis lies a classical linear regression model, regressing prior variance weights on the co-data. Co-data variables are then estimated with empirical Bayes moment estimation. After placing the estimation procedure in the classical regression framework, extension to generalised additive and shape constrained co-data models is straightforward. Besides, we show how ridge penalties may be transformed to elastic net penalties. In simulation studies we first compare various co-data models for continuous co-data from the extension to the original method. Secondly, we compare variable selection performance to other variable selection methods. The extension is faster than the original method and shows improved prediction and variable selection performance for non-linear co-data relations. Moreover, we demonstrate use of the package in several genomics examples throughout the paper.
Conclusions
The 
R
-package 
ecpc
 accommodates linear, generalised additive and shape constrained additive co-data models for the purpose of improved high-dimensional prediction and variable selection. The extended version of the package as presented here (version number 3.1.1 and higher) is available on (
https://cran.r-project.org/web/packages/ecpc/
)."
318,DGH-GO: dissecting the genetic heterogeneity of complex diseases using gene ontology,"Background
Complex diseases such as neurodevelopmental disorders (NDDs) exhibit multiple etiologies. The multi-etiological nature of complex-diseases emerges from distinct but functionally similar group of genes. Different diseases sharing genes of such groups show related clinical outcomes that further restrict our understanding of disease mechanisms, thus, limiting the applications of personalized medicine approaches to complex genetic disorders.
Results
Here, we present an interactive and user-friendly application, called DGH-GO. DGH-GO allows biologists to dissect the genetic heterogeneity of complex diseases by stratifying the putative disease-causing genes into clusters that may contribute to distinct disease outcome development. It can also be used to study the shared etiology of complex-diseases. DGH-GO creates a semantic similarity matrix for the input genes by using Gene Ontology (GO). The resultant matrix can be visualized in 2D plots using different dimension reduction methods (T-SNE, Principal component analysis, umap and Principal coordinate analysis). In the next step, clusters of functionally similar genes are identified from genes functional similarities assessed through GO. This is achieved by employing four different clustering methods (K-means, Hierarchical, Fuzzy and PAM). The user may change the clustering parameters and explore their effect on stratification immediately. DGH-GO was applied to genes disrupted by rare genetic variants in Autism Spectrum Disorder (ASD) patients. The analysis confirmed the multi-etiological nature of ASD by identifying four clusters of genes that were enriched for distinct biological mechanisms and clinical outcome. In the second case study, the analysis of genes shared by different NDDs showed that genes causing multiple disorders tend to aggregate in similar clusters, indicating a possible shared etiology.
Conclusion
DGH-GO is a user-friendly application that allows biologists to study the multi-etiological nature of complex diseases by dissecting their genetic heterogeneity. In summary, functional similarities, dimension reduction and clustering methods, coupled with interactive visualization and control over analysis allows biologists to explore and analyze their datasets without requiring expert knowledge on these methods. The source code of proposed application is available at 
https://github.com/Muh-Asif/DGH-GO"
319,"Priors, population sizes, and power in genome-wide hypothesis tests","Background
Genome-wide tests, including genome-wide association studies (GWAS) of germ-line genetic variants, driver tests of cancer somatic mutations, and transcriptome-wide association tests of RNAseq data, carry a high multiple testing burden. This burden can be overcome by enrolling larger cohorts or alleviated by using prior biological knowledge to favor some hypotheses over others. Here we compare these two methods in terms of their abilities to boost the power of hypothesis testing.
Results
We provide a quantitative estimate for progress in cohort sizes and present a theoretical analysis of the power of oracular hard priors: priors that select a subset of hypotheses for testing, with an oracular guarantee that all true positives are within the tested subset. This theory demonstrates that for GWAS, strong priors that limit testing to 100–1000 genes provide less power than typical annual 20–40% increases in cohort sizes. Furthermore, non-oracular priors that exclude even a small fraction of true positives from the tested set can perform worse than not using a prior at all.
Conclusion
Our results provide a theoretical explanation for the continued dominance of simple, unbiased univariate hypothesis tests for GWAS: if a statistical question can be answered by larger cohort sizes, it should be answered by larger cohort sizes rather than by more complicated biased methods involving priors. We suggest that priors are better suited for non-statistical aspects of biology, such as pathway structure and causality, that are not yet easily captured by standard hypothesis tests."
320,moBRCA-net: a breast cancer subtype classification framework based on multi-omics attention neural networks,"Background
Breast cancer is a highly heterogeneous disease that comprises multiple biological components. Owing its diversity, patients have different prognostic outcomes; hence, early diagnosis and accurate subtype prediction are critical for treatment. Standardized breast cancer subtyping systems, mainly based on single-omics datasets, have been developed to ensure proper treatment in a systematic manner. Recently, multi-omics data integration has attracted attention to provide a comprehensive view of patients but poses a challenge due to the high dimensionality. In recent years, deep learning-based approaches have been proposed, but they still present several limitations.
Results
In this study, we describe moBRCA-net, an interpretable deep learning-based breast cancer subtype classification framework that uses multi-omics datasets. Three omics datasets comprising gene expression, DNA methylation and microRNA expression data were integrated while considering the biological relationships among them, and a self-attention module was applied to each omics dataset to capture the relative importance of each feature. The features were then transformed to new representations considering the respective learned importance, allowing moBRCA-net to predict the subtype.
Conclusions
Experimental results confirmed that moBRCA-net has a significantly enhanced performance compared with other methods, and the effectiveness of multi-omics integration and omics-level attention were identified. moBRCA-net is publicly available at 
https://github.com/cbi-bioinfo/moBRCA-net
."
321,meth-SemiCancer: a cancer subtype classification framework via semi-supervised learning utilizing DNA methylation profiles,"Background
Identification of the cancer subtype plays a crucial role to provide an accurate diagnosis and proper treatment to improve the clinical outcomes of patients. Recent studies have shown that DNA methylation is one of the key factors for tumorigenesis and tumor growth, where the DNA methylation signatures have the potential to be utilized as cancer subtype-specific markers. However, due to the high dimensionality and the low number of DNA methylome cancer samples with the subtype information, still, to date, a cancer subtype classification method utilizing DNA methylome datasets has not been proposed.
Results
In this paper, we present meth-SemiCancer, a semi-supervised cancer subtype classification framework based on DNA methylation profiles. The proposed model was first pre-trained based on the methylation datasets with the cancer subtype labels. After that, meth-SemiCancer generated the pseudo-subtypes for the cancer datasets without subtype information based on the model’s prediction. Finally, fine-tuning was performed utilizing both the labeled and unlabeled datasets.
Conclusions
From the performance comparison with the standard machine learning-based classifiers, meth-SemiCancer achieved the highest average F1-score and Matthews correlation coefficient, outperforming other methods. Fine-tuning the model with the unlabeled patient samples by providing the proper pseudo-subtypes, encouraged meth-SemiCancer to generalize better than the supervised neural network-based subtype classification method. meth-SemiCancer is publicly available at 
https://github.com/cbi-bioinfo/meth-SemiCancer
."
322,CRISPR-Cas-Docker: web-based in silico docking and machine learning-based classification of crRNAs with Cas proteins,"Background
CRISPR-Cas-Docker is a web server for in silico docking experiments with CRISPR RNAs (crRNAs) and Cas proteins. This web server aims at providing experimentalists with the optimal crRNA-Cas pair predicted computationally when prokaryotic genomes have multiple CRISPR arrays and Cas systems, as frequently observed in metagenomic data.

Results
CRISPR-Cas-Docker provides two methods to predict the optimal Cas protein given a particular crRNA sequence: a structure-based method (in silico docking) and a sequence-based method (machine learning classification). For the structure-based method, users can either provide experimentally determined 3D structures of these macromolecules or use an integrated pipeline to generate 3D-predicted structures for in silico docking experiments.
Conclusion
CRISPR-Cas-Docker addresses the need of the CRISPR-Cas community to predict RNA–protein interactions in silico by optimizing multiple stages of computation and evaluation, specifically for CRISPR-Cas systems. CRISPR-Cas-Docker is available at 
www.crisprcasdocker.org
 as a web server, and at 
https://github.com/hshimlab/CRISPR-Cas-Docker
 as an open-source tool."
323,Robust classification of wound healing stages in both mice and humans for acute and burn wounds based on transcriptomic data,"Background
Wound healing involves careful coordination among various cell types carrying out unique or even multifaceted functions. The abstraction of this complex dynamic process into four primary wound stages is essential to the study of wound care for timing treatment and tracking wound progression. For example, a treatment that may promote healing in the inflammatory stage may prove detrimental in the proliferative stage. Additionally, the time scale of individual responses varies widely across and within the same species. Therefore, a robust method to assess wound stages can help advance translational work from animals to humans.
Results
In this work, we present a data-driven model that robustly identifies the dominant wound healing stage using transcriptomic data from biopsies gathered from mouse and human wounds, both burn and surgical. A training dataset composed of publicly available transcriptomic arrays is used to derive 58 shared genes that are commonly differentially expressed. They are divided into 5 clusters based on temporal gene expression dynamics. The clusters represent a 5-dimensional parametric space containing the wound healing trajectory. We then create a mathematical classification algorithm in the 5-dimensional space and demonstrate that it can distinguish between the four stages of wound healing: hemostasis, inflammation, proliferation, and remodeling.
Conclusions
In this work, we present an algorithm for wound stage detection based on gene expression. This work suggests that there are universal characteristics of gene expression in wound healing stages despite the seeming disparities across species and wounds. Our algorithm performs well for human and mouse wounds of both burn and surgical types. The algorithm has the potential to serve as a diagnostic tool that can advance precision wound care by providing a way of tracking wound healing progression with more accuracy and finer temporal resolution compared to visual indicators. This increases the potential for preventive action."
324,Machine learning and drug discovery for neglected tropical diseases,"Neglected tropical diseases affect millions of individuals and cause loss of productivity worldwide. They are common in developing countries without the financial resources for research and drug development. With increased availability of data from high throughput screening, machine learning has been introduced into the drug discovery process. Models can be trained to predict biological activities of compounds before working in the lab. In this study, we use three publicly available, high-throughput screening datasets to train machine learning models to predict biological activities related to inhibition of species that cause leishmaniasis, American trypanosomiasis (Chagas disease), and African trypanosomiasis (sleeping sickness). We compare machine learning models (tree based models, naive Bayes classifiers, and neural networks), featurizing methods (circular fingerprints, MACCS fingerprints, and RDKit descriptors), and techniques to deal with the imbalanced data (oversampling, undersampling, class weight/sample weight)."
325,Fast all versus all genotype comparison using DNA/RNA sequencing data: method and workflow,"Background
Massively parallel sequencing includes many liquid handling steps which introduce the possibility of sample swaps, mixing, and duplication. The unique profile of inherited variants in human genomes allows for comparison of sample identity using sequence data. A comparison of all samples vs. each other (all vs. all) provides both identification of mismatched samples and the possibility of resolving swapped samples. However, all vs. all comparison complexity grows as the square of the number of samples, so efficiency becomes essential.
Results
We have developed a tool for fast all vs. all genotype comparison using low level bitwise operations built into the Perl programming language. Importantly, we have also developed a complete workflow allowing users to start with either raw FASTQ sequence files, aligned BAM files, or genotype VCF files and automatically generate comparison metrics and summary plots. The tool is freely available at 
https://github.com/teerjk/TimeAttackGenComp/
.
Conclusions
A fast and easy to use method for genotype comparison as described here is an important tool to ensure high quality and robust results in sequencing studies."
326,A gene regulatory network inference model based on pseudo-siamese network,"Motivation
Gene regulatory networks (GRNs) arise from the intricate interactions between transcription factors (TFs) and their target genes during the growth and development of organisms. The inference of GRNs can unveil the underlying gene interactions in living systems and facilitate the investigation of the relationship between gene expression patterns and phenotypic traits. Although several machine-learning models have been proposed for inferring GRNs from single-cell RNA sequencing (scRNA-seq) data, some of these models, such as Boolean and tree-based networks, suffer from sensitivity to noise and may encounter difficulties in handling the high noise and dimensionality of actual scRNA-seq data, as well as the sparse nature of gene regulation relationships. Thus, inferring large-scale information from GRNs remains a formidable challenge.
Results
This study proposes a multilevel, multi-structure framework called a pseudo-Siamese GRN (PSGRN) for inferring large-scale GRNs from time-series expression datasets. Based on the pseudo-Siamese network, we applied a gated recurrent unit to capture the time features of each TF and target matrix and learn the spatial features of the matrices after merging by applying the DenseNet framework. Finally, we applied a sigmoid function to evaluate interactions. We constructed two maize sub-datasets, including gene expression levels and GRNs, using existing open-source maize multi-omics data and compared them to other GRN inference methods, including GENIE3, GRNBoost2, nonlinear ordinary differential equations, CNNC, and DGRNS. Our results show that PSGRN outperforms state-of-the-art methods. This study proposed a new framework: a PSGRN that allows GRNs to be inferred from scRNA-seq data, elucidating the temporal and spatial features of TFs and their target genes. The results show the model’s robustness and generalization, laying a theoretical foundation for maize genotype-phenotype associations with implications for breeding work."
327,Predicting disease genes based on multi-head attention fusion,"Background
The identification of disease-related genes is of great significance for the diagnosis and treatment of human disease. Most studies have focused on developing efficient and accurate computational methods to predict disease-causing genes. Due to the sparsity and complexity of biomedical data, it is still a challenge to develop an effective multi-feature fusion model to identify disease genes.
Results
This paper proposes an approach to predict the pathogenic gene based on multi-head attention fusion (MHAGP). Firstly, the heterogeneous biological information networks of disease genes are constructed by integrating multiple biomedical knowledge databases. Secondly, two graph representation learning algorithms are used to capture the feature vectors of gene-disease pairs from the network, and the features are fused by introducing multi-head attention. Finally, multi-layer perceptron model is used to predict the gene-disease association.
Conclusions
The MHAGP model outperforms all of other methods in comparative experiments. Case studies also show that MHAGP is able to predict genes potentially associated with diseases. In the future, more biological entity association data, such as gene-drug, disease phenotype-gene ontology and so on, can be added to expand the information in heterogeneous biological networks and achieve more accurate predictions. In addition, MHAGP with strong expansibility can be used for potential tasks such as gene-drug association and drug-disease association prediction."
328,Dose–response prediction for in-vitro drug combination datasets: a probabilistic approach,"In this paper we propose PIICM, a probabilistic framework for dose–response prediction in high-throughput drug combination datasets. PIICM utilizes a permutation invariant version of the intrinsic co-regionalization model for multi-output Gaussian process regression, to predict dose–response surfaces in untested drug combination experiments. Coupled with an observation model that incorporates experimental uncertainty, PIICM is able to learn from noisily observed cell-viability measurements in settings where the underlying dose–response experiments are of varying quality, utilize different experimental designs, and the resulting training dataset is sparsely observed. We show that the model can accurately predict dose–response in held out experiments, and the resulting function captures relevant features indicating synergistic interaction between drugs."
329,In-vitro validated methods for encoding digital data in deoxyribonucleic acid (DNA),"Deoxyribonucleic acid (DNA) is emerging as an alternative archival memory technology. Recent advancements in DNA synthesis and sequencing have both increased the capacity and decreased the cost of storing information in de novo synthesized DNA pools. In this survey, we review methods for translating digital data to and/or from DNA molecules. An emphasis is placed on methods which have been validated by storing and retrieving real-world data via in-vitro experiments."
330,"Schema Playground: a tool for authoring, extending, and using metadata schemas to improve FAIRness of biomedical data","Background
Biomedical researchers are strongly encouraged to make their research outputs more Findable, Accessible, Interoperable, and Reusable (FAIR). While many biomedical research outputs are more readily accessible through open data efforts, finding relevant outputs remains a significant challenge. Schema.org is a metadata vocabulary standardization project that enables web content creators to make their content more FAIR. Leveraging Schema.org could benefit biomedical research resource providers, but it can be challenging to apply Schema.org standards to biomedical research outputs. We created an online browser-based tool that empowers researchers and repository developers to utilize Schema.org or other biomedical schema projects.

Results
Our browser-based tool includes features which can help address many of the barriers towards Schema.org-compliance such as: The ability to easily browse for relevant Schema.org classes, the ability to extend and customize a class to be more suitable for biomedical research outputs, the ability to create data validation to ensure adherence of a research output to a customized class, and the ability to register a custom class to our schema registry enabling others to search and re-use it. We demonstrate the use of our tool with the creation of the Outbreak.info schema—a large multi-class schema for harmonizing various COVID-19 related resources.

Conclusions
We have created a browser-based tool to empower biomedical research resource providers to leverage Schema.org classes to make their research outputs more FAIR."
331,Validation of genetic variants from NGS data using deep convolutional neural networks,"Accurate somatic variant calling from next-generation sequencing data is one most important tasks in personalised cancer therapy. The sophistication of the available technologies is ever-increasing, yet, manual candidate refinement is still a necessary step in state-of-the-art processing pipelines. This limits reproducibility and introduces a bottleneck with respect to scalability. We demonstrate that the validation of genetic variants can be improved using a machine learning approach resting on a Convolutional Neural Network, trained using existing human annotation. In contrast to existing approaches, we introduce a way in which contextual data from sequencing tracks can be included into the automated assessment. A rigorous evaluation shows that the resulting model is robust and performs on par with trained researchers following published standard operating procedure."
332,Development of revised ResNet-50 for diabetic retinopathy detection,"Background
Diabetic retinopathy (DR) produces bleeding, exudation, and new blood vessel formation conditions. DR can damage the retinal blood vessels and cause vision loss or even blindness. If DR is detected early, ophthalmologists can use lasers to create tiny burns around the retinal tears to inhibit bleeding and prevent the formation of new blood vessels, in order to prevent deterioration of the disease. The rapid improvement of deep learning has made image recognition an effective technology; it can avoid misjudgments caused by different doctors’ evaluations and help doctors to predict the condition quickly. The aim of this paper is to adopt visualization and preprocessing in the ResNet-50 model to improve module calibration, to enable the model to predict DR accurately.
Results
This study compared the performance of the proposed method with other common CNNs models (Xception, AlexNet, VggNet-s, VggNet-16 and ResNet-50). In examining said models, the results alluded to an over-fitting phenomenon, and the outcome of the work demonstrates that the performance of the revised ResNet-50 (Train accuracy: 0.8395 and Test accuracy: 0.7432) is better than other common CNNs (that is, the revised structure of ResNet-50 could avoid the overfitting problem, decease the loss value, and reduce the fluctuation problem).

Conclusions
This study proposed two approaches to designing the DR grading system: a standard operation procedure (SOP) for preprocessing the fundus image, and a revised structure of ResNet-50, including an adaptive learning rating to adjust the weight of layers, regularization and change the structure of ResNet-50, which was selected for its suitable features. It is worth noting that the purpose of this study was not to design the most accurate DR screening network, but to demonstrate the effect of the SOP of DR and the visualization of the revised ResNet-50 model. The results provided an insight to revise the structure of CNNs using the visualization tool."
333,Integrative analysis of TP53 mutations in lung adenocarcinoma for immunotherapies and prognosis,"Background
The 
TP53
 tumor suppressor gene is one of the most mutated genes in lung adenocarcinoma (LUAD) and plays a vital role in regulating the occurrence and progression of cancer. We aimed to elucidate the association between 
TP53
 mutations, response to immunotherapies and the prognosis of LUAD.
Methods
Genomic, transcriptomic, and clinical data of LUAD were downloaded from The Cancer Genome Atlas (TCGA) dataset. Gene ontology (GO) analysis, Kyoto Encyclopedia of Genes and Genomes (KEGG) enrichment analysis, gene set enrichment analysis (GSEA). Gene set variation analysis (GSVA) were performed to determine the differences in biological pathways. A merged protein–protein interaction (PPI) network was constructed and analyzed. MSIpred was used to analyze the correlation between the expression of the 
TP53
 gene, tumor mutation burden (TMB) and tumor microsatellite instability (MSI). CIBERSORT was used to calculate the abundance of immune cells. Univariate and multivariate Cox regression analyses were used to determine the prognostic value of 
TP53
 mutations in LUAD.
Results
TP53
 was the most frequently mutated in LUAD, with a mutational frequency of 48%. GO and KEGG enrichment analysis, GSEA, and GSVA results showed a significant upregulation of several signaling pathways, including PI3K-AKT mTOR (P < 0.05), Notch (P < 0.05), E2F target (NES = 1.8, P < 0.05), and G2M checkpoint (NES = 1.7, P < 0.05). Moreover, we found a significant correlation between T cells, plasma cells, and 
TP53
 mutations (R
2
 < 0.01, P = 0.040). Univariate and multivariate Cox regression analyses revealed that the survival prognosis of LUAD patients was related to 
TP53
 mutations (Hazard Ratio (HR) = 0.72 [95% CI, 0.53 to 0.98], P < 0.05), cancer status (P < 0.05), and treatment outcomes (P < 0.05). Lastly, the Cox regression models showed that 
TP53
 exhibited good power in predicting three- and five-year survival rates.
Conclusions
TP53
 may be an independent predictor of response to immunotherapy in LUAD, and patients with 
TP53
 mutations have higher immunogenicity and immune cell infiltration."
334,Benchmarking causal reasoning algorithms for gene expression-based compound mechanism of action analysis,"Background
Elucidating compound mechanism of action (MoA) is beneficial to drug discovery, but in practice often represents a significant challenge. Causal Reasoning approaches aim to address this situation by inferring dysregulated signalling proteins using transcriptomics data and biological networks; however, a comprehensive benchmarking of such approaches has not yet been reported. Here we benchmarked four causal reasoning algorithms (SigNet, CausalR, CausalR ScanR and CARNIVAL) with four networks (the smaller Omnipath network vs. 3 larger MetaBase™ networks), using LINCS L1000 and CMap microarray data, and assessed to what extent each factor dictated the successful recovery of direct targets and compound-associated signalling pathways in a benchmark dataset comprising 269 compounds. We additionally examined impact on performance in terms of the functions and roles of protein targets and their connectivity bias in the prior knowledge networks.
Results
According to statistical analysis (negative binomial model), the combination of algorithm and network most significantly dictated the performance of causal reasoning algorithms, with the SigNet recovering the greatest number of 
direct targets
. With respect to the recovery of 
signalling pathways
, CARNIVAL with the Omnipath network was able to recover the most informative pathways containing compound targets, based on the Reactome pathway hierarchy. Additionally, CARNIVAL, SigNet and CausalR ScanR all outperformed baseline gene expression pathway enrichment results. We found no significant difference in performance between L1000 data or microarray data, even when limited to just 978 ‘landmark’ genes. Notably, all causal reasoning algorithms also outperformed pathway recovery based on input DEGs, despite these often being used for pathway enrichment. Causal reasoning methods performance was somewhat correlated with connectivity and biological role of the targets.
Conclusions
Overall, we conclude that causal reasoning performs well at recovering signalling proteins related to compound MoA upstream from gene expression changes by leveraging prior knowledge networks, and that the choice of network and algorithm has a profound impact on the performance of causal reasoning algorithms. Based on the analyses presented here this is true for both microarray-based gene expression data as well as those based on the L1000 platform."
335,PyAGH: a python package to fast construct kinship matrices based on different levels of omic data,"Background
Construction of kinship matrices among individuals is an important step for both association studies and prediction studies based on different levels of omic data. Methods for constructing kinship matrices are becoming diverse and different methods have their specific appropriate scenes. However, software that can comprehensively calculate kinship matrices for a variety of scenarios is still in an urgent demand.
Results
In this study, we developed an efficient and user-friendly python module, PyAGH, that can accomplish (1) conventional additive kinship matrces construction based on pedigree, genotypes, abundance data from transcriptome or microbiome; (2) genomic kinship matrices construction in combined population; (3) dominant and epistatic effects kinship matrices construction; (4) pedigree selection, tracing, detection and visualization; (5) visualization of cluster, heatmap and PCA analysis based on kinship matrices. The output from PyAGH can be easily integrated in other mainstream software based on users’ purposes. Compared with other softwares, PyAGH integrates multiple methods for calculating the kinship matrix and has advantages in terms of speed and data size compared to other software. PyAGH is developed in python and C +  + and can be easily installed by pip tool. Installation instructions and a manual document can be freely available from 
https://github.com/zhaow-01/PyAGH
.
Conclusion
PyAGH is a fast and user-friendly Python package for calculating kinship matrices using pedigree, genotype, microbiome and transcriptome data as well as processing, analyzing and visualizing data and results. This package makes it easier to perform predictions and association studies processes based on different levels of omic data."
336,SynBioTools: a one-stop facility for searching and selecting synthetic biology tools,"Background
The rapid development of synthetic biology relies heavily on the use of databases and computational tools, which are also developing rapidly. While many tool registries have been created to facilitate tool retrieval, sharing, and reuse, no relatively comprehensive tool registry or catalog addresses all aspects of synthetic biology.
Results
We constructed SynBioTools, a comprehensive collection of synthetic biology databases, computational tools, and experimental methods, as a one-stop facility for searching and selecting synthetic biology tools. SynBioTools includes databases, computational tools, and methods extracted from reviews via SCIentific Table Extraction, a scientific table-extraction tool that we built. Approximately 57% of the resources that we located and included in SynBioTools are not mentioned in bio.tools, the dominant tool registry. To improve users’ understanding of the tools and to enable them to make better choices, the tools are grouped into nine modules (each with subdivisions) based on their potential biosynthetic applications. Detailed comparisons of similar tools in every classification are included. The URLs, descriptions, source references, and the number of citations of the tools are also integrated into the system.
Conclusions
SynBioTools is freely available at 
https://synbiotools.lifesynther.com/
. It provides end-users and developers with a useful resource of categorized synthetic biology databases, tools, and methods to facilitate tool retrieval and selection."
337,Drug-target interaction prediction based on spatial consistency constraint and graph convolutional autoencoder,"Background
Drug-target interaction (DTI) prediction plays an important role in drug discovery and repositioning. However, most of the computational methods used for identifying relevant DTIs do not consider the invariance of the nearest neighbour relationships between drugs or targets. In other words, they do not take into account the invariance of the topological relationships between nodes during representation learning. It may limit the performance of the DTI prediction methods.
Results
Here, we propose a novel graph convolutional autoencoder-based model, named SDGAE, to predict DTIs. As the graph convolutional network cannot handle isolated nodes in a network, a pre-processing step was applied to reduce the number of isolated nodes in the heterogeneous network and facilitate effective exploitation of the graph convolutional network. By maintaining the graph structure during representation learning, the nearest neighbour relationships between nodes in the embedding space remained as close as possible to the original space.
Conclusions
Overall, we demonstrated that SDGAE can automatically learn more informative and robust feature vectors of drugs and targets, thus exhibiting significantly improved predictive accuracy for DTIs."
338,Genealyzer: web application for the analysis and comparison of gene expression data,"Background
Gene expression profiling is a widely adopted method in areas like drug development or functional gene analysis. Microarray data of gene expression experiments is still commonly used and widely available for retrospective analyses. However, due to to changes of the underlying technologies data sets from different technologies are often difficult to compare and thus a multitude of already available data becomes difficult to use. We present a web application that abstracts away mathematical and programmatical details in order to enable a convenient and customizable analysis of microarray data for large-scale reproducibility studies. In addition, the web application provides a feature that allows easy access to large microarray repositories.
Results
Our web application consists of three basic steps which are necessary for a differential gene expression analysis as well as Gene Ontology (GO) enrichment analysis and the comparison of multiple analysis results. Genealyzer can handle Affymetrix data as well as one-channel and two-channel Agilent data. All steps are visualized with meaningful plots. The application offers flexible analysis while being intuitively operable.
Conclusions
Our web application provides a unified platform for analysing microarray data, while allowing users to compare the results of different technologies and organisms. Beyond reproducibility, this also offers many possibilities for gaining further insights from existing study data, especially since data from different technologies or organisms can also be compared. The web application can be accessed via this URL: 
https://genealyzer.item.fraunhofer.de/
. Login credentials can be found at the end."
339,EZH2 as a prognostic-related biomarker in lung adenocarcinoma correlating with cell cycle and immune infiltrates,"Backgrounds
It has been observed that high levels of enhancer of zeste homolog 2 (EZH2) expression are associated with unsatisfactory prognoses and can be found in a wide range of malignancies. However, the effects of EZH2 on Lung Adenocarcinoma (LUAD) remain elusive. Through the integration of bioinformatic analyses, the present paper sought to ascertain the effects of EZH2 in LUAD.
Methods
The TIMER and UALCAN databases were applied to analyze mRNA and protein expression data for EZH2 in LUAD. The result of immunohistochemistry was obtained from the HPA database, and the survival curve was drawn according to the library provided by the HPA database. The LinkedOmics database was utilized to investigate the co-expressed genes and signal transduction pathways with EZH2. Up- and down-regulated genes from The Linked Omics database were introduced to the CMap database to predict potential drug targets for LUAD using the CMap database. The association between EZH2 and cancer-infiltrating immunocytes was studied through TIMER and TISIDB. In addition, this paper explores the relationship between EZH2 mRNA expression and NSCLC OS using the Kaplan–Meier plotter database to further validate and complement the research. Furthermore, the correlation between EZH2 expression and EGFR genes, KRAS genes, BRAF genes, and smoking from the Cancer Genome Atlas (TCGA) database is analyzed.
Results
In contrast to paracancer specimens, the mRNA and protein levels of EZH2 were higher in LUAD tissues. Significantly, high levels of EZH2 were associated with unsatisfactory prognoses in LUAD patients. Additionally, the coexpressed genes of EZH2 were predominantly associated with numerous cell growth-associated pathways, including the cell cycle, DNA replication, RNA transport, and the p53 signaling pathway, according to Gene Ontology and Kyoto Encyclopedia of Genes and Genomes pathways. The results of TCGA database revealed that the expression of EZH2 was lower in normal tissues than in lung cancer tissues (
p
 < 0.05). Smoking was associated with elevated EZH2 expression (
p
 < 0.001). EZH2 was highly expressed in lung cancers with positive KRAS expression, and the correlation was significant in lung adenocarcinoma (
r
 = 0.3129, 
p
 < 0.001). CMap was applied to determine the top 15 positively correlated drugs/molecules and the top 15 negatively correlated drugs/molecules. MK-1775, MK-5108, fenbendazole, albendazole, BAY-K8644, evodiamine, purvalanol-a, mycophenolic-acid, PHA-793887, and cyclopamine are potential drugs for patients with lung adenocarcinoma and high EZH2 expression.
Conclusions
Highly expressed EZH2 is a predictor of a suboptimal prognosis in LUAD and may serve as a prognostic marker and target gene for LUAD. The underlying cause may be associated with the synergistic effect of KRAS, immune cell infiltration, and metabolic processes."
340,Application of sequence semantic and integrated cellular geography approach to study alternative biogenesis of exonic circular RNA,"Background
Concurrent existence of lncRNA and circular RNA at both nucleus and cytosol within a cell at different proportions is well reported. Previous studies showed that circular RNAs are synthesized in nucleus followed by transportation across the nuclear membrane and the export is primarily defined by their length. lncRNAs primarily originated through inefficient splicing and seem to use NXF1 for cytoplasm export. However, it is not clear whether circularization of lncRNA happens only in nucleus or it also occurs in cytoplasm. Studies indicate that circular RNAs arise when the splicing apparatus undergoes a phenomenon of back splicing. Minor spliceosome (U12 type) mediated splicing occurs in cytoplasm and is responsible for the splicing of 0.5% of introns of human cells. Therefore, possibility of cRNA biogenesis mediated by minor spliceosome at cytoplasm cannot be ruled out. Secondly, information on genes transcribing both circular and lncRNAs along with total number of RBP binding sites for both of these RNA types is extractable from databases. This study showed how these apparently unconnected pieces of reports could be put together to build a model for exploring biogenesis of circular RNA.
Results
As a result of this study, a model was built under the premises that, sequences with special semantics were molecular precursors in biogenesis of circular RNA which occurred through catalytic role of some specific RBPs. The model outcome was further strengthened by fulfillment of three logical lemmas which were extracted and assimilated in this work using a novel data analytic approach, Integrated Cellular Geography. Result of the study was found to be in well agreement with proposed model. Furthermore this study also indicated that biogenesis of circular RNA was a post-transcriptional event.
Conclusions
Overall, this study provides a novel systems biology based model under the paradigm of Integrated Cellular Geography which can assimilate independently performed experimental results and data published by global researchers on RNA biology to provide important information on biogenesis of circular RNAs considering lncRNAs as precursor molecule. This study also suggests the possible RBP-mediated circularization of RNA in the cytoplasm through back-splicing using minor spliceosome."
341,Construction of an RNA modification-related gene predictive model associated with prognosis and immunity in gastric cancer,"Background
Gastric cancer (GC) is one of the most common causes of cancer-related fatalities worldwide, and its progression is associated with RNA modifications. Here, using RNA modification-related genes (RNAMRGs), we aimed to construct a prognostic model for patients with GC.
Methods
Based on RNAMRGs, RNA modification scores (RNAMSs) were obtained for GC samples from The Cancer Genome Atlas and were divided into high- and low-RNAMS groups. Differential analysis and weighted correlation network analysis were performed for the differential expressed genes (DEGs) to obtain the key genes. Next, univariate Cox regression, least absolute shrinkage and selection operator, and multivariate Cox regression analyses were performed to obtain the model. According to the model risk score, samples were divided into high- and low-risk groups. Enrichment analysis and immunoassays were performed for the DEGs in these groups. Four external datasets from Gene Expression Omnibus data base were used to test the accuracy of the predictive model.
Results
We identified 
SELP
 and 
CST2
 as key DEGs, which were used to generate the predictive model. The high-risk group had a worse prognosis compared to the low-risk group (
p
 < 0.05). Enrichment analysis and immunoassays revealed that 144 DEGs related to immune cell infiltration were associated with the Wnt signaling pathway and included hub genes such as 
ELN
. Overall mutation levels, tumor mutation burden, and microsatellite instability were lower, but tumor immune dysfunction and exclusion scores were greater (
p
 < 0.05) in the high-risk group than in the low-risk group. The validation results showed that the prediction model score can accurately predict the prognosis of GC patients. Finally, a nomogram was constructed using the risk score combined with the clinicopathological characteristics of patients with GC.
Conclusion
This risk score from the prediction model related to the tumor microenvironment and immunotherapy could accurately predict the overall survival of GC patients."
342,Development and validation of a deep learning survival model for cervical adenocarcinoma patients,"Background
The aim was to develop a personalized survival prediction deep learning model for cervical adenocarcinoma patients and process personalized survival prediction.
Methods
A total of 2501 cervical adenocarcinoma patients from the surveillance, epidemiology and end results database and 220 patients from Qilu hospital were enrolled in this study. We created our deep learning (DL) model to manipulate the data and evaluated its performance against four other competitive models. We tried to demonstrate a new grouping system oriented by survival outcomes and process personalized survival prediction by using our DL model.
Results
The DL model reached 0.878 c-index and 0.09 Brier score in the test set, which was better than the other four models. In the external test set, our model achieved a 0.80 c-index and 0.13 Brier score. Thus, we developed prognosis-oriented risk grouping for patients according to risk scores computed by our DL model. Notable differences among groupings were observed. In addition, a personalized survival prediction system based on our risk-scoring grouping was developed.
Conclusions
We developed a deep neural network model for cervical adenocarcinoma patients. The performance of this model proved to be superior to other models. The results of external validation supported the possibility that the model can be used in clinical work. Finally, our survival grouping and personalized prediction system provided more accurate prognostic information for patients than traditional FIGO stages."
343,"nRCFV: a new, dataset-size-independent metric to quantify compositional heterogeneity in nucleotide and amino acid datasets","Motivation
Compositional heterogeneity—when the proportions of nucleotides and amino acids are not broadly similar across the dataset—is a cause of a great number of phylogenetic artefacts. Whilst a variety of methods can identify it post-hoc, few metrics exist to quantify compositional heterogeneity prior to the computationally intensive task of phylogenetic tree reconstruction. Here we assess the efficacy of one such existing, widely used, metric: Relative Composition Frequency Variability (RCFV), using both real and simulated data.
Results
Our results show that RCFV can be biased by sequence length, the number of taxa, and the number of possible character states within the dataset. However, we also find that missing data does not appear to have an appreciable effect on RCFV. We discuss the theory behind this, the consequences of this for the future of the usage of the RCFV value and propose a new metric, nRCFV, which accounts for these biases. Alongside this, we present a new software that calculates both RCFV and nRCFV, called nRCFV_Reader.
Availability and implementation
nRCFV has been implemented in RCFV_Reader, available at: 
https://github.com/JFFleming/RCFV_Reader
. Both our simulation and real data are available at Datadryad: 
https://doi.org/10.5061/dryad.wpzgmsbpn
."
344,Automatic extraction of ranked SNP-phenotype associations from text using a BERT-LSTM-based method,"Extraction of associations of singular nucleotide polymorphism (SNP) and phenotypes from biomedical literature is a vital task in BioNLP. Recently, some methods have been developed to extract mutation-diseases affiliations. However, no accessible method of extracting associations of SNP-phenotype from content considers their degree of certainty. In this paper, several machine learning methods were developed to extract ranked SNP-phenotype associations from biomedical abstracts and then were compared to each other. In addition, shallow machine learning methods, including random forest, logistic regression, and decision tree and two kernel-based methods like subtree and local context, a rule-based and a deep CNN-LSTM-based and two BERT-based methods were developed in this study to extract associations. Furthermore, the experiments indicated that although the used linguist features could be employed to implement a superior association extraction method outperforming the kernel-based counterparts, the used deep learning and BERT-based methods exhibited the best performance. However, the used PubMedBERT-LSTM outperformed the other developed methods among the used methods. Moreover, similar experiments were conducted to estimate the degree of certainty of the extracted association, which can be used to assess the strength of the reported association. The experiments revealed that our proposed PubMedBERT–CNN-LSTM method outperformed the sophisticated methods on the task."
345,lifex-fiber: an open tool for myofibers generation in cardiac computational models,"Background
Modeling the whole cardiac function involves the solution of several complex multi-physics and multi-scale models that are highly computationally demanding, which call for simpler yet accurate, high-performance computational tools. Despite the efforts made by several research groups, no software for whole-heart fully-coupled cardiac simulations in the scientific community has reached full maturity yet.
Results
In this work we present 
\(\texttt {life}^{\texttt {x}}\)
-fiber, an innovative tool for the generation of myocardial fibers based on Laplace-Dirichlet Rule-Based Methods, which are the essential building blocks for modeling the electrophysiological, mechanical and electromechanical cardiac function, from single-chamber to whole-heart simulations. 
\(\texttt {life}^{\texttt {x}}\)
-fiber is the first publicly released module for cardiac simulations based on 
\(\texttt {life}^{\texttt {x}}\)
, an open-source, high-performance Finite Element solver for multi-physics, multi-scale and multi-domain problems developed in the framework of the iHEART project, which aims at making 
in silico
 experiments easily reproducible and accessible to a wide community of users, including those with a background in medicine or bio-engineering.
Conclusions
The tool presented in this document is intended to provide the scientific community with a computational tool that incorporates general state of the art models and solvers for simulating the cardiac function within a high-performance framework that exposes a user- and developer-friendly interface. This report comes with an extensive technical and mathematical documentation to welcome new users to the core structure of 
\(\texttt {life}^{\texttt {x}}\)
-fiber and to provide them with a possible approach to include the generated cardiac fibers into more sophisticated computational pipelines. In the near future, more modules will be successively published either as pre-compiled binaries for 
x86-64 Linux
 systems or as open source software."
346,Single-nucleus gene and gene set expression-based similarity network fusion identifies autism molecular subtypes,"Background
Autism spectrum disorder (ASD) is a complex neurodevelopmental disorder that is highly phenotypically and genetically heterogeneous. With the accumulation of biological sequencing data, more and more studies shift to molecular subtype-first approach, from identifying molecular subtypes based on genetic and molecular data to linking molecular subtypes with clinical manifestation, which can reduce heterogeneity before phenotypic profiling.
Results
In this study, we perform similarity network fusion to integrate gene and gene set expression data of multiple human brain cell types for ASD molecular subtype identification. Then we apply subtype-specific differential gene and gene set expression analyses to study expression patterns specific to molecular subtypes in each cell type. To demonstrate the biological and practical significance, we analyze the molecular subtypes, investigate their correlation with ASD clinical phenotype, and construct ASD molecular subtype prediction models.
Conclusions
The identified molecular subtype-specific gene and gene set expression may be used to differentiate ASD molecular subtypes, facilitating the diagnosis and treatment of ASD. Our method provides an analytical pipeline for the identification of molecular subtypes and even disease subtypes of complex disorders."
347,iIL13Pred: improved prediction of IL-13 inducing peptides using popular machine learning classifiers,"Background
Inflammatory mediators play havoc in several diseases including the novel Coronavirus disease 2019 (COVID-19) and generally correlate with the severity of the disease. Interleukin-13 (IL-13), is a pleiotropic cytokine that is known to be associated with airway inflammation in asthma and reactive airway diseases, in neoplastic and autoimmune diseases. Interestingly, the recent association of IL-13 with COVID-19 severity has sparked interest in this cytokine. Therefore characterization of new molecules which can regulate IL-13 induction might lead to novel therapeutics.

Results
Here, we present an improved prediction of IL-13-inducing peptides. The positive and negative datasets were obtained from a recent study (IL13Pred) and the Pfeature algorithm was used to compute features for the peptides. As compared to the state-of-the-art which used the regularization based feature selection technique (linear support vector classifier with the L1 penalty), we used a multivariate feature selection technique (minimum redundancy maximum relevance) to obtain non-redundant and highly relevant features. In the proposed study (improved IL-13 prediction (iIL13Pred)), the use of the mRMR feature selection method is instrumental in choosing the most discriminatory features of IL-13-inducing peptides with improved performance. We investigated seven common machine learning classifiers including Decision Tree, Gaussian Naïve Bayes, k-Nearest Neighbour, Logistic Regression, Support Vector Machine, Random Forest, and extreme gradient boosting to efficiently classify IL-13-inducing peptides. We report improved AUC, and MCC scores of 0.83 and 0.33 on validation data as compared to the current method.
Conclusions
Extensive benchmarking experiments suggest that the proposed method (iIL13Pred) could provide improved performance metrics in terms of sensitivity, specificity, accuracy, the area under the curve - receiver operating characteristics (AUCROC) and Matthews correlation coefficient (MCC) than the existing state-of-the-art approach (IL13Pred) on the validation dataset and an external dataset comprising of experimentally validated IL-13-inducing peptides. Additionally, the experiments were performed with an increased number of experimentally validated training datasets to obtain a more robust model. A user-friendly web server (
www.soodlab.com/iil13pred
) is also designed to facilitate rapid screening of IL-13-inducing peptides."
348,A voting-based machine learning approach for classifying biological and clinical datasets,"Background
Different machine learning techniques have been proposed to classify a wide range of biological/clinical data. Given the practicability of these approaches accordingly, various software packages have been also designed and developed. However, the existing methods suffer from several limitations such as overfitting on a specific dataset, ignoring the feature selection concept in the preprocessing step, and losing their performance on large-size datasets. To tackle the mentioned restrictions, in this study, we introduced a machine learning framework consisting of two main steps. First, our previously suggested optimization algorithm (
Trader
) was extended to select a near-optimal subset of features/genes. Second, a voting-based framework was proposed to classify the biological/clinical data with high accuracy. To evaluate the efficiency of the proposed method, it was applied to 13 biological/clinical datasets, and the outcomes were comprehensively compared with the prior methods.
Results
The results demonstrated that the 
Trader
 algorithm could select a near-optimal subset of features with a significant level of p-value < 0.01 relative to the compared algorithms. Additionally, on the large-sie datasets, the proposed machine learning framework improved prior studies by ~ 10% in terms of the mean values associated with fivefold cross-validation of accuracy, precision, recall, specificity, and F-measure.
Conclusion
Based on the obtained results, it can be concluded that a proper configuration of efficient algorithms and methods can increase the prediction power of machine learning approaches and help researchers in designing practical diagnosis health care systems and offering effective treatment plans."
349,Genetic algorithm-based feature selection with manifold learning for cancer classification using microarray data,"Background
Microarray data have been widely utilized for cancer classification. The main characteristic of microarray data is “large p and small n” in that data contain a small number of subjects but a large number of genes. It may affect the validity of the classification. Thus, there is a pressing demand of techniques able to select genes relevant to cancer classification.
Results
This study proposed a novel feature (gene) selection method, Iso-GA, for cancer classification. Iso-GA hybrids the manifold learning algorithm, Isomap, in the genetic algorithm (GA) to account for the latent nonlinear structure of the gene expression in the microarray data. The Davies–Bouldin index is adopted to evaluate the candidate solutions in Isomap and to avoid the classifier dependency problem. Additionally, a probability-based framework is introduced to reduce the possibility of genes being randomly selected by GA. The performance of Iso-GA was evaluated on eight benchmark microarray datasets of cancers. Iso-GA outperformed other benchmarking gene selection methods, leading to good classification accuracy with fewer critical genes selected.
Conclusions
The proposed Iso-GA method can effectively select fewer but critical genes from microarray data to achieve competitive classification performance."
350,Automatic block-wise genotype-phenotype association detection based on hidden Markov model,"Background
For detecting genotype-phenotype association from case–control single nucleotide polymorphism (SNP) data, one class of methods relies on testing each genomic variant site individually. However, this approach ignores the tendency for associated variant sites to be spatially clustered instead of uniformly distributed along the genome. Therefore, a more recent class of methods looks for blocks of influential variant sites. Unfortunately, existing such methods either assume prior knowledge of the blocks, or rely on ad hoc moving windows. A principled method is needed to automatically detect genomic variant blocks which are associated with the phenotype.
Results
In this paper, we introduce an automatic block-wise Genome-Wide Association Study (GWAS) method based on Hidden Markov model. Using case–control SNP data as input, our method detects the number of blocks associated with the phenotype and the locations of the blocks. Correspondingly, the minor allele of each variate site will be classified as having negative influence, no influence or positive influence on the phenotype. We evaluated our method using both datasets simulated from our model and datasets from a block model different from ours, and compared the performance with other methods. These included both simple methods based on the Fisher’s exact test, applied site-by-site, as well as more complex methods built into the recent Zoom-Focus Algorithm. Across all simulations, our method consistently outperformed the comparisons.
Conclusions
With its demonstrated better performance, we expect our algorithm for detecting influential variant sites may help find more accurate signals across a wide range of case–control GWAS."
351,Stack-VTP: prediction of vesicle transport proteins based on stacked ensemble classifier and evolutionary information,"Vesicle transport proteins not only play an important role in the transmembrane transport of molecules, but also have a place in the field of biomedicine, so the identification of vesicle transport proteins is particularly important. We propose a method based on ensemble learning and evolutionary information to identify vesicle transport proteins. Firstly, we preprocess the imbalanced dataset by random undersampling. Secondly, we extract position-specific scoring matrix (PSSM) from protein sequences, and then further extract AADP-PSSM and RPSSM features from PSSM, and use the Max-Relevance-Max-Distance (MRMD) algorithm to select the optimal feature subset. Finally, the optimal feature subset is fed into the stacked classifier for vesicle transport proteins identification. The experimental results show that the of accuracy (ACC), sensitivity (SN) and specificity (SP) of our method on the independent testing set are 82.53%, 0.774 and 0.836, respectively. The SN, SP and ACC of our proposed method are 0.013, 0.007 and 0.76% higher than the current state-of-the-art methods."
352,Using sensitivity analyses to understand bistable system behavior,"Background
Bistable systems, i.e., systems that exhibit two stable steady states, are of particular interest in biology. They can implement binary cellular decision making, e.g., in pathways for cellular differentiation and cell cycle regulation. The onset of cancer, prion diseases, and neurodegenerative diseases are known to be associated with malfunctioning bistable systems. Exploring and characterizing parameter spaces in bistable systems, so that they retain or lose bistability, is part of a lot of therapeutic research such as cancer pharmacology.
Results
We use eigenvalue sensitivity analysis and stable state separation sensitivity analysis to understand bistable system behaviors, and to characterize the most sensitive parameters of a bistable system. While eigenvalue sensitivity analysis is an established technique in engineering disciplines, it has not been frequently used to study biological systems. We demonstrate the utility of these approaches on a published bistable system. We also illustrate scalability and generalizability of these methods to larger bistable systems.
Conclusions
Eigenvalue sensitivity analysis and separation sensitivity analysis prove to be promising tools to define parameter design rules to make switching decisions between either stable steady state of a bistable system and a corresponding monostable state after bifurcation. These rules were applied to the smallest two-component bistable system and results were validated analytically. We showed that with multiple parameter settings of the same bistable system, we can design switching to a desirable state to retain or lose bistability when the most sensitive parameter is varied according to our parameter perturbation recommendations. We propose eigenvalue and stable state separation sensitivity analyses as a framework to evaluate large and complex bistable systems."
353,PSReliP: an integrated pipeline for analysis and visualization of population structure and relatedness based on genome-wide genetic variant data,"Background
Population structure and cryptic relatedness between individuals (samples) are two major factors affecting false positives in genome-wide association studies (GWAS). In addition, population stratification and genetic relatedness in genomic selection in animal and plant breeding can affect prediction accuracy. The methods commonly used for solving these problems are principal component analysis (to adjust for population stratification) and marker-based kinship estimates (to correct for the confounding effects of genetic relatedness). Currently, many tools and software are available that analyze genetic variation among individuals to determine population structure and genetic relationships. However, none of these tools or pipelines perform such analyses in a single workflow and visualize all the various results in a single interactive web application.
Results
We developed PSReliP, a standalone, freely available pipeline for the analysis and visualization of population structure and relatedness between individuals in a user-specified genetic variant dataset. The analysis stage of PSReliP is responsible for executing all steps of data filtering and analysis and contains an ordered sequence of commands from PLINK, a whole-genome association analysis toolset, along with in-house shell scripts and Perl programs that support data pipelining. The visualization stage is provided by Shiny apps, an R-based interactive web application. In this study, we describe the characteristics and features of PSReliP and demonstrate how it can be applied to real genome-wide genetic variant data.
Conclusions
The PSReliP pipeline allows users to quickly analyze genetic variants such as single nucleotide polymorphisms and small insertions or deletions at the genome level to estimate population structure and cryptic relatedness using PLINK software and to visualize the analysis results in interactive tables, plots, and charts using Shiny technology. The analysis and assessment of population stratification and genetic relatedness can aid in choosing an appropriate approach for the statistical analysis of GWAS data and predictions in genomic selection. The various outputs from PLINK can be used for further downstream analysis. The code and manual for PSReliP are available at 
https://github.com/solelena/PSReliP
."
354,"clusterMaker2:
 a major update to 
clusterMaker
, a multi-algorithm clustering app for Cytoscape","Background
Since the initial publication of 
clusterMaker
, the need for tools to analyze large biological datasets has only increased. New datasets are significantly larger than a decade ago, and new experimental techniques such as single-cell transcriptomics continue to drive the need for clustering or classification techniques to focus on portions of datasets of interest. While many libraries and packages exist that implement various algorithms, there remains the need for clustering packages that are easy to use, integrated with visualization of the results, and integrated with other commonly used tools for biological data analysis. 
clusterMaker2
 has added several new algorithms, including two entirely new categories of analyses: node ranking and dimensionality reduction. Furthermore, many of the new algorithms have been implemented using the Cytoscape 
jobs
 API, which provides a mechanism for executing remote jobs from within Cytoscape. Together, these advances facilitate meaningful analyses of modern biological datasets despite their ever-increasing size and complexity.

Results
The use of 
clusterMaker2
 is exemplified by reanalyzing the yeast heat shock expression experiment that was included in our original paper; however, here we explored this dataset in significantly more detail. Combining this dataset with the yeast protein–protein interaction network from STRING, we were able to perform a variety of analyses and visualizations from within 
clusterMaker2
, including Leiden clustering to break the entire network into smaller clusters, hierarchical clustering to look at the overall expression dataset, dimensionality reduction using UMAP to find correlations between our hierarchical visualization and the UMAP plot, fuzzy clustering, and cluster ranking. Using these techniques, we were able to explore the highest-ranking cluster and determine that it represents a strong contender for proteins working together in response to heat shock. We found a series of clusters that, when re-explored as fuzzy clusters, provide a better presentation of mitochondrial processes.
Conclusions
clusterMaker2
 represents a significant advance over the previously published version, and most importantly, provides an easy-to-use tool to perform clustering and to visualize clusters within the Cytoscape network context. The new algorithms should be welcome to the large population of Cytoscape users, particularly the new dimensionality reduction and fuzzy clustering techniques."
355,transXpress: a Snakemake pipeline for streamlined de novo transcriptome assembly and annotation,"Background
RNA-seq followed by de novo transcriptome assembly has been a transformative technique in biological research of non-model organisms, but the computational processing of RNA-seq data entails many different software tools. The complexity of these de novo transcriptomics workflows therefore presents a major barrier for researchers to adopt best-practice methods and up-to-date versions of software.
Results
Here we present a streamlined and universal de novo transcriptome assembly and annotation pipeline, transXpress, implemented in Snakemake. transXpress supports two popular assembly programs, Trinity and rnaSPAdes, and allows parallel execution on heterogeneous cluster computing hardware.
Conclusions
transXpress simplifies the use of best-practice methods and up-to-date software for de novo transcriptome assembly, and produces standardized output files that can be mined using SequenceServer to facilitate rapid discovery of new genes and proteins in non-model organisms."
356,ICOR: improving codon optimization with recurrent neural networks,"Background
In protein sequences—as there are 61 sense codons but only 20 standard amino acids—most amino acids are encoded by more than one codon. Although such synonymous codons do not alter the encoded amino acid sequence, their selection can dramatically affect the expression of the resulting protein. Codon optimization of synthetic DNA sequences is important for heterologous expression. However, existing solutions are primarily based on choosing high-frequency codons only, neglecting the important effects of rare codons. In this paper, we propose a novel recurrent-neural-network based codon optimization tool, ICOR, that aims to learn codon usage bias on a genomic dataset of 
Escherichia coli
. We compile a dataset of over 7,000 non-redundant, high-expression, robust genes which are used for deep learning. The model uses a bidirectional long short-term memory-based architecture, allowing for the sequential context of codon usage in genes to be learned. Our tool can predict synonymous codons for synthetic genes toward optimal expression in 
Escherichia coli
.
Results
We demonstrate that sequential context achieved via RNN may yield codon selection that is more similar to the host genome. Based on computational metrics that predict protein expression, ICOR theoretically optimizes protein expression more than frequency-based approaches. ICOR is evaluated on 1,481 
Escherichia coli
 genes as well as a benchmark set of 40 select DNA sequences whose heterologous expression has been previously characterized. ICOR’s performance is measured across five metrics: the Codon Adaptation Index, GC-content, negative repeat elements, negative cis-regulatory elements, and codon frequency distribution.
Conclusions
The results, based on in silico metrics, indicate that ICOR codon optimization is theoretically more effective in enhancing recombinant expression of proteins over other established codon optimization techniques. Our tool is provided as an open-source software package that includes the benchmark set of sequences used in this study."
357,DNAfusion: an R/Bioconductor package for increased sensitivity of detecting gene fusions in liquid biopsies,"Background
EML4-ALK gene fusions are oncogenic drivers in non-small cell lung cancer (NSCLC), and liquid biopsies containing 
EML4-ALK
 fragments can be used to study tumor dynamics using next-generation sequencing (NGS). However, the sensitivity of 
EML4-ALK
 detection varies between pipelines and analysis tools.

Results
We developed an R/Bioconductor package, DNAfusion, which can be applied to BAM files generated by commercially available NGS pipelines, such as AVENIO. Forty-eight blood samples from a training cohort consisting of 41 stage IV 
EML4-ALK
-positive NSCLC patients and seven healthy controls were used to develop DNAfusion. DNAfusion detected 
EML4-ALK
 in significantly more samples (sensitivity = 61.0%) compared to AVENIO (sensitivity = 36.6%). The newly identified 
EML4-ALK
-positive patients were verified using droplet digital PCR. DNAfusion was subsequently validated in a blinded validation cohort comprising 24 
EML4-ALK
-positive and 24 
EML4-ALK
-negative stage IV NSCLC patients. DNAfusion detected significantly more 
EML4-ALK
 individuals in the validation cohort (sensitivity = 62.5%) compared to AVENIO (sensitivity = 29.2%). DNAfusion demonstrated a specificity of 100% in both the training and validation cohorts.
Conclusion
Here we present DNAfusion, which increases the sensitivity of 
EML4-ALK
 detection in liquid biopsies and can be implemented downstream of commercially available NGS pipelines. The simplistic method of operating the R package makes it easy to implement in the clinical setting, enabling wider expansion of NGS-based diagnostics."
358,A two-stage hybrid biomarker selection method based on ensemble filter and binary differential evolution incorporating binary African vultures optimization,"Background
In the field of genomics and personalized medicine, it is a key issue to find biomarkers directly related to the diagnosis of specific diseases from high-throughput gene microarray data. Feature selection technology can discover biomarkers with disease classification information.
Results
We use support vector machines as classifiers and use the five-fold cross-validation average classification accuracy, recall, precision and F1 score as evaluation metrics to evaluate the identified biomarkers. Experimental results show classification accuracy above 0.93, recall above 0.92, precision above 0.91, and F1 score above 0.94 on eight microarray datasets.
Method
This paper proposes a two-stage hybrid biomarker selection method based on ensemble filter and binary differential evolution incorporating binary African vultures optimization (EF-BDBA), which can effectively reduce the dimension of microarray data and obtain optimal biomarkers. In the first stage, we propose an ensemble filter feature selection method. The method combines an improved fast correlation-based filter algorithm with Fisher score. obviously redundant and irrelevant features can be filtered out to initially reduce the dimensionality of the microarray data. In the second stage, the optimal feature subset is selected using an improved binary differential evolution incorporating an improved binary African vultures optimization algorithm. The African vultures optimization algorithm has excellent global optimization ability. It has not been systematically applied to feature selection problems, especially for gene microarray data. We combine it with a differential evolution algorithm to improve population diversity.
Conclusion
Compared with traditional feature selection methods and advanced hybrid methods, the proposed method achieves higher classification accuracy and identifies excellent biomarkers while retaining fewer features. The experimental results demonstrate the effectiveness and advancement of our proposed algorithmic model."
359,Prediction of hot spots in protein–DNA binding interfaces based on discrete wavelet transform and wavelet packet transform,"Background
Identification of hot spots in protein–DNA binding interfaces is extremely important for understanding the underlying mechanisms of protein–DNA interactions and drug design. Since experimental methods for identifying hot spots are time-consuming and expensive, and most of the existing computational methods are based on traditional protein–DNA features to predict hot spots, unable to make full use of the effective information in the features.
Results
In this work, a method named WTL-PDH is proposed for hot spots prediction. To deal with the unbalanced dataset, we used the Synthetic Minority Over-sampling Technique to generate minority class samples to achieve the balance of dataset. First, we extracted the solvent accessible surface area features and structural features, and then processed the traditional features using discrete wavelet transform and wavelet packet transform to extract the wavelet energy information and wavelet entropy information, and obtained a total of 175 dimensional features. In order to obtain the best feature subset, we systematically evaluate these features in various feature selection strategies. Finally, light gradient boosting machine (LightGBM) was used to establish the model.
Conclusions
Our method achieved good results on independent test set with AUC, MCC and F1 scores of 0.838, 0.533 and 0.750, respectively. WTL-PDH can achieve generally better performance in predicting hot spots when compared with state-of-the-art methods. The dataset and source code are available at 
https://github.com/chase2555/WTL-PDH
."
360,"Shine: A novel strategy to extract specific, sensitive and well-conserved biomarkers from massive microbial genomic datasets","Background
Concentrations of the pathogenic microorganisms’ DNA in biological samples are typically low. Therefore, DNA diagnostics of common infections are costly, rarely accurate, and challenging. Limited by failing to cover updated epidemic testing samples, computational services are difficult to implement in clinical applications without complex customized settings. Furthermore, the combined biomarkers used to maintain high conservation may not be cost effective and could cause several experimental errors in many clinical settings. Given the limitations of recent developed technology, 16S rRNA is too conserved to distinguish closely related species, and mosaic plasmids are not effective as well because of their uneven distribution across prokaryotic taxa.
Results
Here, we provide a computational strategy, Shine, that allows extraction of specific, sensitive and well-conserved biomarkers from massive microbial genomic datasets. Distinguished with simple concatenations with blast-based filtering, our method involves a de novo genome alignment-based pipeline to explore the original and specific repetitive biomarkers in the defined population. It can cover all members to detect newly discovered multicopy conserved species-specific or even subspecies-specific target probes and primer sets. The method has been successfully applied to a number of clinical projects and has the overwhelming advantages of automated detection of all pathogenic microorganisms without the limitations of genome annotation and incompletely assembled motifs. Using on our pipeline, users may select different configuration parameters depending on the purpose of the project for routine clinical detection practices on the website 
https://bioinfo.liferiver.com.cn
 with easy registration.
Conclusions
The proposed strategy is suitable for identifying shared phylogenetic markers while featuring low rates of false positive or false negative. This technology is suitable for the automatic design of minimal and efficient PCR primers and other types of detection probes."
361,Kernelized multiview signed graph learning for single-cell RNA sequencing data,"Background
Characterizing the topology of gene regulatory networks (GRNs) is a fundamental problem in systems biology. The advent of single cell technologies has made it possible to construct GRNs at finer resolutions than bulk and microarray datasets. However, cellular heterogeneity and sparsity of the single cell datasets render void the application of regular Gaussian assumptions for constructing GRNs. Additionally, most GRN reconstruction approaches estimate a single network for the entire data. This could cause potential loss of information when single cell datasets are generated from multiple treatment conditions/disease states.
Results
To better characterize single cell GRNs under different but related conditions, we propose the joint estimation of multiple networks using multiple signed graph learning (scMSGL). The proposed method is based on recently developed graph signal processing (GSP) based graph learning, where GRNs and gene expressions are modeled as signed graphs and graph signals, respectively. scMSGL learns multiple GRNs by optimizing the total variation of gene expressions with respect to GRNs while ensuring that the learned GRNs are similar to each other through regularization with respect to a learned signed consensus graph. We further kernelize scMSGL with the kernel selected to suit the structure of single cell data.
Conclusions
scMSGL is shown to have superior performance over existing state of the art methods in GRN recovery on simulated datasets. Furthermore, scMSGL successfully identifies well-established regulators in a mouse embryonic stem cell differentiation study and a cancer clinical study of medulloblastoma."
362,Automatic disease prediction from human gut metagenomic data using boosting GraphSAGE,"Background
The human microbiome plays a critical role in maintaining human health. Due to the recent advances in high-throughput sequencing technologies, the microbiome profiles present in the human body have become publicly available. Hence, many works have been done to analyze human microbiome profiles. These works have identified that different microbiome profiles are present in healthy and sick individuals for different diseases. Recently, several computational methods have utilized the microbiome profiles to automatically diagnose and classify the host phenotype.
Results
In this work, a novel deep learning framework based on boosting GraphSAGE is proposed for automatic prediction of diseases from metagenomic data. The proposed framework has two main components, (a). Metagenomic Disease graph (MD-graph) construction module, (b). Disease prediction Network (DP-Net) module. The graph construction module constructs a graph by considering each metagenomic sample as a node in the graph. The graph captures the relationship between the samples using a proximity measure. The DP-Net consists of a boosting GraphSAGE model which predicts the status of a sample as sick or healthy. The effectiveness of the proposed method is verified using real and synthetic datasets corresponding to diseases like inflammatory bowel disease and colorectal cancer. The proposed model achieved a highest AUC of 93%, Accuracy of 95%, F1-score of 95%, AUPRC of 95% for the real inflammatory bowel disease dataset and a best AUC of 90%, Accuracy of 91%, F1-score of 87% and AUPRC of 93% for the real colorectal cancer dataset.
Conclusion
The proposed framework outperforms other machine learning and deep learning models in terms of classification accuracy, AUC, F1-score and AUPRC for both synthetic and real metagenomic data."
363,Sparse clusterability: testing for cluster structure in high dimensions,"Background
Cluster analysis is utilized frequently in scientific theory and applications to separate data into groups. A key assumption in many clustering algorithms is that the data was generated from a population consisting of multiple distinct clusters. Clusterability testing allows users to question the inherent assumption of latent cluster structure, a theoretical requirement for meaningful results in cluster analysis.

Results
This paper proposes methods for clusterability testing designed for high-dimensional data by utilizing sparse principal component analysis. Type I error and power of the clusterability tests are evaluated using simulated data with different types of cluster structure in high dimensions. Empirical performance of the new methods is evaluated and compared with prior methods on gene expression, microarray, and shotgun proteomics data. Our methods had reasonably low Type I error and maintained power for many datasets with a variety of structures and dimensions. Cluster structure was not detectable in other datasets with spatially close clusters.
Conclusion
This is the first analysis of clusterability testing on both simulated and real-world high-dimensional data."
364,CellTrackVis: interactive browser-based visualization for analyzing cell trajectories and lineages,"Background
Automatic cell tracking methods enable practitioners to analyze cell behaviors efficiently. Notwithstanding the continuous development of relevant software, user-friendly visualization tools have room for further improvements. Typical visualization mostly comes with main cell tracking tools as a simple plug-in, or relies on specific software/platforms. Although some tools are standalone, limited visual interactivity is provided, or otherwise cell tracking outputs are partially visualized.
Results
This paper proposes a self-reliant visualization system, CellTrackVis, to support quick and easy analysis of cell behaviors. Interconnected views help users discover meaningful patterns of cell motions and divisions in common web browsers. Specifically, cell trajectory, lineage, and quantified information are respectively visualized in a coordinated interface. In particular, immediate interactions among modules enable the study of cell tracking outputs to be more effective, and also each component is highly customizable for various biological tasks.
Conclusions
CellTrackVis is a standalone browser-based visualization tool. Source codes and data sets are freely available at 
http://github.com/scbeom/celltrackvis
 with the tutorial at 
http://scbeom.github.io/ctv_tutorial
."
365,REDfold: accurate RNA secondary structure prediction using residual encoder-decoder network,"Background
As the RNA secondary structure is highly related to its stability and functions, the structure prediction is of great value to biological research. The traditional computational prediction for RNA secondary prediction is mainly based on the thermodynamic model with dynamic programming to find the optimal structure. However, the prediction performance based on the traditional approach is unsatisfactory for further research. Besides, the computational complexity of the structure prediction using dynamic programming is 
\(O(N^3)\)
; it becomes 
\(O(N^6)\)
 for RNA structure with pseudoknots, which is computationally impractical for large-scale analysis.
Results
In this paper, we propose REDfold, a novel deep learning-based method for RNA secondary prediction. REDfold utilizes an encoder-decoder network based on CNN to learn the short and long range dependencies among the RNA sequence, and the network is further integrated with symmetric skip connections to efficiently propagate activation information across layers. Moreover, the network output is post-processed with constrained optimization to yield favorable predictions even for RNAs with pseudoknots. Experimental results based on the ncRNA database demonstrate that REDfold achieves better performance in terms of efficiency and accuracy, outperforming the contemporary state-of-the-art methods."
366,GVC: efficient random access compression for gene sequence variations,"Background
In recent years, advances in high-throughput sequencing technologies have enabled the use of genomic information in many fields, such as precision medicine, oncology, and food quality control. The amount of genomic data being generated is growing rapidly and is expected to soon surpass the amount of video data. The majority of sequencing experiments, such as genome-wide association studies, have the goal of identifying variations in the gene sequence to better understand phenotypic variations. We present a novel approach for compressing gene sequence variations with random access capability: the Genomic Variant Codec (GVC). We use techniques such as binarization, joint row- and column-wise sorting of blocks of variations, as well as the image compression standard JBIG for efficient entropy coding.
Results
Our results show that GVC provides the best trade-off between compression and random access compared to the state of the art: it reduces the genotype information size from 758 GiB down to 890 MiB on the publicly available 1000 Genomes Project (phase 3) data, which is 21% less than the state of the art in random-access capable methods.
Conclusions
By providing the best results in terms of combined random access and compression, GVC facilitates the efficient storage of large collections of gene sequence variations. In particular, the random access capability of GVC enables seamless remote data access and application integration. The software is open source and available at 
https://github.com/sXperfect/gvc/
."
367,"CenFind
: a deep-learning pipeline for efficient centriole detection in microscopy datasets","Background
High-throughput and selective detection of organelles in immunofluorescence images is an important but demanding task in cell biology. The centriole organelle is critical for fundamental cellular processes, and its accurate detection is key for analysing centriole function in health and disease. Centriole detection in human tissue culture cells has been achieved typically by manual determination of organelle number per cell. However, manual cell scoring of centrioles has a low throughput and is not reproducible. Published semi-automated methods tally the centrosome surrounding centrioles and not centrioles themselves. Furthermore, such methods rely on hard-coded parameters or require a multichannel input for cross-correlation. Therefore, there is a need for developing an efficient and versatile pipeline for the automatic detection of centrioles in single channel immunofluorescence datasets.

Results
We developed a deep-learning pipeline termed 
CenFind
 that automatically scores cells for centriole numbers in immunofluorescence images of human cells. 
CenFind
 relies on the multi-scale convolution neural network 
SpotNet
, which allows the accurate detection of sparse and minute foci in high resolution images. We built a dataset using different experimental settings and used it to train the model and evaluate existing detection methods. The resulting average F
1
-score achieved by 
CenFind
 is > 90% across the test set, demonstrating the robustness of the pipeline. Moreover, using the 
StarDist
-based nucleus detector, we link the centrioles and procentrioles detected with 
CenFind
 to the cell containing them, overall enabling automatic scoring of centriole numbers per cell.
Conclusions
Efficient, accurate, channel-intrinsic and reproducible detection of centrioles is an important unmet need in the field. Existing methods are either not discriminative enough or focus on a fixed multi-channel input. To fill this methodological gap, we developed 
CenFind
, a command line interface pipeline that automates cell scoring of centrioles, thereby enabling channel-intrinsic, accurate and reproducible detection across experimental modalities. Moreover, the modular nature of 
CenFind
 enables its integration in other pipelines. Overall, we anticipate 
CenFind
 to prove critical for accelerating discoveries in the field."
368,cnnLSV: detecting structural variants by encoding long-read alignment information and convolutional neural network,"Background
Genomic structural variant detection is a significant and challenging issue in genome analysis. The existing long-read based structural variant detection methods still have space for improvement in detecting multi-type structural variants.
Results
In this paper, we propose a method called cnnLSV to obtain detection results with higher quality by eliminating false positives in the detection results merged from the callsets of existing methods. We design an encoding strategy for four types of structural variants to represent long-read alignment information around structural variants into images, input the images into a constructed convolutional neural network to train a filter model, and load the trained model to remove the false positives to improve the detection performance. We also eliminate mislabeled training samples in the training model phase by using principal component analysis algorithm and unsupervised clustering algorithm 
k
-means. Experimental results on both simulated and real datasets show that our proposed method outperforms existing methods overall in detecting insertions, deletions, inversions, and duplications. The program of cnnLSV is available at 
https://github.com/mhuidong/cnnLSV
.
Conclusions
The proposed cnnLSV can detect structural variants by using long-read alignment information and convolutional neural network to achieve overall higher performance, and effectively eliminate incorrectly labeled samples by using the principal component analysis and 
k
-means algorithms in training model stage."
369,Identification and validation of tumor-infiltrating lymphocyte-related prognosis signature for predicting prognosis and immunotherapeutic response in bladder cancer,"Background
It has been discovered that tumor-infiltrating lymphocytes (TILs) are essential for the emergence of bladder cancer (BCa). This study aimed to research TIL-related genes (TILRGs) and create a gene model to predict BCa patients' overall survival.
Methods
The RNA sequencing and clinical data were downloaded from the TGCA and GEO databases. Using Pearson correlation analysis, TILRGs were evaluated. Moreover, hub TILRGs were chosen using a comprehensive analysis. By dividing the TCGA-BCa patients into different clusters based on hub TILRGs, we were able to explore the immune landscape between different clusters.
Results
Here, we constructed a model with five hub TILRGs and split all of the patients into two groups, each of which had a different prognosis and clinical characteristics, TME, immune cell infiltration, drug sensitivity, and immunotherapy responses. Better clinical results and greater immunotherapy sensitivity were seen in the low-risk group. Based on five hub TILRGs, unsupervised clustering analysis identify two molecular subtypes in BCa. The prognosis, clinical outcomes, and immune landscape differed in different subtypes.
Conclusions
The study identifies a new prediction signature based on genes connected to tumor-infiltrating lymphocytes, providing BCa patients with a new theoretical target."
370,ElasticBLAST: accelerating sequence search via cloud computing,"Background
Biomedical researchers use alignments produced by BLAST (Basic Local Alignment Search Tool) to categorize their query sequences. Producing such alignments is an essential bioinformatics task that is well suited for the cloud. The cloud can perform many calculations quickly as well as store and access large volumes of data. Bioinformaticians can also use it to collaborate with other researchers, sharing their results, datasets and even their pipelines on a common platform.
Results
We present ElasticBLAST, a cloud native application to perform BLAST alignments in the cloud. ElasticBLAST can handle anywhere from a few to many thousands of queries and run the searches on thousands of virtual CPUs (if desired), deleting resources when it is done. It uses cloud native tools for orchestration and can request discounted instances, lowering cloud costs for users. It is supported on Amazon Web Services and Google Cloud Platform. It can search BLAST databases that are user provided or from the National Center for Biotechnology Information.
Conclusion
We show that ElasticBLAST is a useful application that can efficiently perform BLAST searches for the user in the cloud, demonstrating that with two examples. At the same time, it hides much of the complexity of working in the cloud, lowering the threshold to move work to the cloud."
371,Complete sequence verification of plasmid DNA using the Oxford Nanopore Technologies’ MinION device,"Background
Sequence verification is essential for plasmids used as critical reagents or therapeutic products. Typically, high-quality plasmid sequence is achieved through capillary-based Sanger sequencing, requiring customized sets of primers for each plasmid. This process can become expensive, particularly for applications where the validated sequence needs to be produced within a regulated and quality-controlled environment for downstream clinical research applications.
Results
Here, we describe a cost-effective and accurate plasmid sequencing and consensus generation procedure using the Oxford Nanopore Technologies’ MinION device as an alternative to capillary-based plasmid sequencing options. This procedure can verify the identity of a pure population of plasmid, either confirming it matches the known and expected sequence, or identifying mutations present in the plasmid if any exist. We use a full MinION flow cell per plasmid, maximizing available data and allowing for stringent quality filters. Pseudopairing reads for consensus base calling reduces read error rates from 5.3 to 0.53%, and our pileup consensus approach provides per-base counts and confidence scores, allowing for interpretation of the certainty of the resulting consensus sequences. For pure plasmid samples, we demonstrate 100% accuracy in the resulting consensus sequence, and the sensitivity to detect small mutations such as insertions, deletions, and single nucleotide variants. In test cases where the sequenced pool of plasmids contains subclonal templates, detection sensitivity is similar to that of traditional capillary sequencing.
Conclusions
Our pipeline can provide significant cost savings compared to outsourcing clinical-grade sequencing of plasmids, making generation of high-quality plasmid sequence for clinical sequence verification more accessible. While other long-read-based methods offer higher-throughput and less cost, our pipeline produces complete and accurate sequence verification for cases where absolute sequence accuracy is required."
372,multiWGCNA: an R package for deep mining gene co-expression networks in multi-trait expression data,"Background
Gene co-expression networks represent modules of genes with shared biological function, and have been widely used to model biological pathways in gene expression data. Co-expression networks associated with a specific trait can be constructed and identified using weighted gene co-expression network analysis (WGCNA), which is especially useful for the study of transcriptional signatures in disease. WGCNA networks are typically constructed using both disease and wildtype samples, so molecular pathways associated with disease are identified. However, it would be advantageous to study such co-expression networks in their disease context across spatiotemporal conditions, but currently there is no comprehensive software implementation for this type of analysis.

Results
Here, we introduce a WGCNA-based procedure, multiWGCNA, that is tailored to datasets with variable spatial or temporal traits. As well as constructing the combined network, multiWGCNA also generates a network for each condition separately, and subsequently maps these modules between and across designs, and performs relevant downstream analyses, including module-trait correlation and module preservation. When applied to astrocyte-specific RNA-sequencing (RNA-seq) data from various brain regions of mice with experimental autoimmune encephalitis, multiWGCNA resolved the de novo formation of the neurotoxic astrocyte transcriptional program exclusively in the disease setting. Using time-course RNA-seq from mice with tau pathology (rTg4510), we demonstrate how multiWGCNA can also be used to study the temporal evolution of pathological modules over the course of disease progression.
Conclusion
The multiWGCNA R package can be applied to expression data with two dimensions, which is especially useful for the study of disease-associated modules across time or space. The source code and functions are freely available at: 
https://github.com/fogellab/multiWGCNA
."
373,EnsInfer: a simple ensemble approach to network inference outperforms any single method,"This study evaluates both a variety of existing base causal inference methods and a variety of ensemble methods. We show that: (i) base network inference methods vary in their performance across different datasets, so a method that works poorly on one dataset may work well on another; (ii) a non-homogeneous ensemble method in the form of a Naive Bayes classifier leads overall to as good or better results than using the best single base method or any other ensemble method; (iii) for the best results, the ensemble method should integrate all methods that satisfy a statistical test of normality on training data. The resulting ensemble model 
EnsInfer
 easily integrates all kinds of RNA-seq data as well as new and existing inference methods. The paper categorizes and reviews state-of-the-art underlying methods, describes the 
EnsInfer
 ensemble approach in detail, and presents experimental results. The source code and data used will be made available to the community upon publication."
374,Predicting miRNA-disease associations based on PPMI and attention network,"Background
With the development of biotechnology and the accumulation of theories, many studies have found that microRNAs (miRNAs) play an important role in various diseases. Uncovering the potential associations between miRNAs and diseases is helpful to better understand the pathogenesis of complex diseases. However, traditional biological experiments are expensive and time-consuming. Therefore, it is necessary to develop more efficient computational methods for exploring underlying disease-related miRNAs.
Results
In this paper, we present a new computational method based on positive point-wise mutual information (PPMI) and attention network to predict miRNA-disease associations (MDAs), called PATMDA. Firstly, we construct the heterogeneous MDA network and multiple similarity networks of miRNAs and diseases. Secondly, we respectively perform random walk with restart and PPMI on different similarity network views to get multi-order proximity features and then obtain high-order proximity representations of miRNAs and diseases by applying the convolutional neural network to fuse the learned proximity features. Then, we design an attention network with neural aggregation to integrate the representations of a node and its heterogeneous neighbor nodes according to the MDA network. Finally, an inner product decoder is adopted to calculate the relationship scores between miRNAs and diseases.
Conclusions
PATMDA achieves superior performance over the six state-of-the-art methods with the area under the receiver operating characteristic curve of 0.933 and 0.946 on the HMDD v2.0 and HMDD v3.2 datasets, respectively. The case studies further demonstrate the validity of PATMDA for discovering novel disease-associated miRNAs."
375,A review and comparative study of cancer detection using machine learning: SBERT and SimCSE application,"Background
Using visual, biological, and electronic health records data as the sole input source, pretrained convolutional neural networks and conventional machine learning methods have been heavily employed for the identification of various malignancies. Initially, a series of preprocessing steps and image segmentation steps are performed to extract region of interest features from noisy features. Then, the extracted features are applied to several machine learning and deep learning methods for the detection of cancer.
Methods
In this work, a review of all the methods that have been applied to develop machine learning algorithms that detect cancer is provided. With more than 100 types of cancer, this study only examines research on the four most common and prevalent cancers worldwide: lung, breast, prostate, and colorectal cancer. Next, by using state-of-the-art sentence transformers namely: SBERT (2019) and the unsupervised SimCSE (2021), this study proposes a new methodology for detecting cancer. This method requires raw DNA sequences of matched tumor/normal pair as the only input. The learnt DNA representations retrieved from SBERT and SimCSE will then be sent to machine learning algorithms (XGBoost, Random Forest, LightGBM, and CNNs) for classification. As far as we are aware, SBERT and SimCSE transformers have not been applied to represent DNA sequences in cancer detection settings.
Results
The XGBoost model, which had the highest overall accuracy of 73 ± 0.13 % using SBERT embeddings and 75 ± 0.12 % using SimCSE embeddings, was the best performing classifier. In light of these findings, it can be concluded that incorporating sentence representations from SimCSE’s sentence transformer only marginally improved the performance of machine learning models."
376,Study of the error correction capability of multiple sequence alignment algorithm (MAFFT) in DNA storage,"Synchronization (insertions–deletions) errors are still a major challenge for reliable information retrieval in DNA storage. Unlike traditional error correction codes (ECC) that add redundancy in the stored information, multiple sequence alignment (MSA) solves this problem by searching the conserved subsequences. In this paper, we conduct a comprehensive simulation study on the error correction capability of a typical MSA algorithm, MAFFT. Our results reveal that its capability exhibits a phase transition when there are around 20% errors. Below this critical value, increasing sequencing depth can eventually allow it to approach complete recovery. Otherwise, its performance plateaus at some poor levels. Given a reasonable sequencing depth (≤ 70), MSA could achieve complete recovery in the low error regime, and effectively correct 90% of the errors in the medium error regime. In addition, MSA is robust to imperfect clustering. It could also be combined with other means such as ECC, repeated markers, or any other code constraints. Furthermore, by selecting an appropriate sequencing depth, this strategy could achieve an optimal trade-off between cost and reading speed. MSA could be a competitive alternative for future DNA storage."
377,CNN-Siam: multimodal siamese CNN-based deep learning approach for drug‒drug interaction prediction,"Background
Drug‒drug interactions (DDIs) are reactions between two or more drugs, i.e., possible situations that occur when two or more drugs are used simultaneously. DDIs act as an important link in both drug development and clinical treatment. Since it is not possible to study the interactions of such a large number of drugs using experimental means, a computer-based deep learning solution is always worth investigating. We propose a deep learning-based model that uses twin convolutional neural networks to learn representations from multimodal drug data and to make predictions about the possible types of drug effects.
Results
In this paper, we propose a novel convolutional neural network algorithm using a Siamese network architecture called CNN-Siam. CNN-Siam uses a convolutional neural network (CNN) as a backbone network in the form of a twin network architecture to learn the feature representation of drug pairs from multimodal data of drugs (including chemical substructures, targets and enzymes). Moreover, this network is used to predict the types of drug interactions with the best optimization algorithms available (RAdam and LookAhead). The experimental data show that the CNN-Siam achieves an area under the precision-recall (AUPR) curve score of 0.96 on the benchmark dataset and a correct rate of 92%. These results are significant improvements compared to the state-of-the-art method (from 86 to 92%) and demonstrate the robustness of the CNN-Siam and the superiority of the new optimization algorithm through ablation experiments.
Conclusion
The experimental results show that our multimodal siamese convolutional neural network can accurately predict DDIs, and the Siamese network architecture is able to learn the feature representation of drug pairs better than individual networks. CNN-Siam outperforms other state-of-the-art algorithms with the combination of data enhancement and better optimizers. But at the same time, CNN-Siam has some drawbacks, longer training time, generalization needs to be improved, and poorer classification results on some classes."
378,MSLP: mRNA subcellular localization predictor based on machine learning techniques,"Background
Subcellular localization of messenger RNA (mRNAs) plays a pivotal role in the regulation of gene expression, cell migration as well as in cellular adaptation. Experiment techniques for pinpointing the subcellular localization of mRNAs are laborious, time-consuming and expensive. Therefore,  in silico approaches for this purpose are attaining great attention in the RNA community.
Methods
In this article, we propose MSLP, a machine learning-based method to predict the subcellular localization of mRNA. We propose a novel combination of four types of features representing k-mer, pseudo k-tuple nucleotide composition (PseKNC),  physicochemical properties of nucleotides, and 3D representation of sequences based on Z-curve transformation to feed into machine learning algorithm to predict the subcellular localization of mRNAs.
Results
Considering the combination of the above-mentioned features, ennsemble-based models achieved state-of-the-art results in mRNA subcellular localization prediction tasks for multiple benchmark datasets. We evaluated the performance of our method  in ten subcellular locations, covering cytoplasm, nucleus, endoplasmic reticulum (ER), extracellular region (ExR), mitochondria, cytosol, pseudopodium, posterior, exosome, and the ribosome. Ablation study highlighted k-mer and PseKNC to be more dominant than other features for predicting cytoplasm, nucleus, and ER localizations. On the other hand, physicochemical properties and Z-curve based features contributed the most to ExR and mitochondria detection. SHAP-based analysis revealed the relative importance of features to provide better insights into the proposed approach.
Availability
We have implemented a Docker container and API for end users to run their sequences on our model. Datasets, the code of API and the Docker are shared for the community in GitHub at: 
https://github.com/smusleh/MSLP
."
379,Bayesian kinetic modeling for tracer-based metabolomic data,"Background
Stable Isotope Resolved Metabolomics (SIRM) is a new biological approach that uses stable isotope tracers such as uniformly 
\(^{13}C\)
-enriched glucose (
\(^{13}C_6\)
-Glc) to trace metabolic pathways or networks at the atomic level in complex biological systems. Non-steady-state kinetic modeling based on SIRM data uses sets of simultaneous ordinary differential equations (ODEs) to quantitatively characterize the dynamic behavior of metabolic networks. It has been increasingly used to understand the regulation of normal metabolism and dysregulation in the development of diseases. However, fitting a kinetic model is challenging because there are usually multiple sets of parameter values that fit the data equally well, especially for large-scale kinetic models. In addition, there is a lack of statistically rigorous methods to compare kinetic model parameters between different experimental groups.
Results
We propose a new Bayesian statistical framework to enhance parameter estimation and hypothesis testing for non-steady-state kinetic modeling of SIRM data. For estimating kinetic model parameters, we leverage the prior distribution not only to allow incorporation of experts’ knowledge but also to provide robust parameter estimation. We also introduce a shrinkage approach for borrowing information across the ensemble of metabolites to stably estimate the variance of an individual isotopomer. In addition, we use a component-wise adaptive Metropolis algorithm with delayed rejection to perform efficient Monte Carlo sampling of the posterior distribution over high-dimensional parameter space. For comparing kinetic model parameters between experimental groups, we propose a new reparameterization method that converts the complex hypothesis testing problem into a more tractable parameter estimation problem. We also propose an inference procedure based on credible interval and credible value. Our method is freely available for academic use at 
https://github.com/xuzhang0131/MCMCFlux
.
Conclusions
Our new Bayesian framework provides robust estimation of kinetic model parameters and enables rigorous comparison of model parameters between experimental groups. Simulation studies and application to a lung cancer study demonstrate that our framework performs well for non-steady-state kinetic modeling of SIRM data."
380,Visual dynamics: a WEB application for molecular dynamics simulation using GROMACS,"Background
The molecular dynamics is an approach to obtain kinetic and thermodynamic characteristics of biomolecular structures. The molecular dynamics simulation softwares are very useful, however, most of them are used in command line form and continue with the same common implementation difficulties that plague researchers who are not computer specialists.

Results
Here, we have developed the VisualDynamics—a WEB tool developed to automate biological simulations performed in Gromacs using a graphical interface to make molecular dynamics simulation user-friendly task. In this new application the researcher can submit a simulation of the protein in the free form or complexed with a ligand. Can also download the graphics analysis and log files at the end of the simulation.

Conclusions
VisualDynamics is a tool that will accelerate implementations and learning in the area of molecular dynamics simulation. Freely available at 
https://visualdynamics.fiocruz.br/login
, is supported by all major web browsers. VisualDynamics was developed with Flask, which is a Python-based free and open-source framework for web development. The code is freely available for download at GitHub 
https://github.com/LABIOQUIM/visualdynamics
."
381,Pickaxe: a Python library for the prediction of novel metabolic reactions,"Background
Biochemical reaction prediction tools leverage enzymatic promiscuity rules to generate reaction networks containing novel compounds and reactions. The resulting reaction networks can be used for multiple applications such as designing novel biosynthetic pathways and annotating untargeted metabolomics data. It is vital for these tools to provide a robust, user-friendly method to generate networks for a given application. However, existing tools lack the flexibility to easily generate networks that are tailor-fit for a user’s application due to lack of exhaustive reaction rules, restriction to pre-computed networks, and difficulty in using the software due to lack of documentation.
Results
Here we present Pickaxe, an open-source, flexible software that provides a user-friendly method to generate novel reaction networks. This software iteratively applies reaction rules to a set of metabolites to generate novel reactions. Users can select rules from the prepackaged JN1224min ruleset, derived from MetaCyc, or define their own custom rules. Additionally, filters are provided which allow for the pruning of a network on-the-fly based on compound and reaction properties. The filters include chemical similarity to target molecules, metabolomics, thermodynamics, and reaction feasibility filters. Example applications are given to highlight the capabilities of Pickaxe: the expansion of common biological databases with novel reactions, the generation of industrially useful chemicals from a yeast metabolome database, and the annotation of untargeted metabolomics peaks from an 
E. coli
 dataset.
Conclusion
Pickaxe predicts novel metabolic reactions and compounds, which can be used for a variety of applications. This software is open-source and available as part of the MINE Database python package (
https://pypi.org/project/minedatabase/
) or on GitHub (
https://github.com/tyo-nu/MINE-Database
). Documentation and examples can be found on Read the Docs (
https://mine-database.readthedocs.io/en/latest/
). Through its documentation, pre-packaged features, and customizable nature, Pickaxe allows users to generate novel reaction networks tailored to their application."
382,PWN: enhanced random walk on a warped network for disease target prioritization,"Background
Extracting meaningful information from unbiased high-throughput data has been a challenge in diverse areas. Specifically, in the early stages of drug discovery, a considerable amount of data was generated to understand disease biology when identifying disease targets. Several random walk-based approaches have been applied to solve this problem, but they still have limitations. Therefore, we suggest a new method that enhances the effectiveness of high-throughput data analysis with random walks.
Results
We developed a new random walk-based algorithm named prioritization with a warped network (PWN), which employs a warped network to achieve enhanced performance. Network warping is based on both internal and external features: graph curvature and prior knowledge.
Conclusions
We showed that these compositive features synergistically increased the resulting performance when applied to random walk algorithms, which led to PWN consistently achieving the best performance among several other known methods. Furthermore, we performed subsequent experiments to analyze the characteristics of PWN."
383,Establishing a prognostic model of chromatin modulators and identifying potential drug candidates in renal clear cell patients,"Background
Renal carcinoma is a common malignant tumor of the urinary system. Advanced renal carcinoma has a low 5-year survival rate and a poor prognosis. More and more studies have confirmed that chromatin regulators (CRs) can regulate the occurrence and development of cancer. This article investigates the functional and prognostic value of CRs in renal carcinoma patients.
Methods
mRNA expression and clinical information were obtained from The Cancer Genome Atlas database. Univariate Cox regression analysis and LASSO regression analysis were used to select prognostic chromatin-regulated genes and use them to construct a risk model for predicting the prognosis of renal cancer. Differences in prognosis between high-risk and low-risk groups were compared using Kaplan–Meier analysis. In addition, we analyzed the relationship between chromatin regulators and tumor immune infiltration, and explored differences in drug sensitivity between risk groups.
Results
We constructed a model consisting of 11 CRs to predict the prognosis of renal cancer patients. We not only successfully validated its feasibility, but also found that the 11 CR-based model was an independent prognostic factor. Functional analysis showed that CRs were mainly enriched in cancer development-related signalling pathways. We also found through the TIMER database that CR-based models were also associated with immune cell infiltration and immune checkpoints. At the same time, the genomics of drug sensitivity in cancer database was used to analyze the commonly used drugs of renal clear cell carcinoma patients. It was found that patients in the low-risk group were sensitive to medicines such as axitinib, pazopanib, sorafenib, and gemcitabine. In contrast, those in the high-risk group may be sensitive to sunitinib.
Conclusion
The chromatin regulator-related prognostic model we constructed can be used to assess the prognostic risk of patients with clear cell renal cell carcinoma. The results of this study can bring new ideas for targeted therapy of clear cell renal carcinoma, helping doctors to take corresponding measures in advance for patients with different risks."
384,Seven bacterial response-related genes are biomarkers for colon cancer,"Background
Colon cancer (CC) is a common tumor that causes significant harm to human health. Bacteria play a vital role in cancer biology, particularly the biology of CC. Genes related to bacterial response were seldom used to construct prognosis models. We constructed a bacterial response-related risk model based on three Molecular Signatures Database gene sets to explore new markers for predicting CC prognosis.
Methods
The Cancer Genome Atlas (TCGA) colon adenocarcinoma samples were used as the training set, and Gene Expression Omnibus (GEO) databases were used as the test set. Differentially expressed bacterial response-related genes were identified for prognostic gene selection. Univariate Cox regression analysis, least absolute shrinkage and selection operator-penalized Cox regression analysis, and multivariate Cox regression analysis were performed to construct a prognostic risk model. The individual diagnostic effects of genes in the prognostic model were also evaluated. Moreover, differentially expressed long noncoding RNAs (lncRNAs) were identified. Finally, the expression of these genes was validated using quantitative polymerase chain reaction (qPCR) in cell lines and tissues.
Results
A prognostic signature was constructed based on seven bacterial response genes: 
LGALS4, RORC, DDIT3, NSUN5, RBCK1, RGL2, and SERPINE1
. Patients were assigned a risk score based on the prognostic model, and patients in the TCGA cohort with a high risk score had a poorer prognosis than those with a low risk score; a similar finding was observed in the GEO cohort. These seven prognostic model genes were also independent diagnostic factors. Finally, qPCR validated the differential expression of the seven model genes and two coexpressed lncRNAs (C6orf223 and SLC12A9-AS1) in 27 pairs of CC and normal tissues. Differential expression of 
LGALS4
 and 
NSUN5
 was also verified in cell lines (FHC, COLO320DM, SW480).
Conclusions
We created a seven-gene bacterial response‐related gene signature that can accurately predict the outcomes of patients with CC. This model can provide valuable insights for personalized treatment."
385,Snapshot: a package for clustering and visualizing epigenetic history during cell differentiation,"Background
Epigenetic modification of chromatin plays a pivotal role in regulating gene expression during cell differentiation. The scale and complexity of epigenetic data pose significant challenges for biologists to identify the regulatory events controlling cell differentiation.
Results
To reduce the complexity, we developed a package, called Snapshot, for clustering and visualizing candidate cis-regulatory elements (cCREs) based on their epigenetic signals during cell differentiation. This package first introduces a binarized indexing strategy for clustering the cCREs. It then provides a series of easily interpretable figures for visualizing the signal and epigenetic state patterns of the cCREs clusters during the cell differentiation. It can also use different hierarchies of cell types to highlight the epigenetic history specific to any particular cell lineage. We demonstrate the utility of Snapshot using data from a consortium project for 
V
al
I
dated 
S
ystematic 
I
ntegrati
ON
 (VISION) of epigenomic data in hematopoiesis.
Conclusion
The package Snapshot can identify all distinct clusters of genomic locations with unique epigenetic signal patterns during cell differentiation. It outperforms other methods in terms of interpreting and reproducing the identified cCREs clusters. The package of Snapshot is available at GitHub: 
https://github.com/guanjue/Snapshot
."
386,Robust classification using average correlations as features (ACF),"Motivation
In single-cell transcriptomics and other omics technologies, large fractions of missing values commonly occur. Researchers often either consider only those features that were measured for each instance of their dataset, thereby accepting severe loss of information, or use imputation which can lead to erroneous results. Pairwise metrics allow for imputation-free classification with minimal loss of data.
Results
Using pairwise correlations as metric, state-of-the-art approaches to classification would include the 
K-nearest-neighbor
- (KNN) and 
distribution-based-classification
-classifier. Our novel method, termed average correlations as features (ACF), significantly outperforms those approaches by training tunable machine learning models on inter-class and intra-class correlations. Our approach is characterized in simulation studies and its classification performance is demonstrated on real-world datasets from single-cell RNA sequencing and bottom-up proteomics. Furthermore, we demonstrate that variants of our method offer superior flexibility and performance over KNN classifiers and can be used in conjunction with other machine learning methods. In summary, ACF is a flexible method that enables missing value tolerant classification with minimal loss of data."
387,Novel m7G-related lncRNA signature for predicting overall survival in patients with gastric cancer,"Presenting with a poor prognosis, gastric cancer (GC) remains one of the leading causes of disease and death worldwide. Long non-coding RNAs (lncRNAs) regulate tumor formation and have been long used to predict tumor prognosis. N7-methylguanosine (m7G) is the most prevalent RNA modification. m7G-lncRNAs regulate GC onset and progression, but their precise mechanism in GC is unclear. The objective of this research was the development of a new m7G-related lncRNA signature as a biomarker for predicting GC survival rate and guiding treatment. The Cancer Genome Atlas database helped extract gene expression data and clinical information for GC. Pearson correlation analysis helped point out m7G-related lncRNAs. Univariate Cox analysis helped in identifying m7G-related lncRNA with predictive capability. The Lasso-Cox method helped point out seven lncRNAs for the purpose of establishing an m7G-related lncRNA prognostic signature (m7G-LPS), followed by the construction of a nomogram. Kaplan–Meier analysis, univariate and multivariate Cox regression analysis, calibration plot of the nomogram model, receiver operating characteristic curve and principal component analysis were utilized for the verification of the risk model’s reliability. Furthermore, q-PCR helped verify the lncRNAs expression of m7G-LPS in-vitro. The study subjects were classified into high and low-risk groups based on the median value of the risk score. Gene enrichment analysis confirmed the constructed m7G-LPS’ correlation with RNA transcription and translation and multiple immune-related pathways. Analysis of the clinicopathological features revealed more progressive features in the high-risk group. CIBERSORT analysis showed the involvement of m7G-LPS in immune cell infiltration. The risk score was correlated with immune checkpoint gene expression, immune cell and immune function score, immune cell infiltration, and chemotherapy drug sensitivity. Therefore, our study shows that m7G-LPS constructed using seven m7G-related lncRNAs can predict the survival time of GC patients and guide chemotherapy and immunotherapy regimens as biomarker."
388,LACE 2.0: an interactive R tool for the inference and visualization of longitudinal cancer evolution,"Background
Longitudinal single-cell sequencing experiments of patient-derived models are increasingly employed to investigate cancer evolution. In this context, robust computational methods are needed to properly exploit the mutational profiles of single cells generated via variant calling, in order to reconstruct the evolutionary history of a tumor and characterize the impact of therapeutic strategies, such as the administration of drugs. To this end, we have recently developed the LACE framework for the Longitudinal Analysis of Cancer Evolution.
Results
The LACE 2.0 release aimed at inferring longitudinal clonal trees enhances the original framework with new key functionalities: an improved data management for preprocessing of standard variant calling data, a reworked inference engine, and direct connection to public databases.
Conclusions
All of this is accessible through a new and interactive Shiny R graphical interface offering the possibility to apply filters helpful in discriminating relevant or potential driver mutations, set up inferential parameters, and visualize the results. The software is available at: 
github.com/BIMIB-DISCo/LACE
."
389,"n
PoRe: 
n
-polymer realigner for improved pileup-based variant calling","Despite recent improvements in nanopore basecalling accuracy, germline variant calling of small insertions and deletions (INDELs) remains poor. Although precision and recall for single nucleotide polymorphisms (SNPs) now exceeds 99.5%, INDEL recall remains below 80% for standard R9.4.1 flow cells. We show that read phasing and realignment can recover a significant portion of false negative INDELs. In particular, we extend Needleman-Wunsch affine gap alignment by introducing new gap penalties for more accurately aligning repeated 
n
-polymer sequences such as homopolymers (
\(n=1\)
) and tandem repeats (
\(2 \le n \le 6\)
). At the same precision, haplotype phasing improves INDEL recall from 63.76 to 
\(70.66\%\)
 and nPoRe realignment improves it further to 
\(73.04\%\)
."
390,B-LBConA: a medical entity disambiguation model based on Bio-LinkBERT and context-aware mechanism,"Background
The main task of medical entity disambiguation is to link mentions, such as diseases, drugs, or complications, to standard entities in the target knowledge base. To our knowledge, models based on Bidirectional Encoder Representations from Transformers (BERT) have achieved good results in this task. Unfortunately, these models only consider text in the current document, fail to capture dependencies with other documents, and lack sufficient mining of hidden information in contextual texts.
Results
We propose B-LBConA, which is based on Bio-LinkBERT and context-aware mechanism. Specifically, B-LBConA first utilizes Bio-LinkBERT, which is capable of learning cross-document dependencies, to obtain embedding representations of mentions and candidate entities. Then, cross-attention is used to capture the interaction information of mention-to-entity and entity-to-mention. Finally, B-LBConA incorporates disambiguation clues about the relevance between the mention context and candidate entities via the context-aware mechanism.
Conclusions
Experiment results on three publicly available datasets, NCBI, ADR and ShARe/CLEF, show that B-LBConA achieves a signifcantly more accurate performance compared with existing models."
391,Favoring the hierarchical constraint in penalized survival models for randomized trials in precision medicine,"Background
The research of biomarker-treatment interactions is commonly investigated in randomized clinical trials (RCT) for improving medicine precision. The hierarchical interaction constraint states that an interaction should only be in a model if its main effects are also in the model. However, this constraint is not guaranteed in the standard penalized statistical approaches. We aimed to find a compromise for high-dimensional data between the need for sparse model selection and the need for the hierarchical constraint.
Results
To favor the property of the hierarchical interaction constraint, we proposed to create groups composed of the biomarker main effect and its interaction with treatment and to perform the bi-level selection on these groups. We proposed two weighting approaches (Single Wald (SW) and likelihood ratio test (LRT)) for the adaptive lasso method. The selection performance of these two approaches is compared to alternative lasso extensions (adaptive lasso with ridge-based weights, composite Minimax Concave Penalty, group exponential lasso and Sparse Group Lasso) through a simulation study. A RCT (NSABP B-31) randomizing 1574 patients (431 events) with early breast cancer aiming to evaluate the effect of adjuvant trastuzumab on distant-recurrence free survival with expression data from 462 genes measured in the tumour will serve for illustration. The simulation study illustrates that the adaptive lasso LRT and SW, and the group exponential lasso favored the hierarchical interaction constraint. Overall, in the alternative scenarios, they had the best balance of false discovery and false negative rates for the main effects of the selected interactions. For NSABP B-31, 12 gene-treatment interactions were identified more than 20% by the different methods. Among them, the adaptive lasso (SW) approach offered the best trade-off between a high number of selected gene-treatment interactions and a high proportion of selection of both the gene-treatment interaction and its main effect.
Conclusions
Adaptive lasso with Single Wald and likelihood ratio test weighting and the group exponential lasso approaches outperformed their competitors in favoring the hierarchical constraint of the biomarker-treatment interaction. However, the performance of the methods tends to decrease in the presence of prognostic biomarkers."
392,CACSV: a computational web-sever that provides classification for cancer somatic genetic variants from different tissues,"Background
Understanding the role and function of genetic variants is extremely important when analyzing and interpreting a myriad of human disease processes. For cancer in general, cell-specific genetic variants are ubiquitous and distinct tissues have significantly heterogenic genetic profiles. In clinical practice, only a few genetic variants have identifiable clinical utility. Finding clinically relevant genetic variants constitute a challenging process. In addition, there had been no reference protocol to provide guidance for cancer somatic genetic variants classification and interpretation. In 2017, the first version of a reference protocol was published by the Association for Molecular Pathology, the American Society of Clinical Oncology, and the College of American Pathologists. Previously, we incorporated the reference protocol into a computational method to expedite the process of identification of clinically relevant genetic variants. In this work, we developed a computational web-server to increase the accessibility and availability of clinically relevant genetic variants.
Results
Our work provides the clinical classification for ~ 3 million cancer genetic variants that are now publicly available in a shareable database on GitHub. We have developed a graphical user interface for the database to enhance the accessibility and ease-of-use.
Conclusion
CACSV provides an open-source for about 3 million cancer tissue-specific genetic variants with their assigned clinical annotations."
393,Progressive search in tandem mass spectrometry,"Background
High-throughput Proteomics has been accelerated by (tandem) mass spectrometry. However, the slow speed of mass spectra analysis prevents the analysis results from being up-to-date. Tandem mass spectrometry database search requires 
O
(|
S||D|
) time where 
S
 is the set of spectra and 
D
 is the set of peptides in a database. With usual values of |
S|
 and |
D|
, database search is quite time consuming. Meanwhile, the database for search is usually updated every month, with 0.5–2% changes. Although the change in the database is usually very small, it may cause extensive changes in the overall analysis results because individual PSM scores such as deltaCn and E-value depend on the entire search results. Therefore, to keep the search results up-to-date, one needs to perform database search from scratch every time the database is updated, which is very inefficient.
Results
Thus, we present a very efficient method to keep the search results up-to-date where the results are the same as those achieved by the normal search from scratch. This method, called progressive search, runs in 
O
(|
S||ΔD|
) time on average where 
ΔD
 is the difference between the old and the new databases. The experimental results show that the progressive search is up to 53.9 times faster for PSM update only and up to 16.5 times faster for both PSM and E-value update.
Conclusions
Progressive search is a novel approach to efficiently obtain analysis results for updated database in tandem mass spectrometry. Compared to performing a normal search from scratch, progressive search achieves the same results much faster. Progressive search is freely available at: 
https://isa.hanyang.ac.kr/ProgSearch.html
."
394,Multi-view feature representation and fusion for drug-drug interactions prediction,"Background
Drug-drug interactions (DDIs) prediction is vital for pharmacology and clinical application to avoid adverse drug reactions on patients. It is challenging because DDIs are related to multiple factors, such as genes, drug molecular structure, diseases, biological processes, side effects, etc. It is a crucial technology for Knowledge graph to present multi-relation among entities. Recently some existing graph-based computation models have been proposed for DDIs prediction and get good performance. However, there are still some challenges in the knowledge graph representation, which can extract rich latent features from drug knowledge graph (KG).
Results
In this work, we propose a novel multi-view feature representation and fusion (MuFRF) architecture to realize DDIs prediction. It consists of two views of feature representation and a multi-level latent feature fusion. For the feature representation from the graph view and KG view, we use graph isomorphism network to map drug molecular structures and use RotatE to implement the vector representation on bio-medical knowledge graph, respectively. We design concatenate-level and scalar-level strategies in the multi-level latent feature fusion to capture latent features from drug molecular structure information and semantic features from bio-medical KG. And the multi-head attention mechanism achieves the optimization of features on binary and multi-class classification tasks. We evaluate our proposed method based on two open datasets in the experiments. Experiments indicate that MuFRF outperforms the classic and state-of-the-art models.
Conclusions
Our proposed model can fully exploit and integrate the latent feature from the drug molecular structure graph (graph view) and rich bio-medical knowledge graph (KG view). We find that a multi-view feature representation and fusion model can accurately predict DDIs. It may contribute to providing with some guidance for research and validation for discovering novel DDIs."
395,New proposal of viral genome representation applied in the classification of SARS-CoV-2 with deep learning,"Background
In December 2019, the first case of COVID-19 was described in Wuhan, China, and by July 2022, there were already 540 million confirmed cases. Due to the rapid spread of the virus, the scientific community has made efforts to develop techniques for the viral classification of SARS-CoV-2.
Results
In this context, we developed a new proposal for gene sequence representation with Genomic Signal Processing techniques for the work presented in this paper. First, we applied the mapping approach to samples of six viral species of the Coronaviridae family, which belongs SARS-CoV-2 Virus. We then used the sequence downsized obtained by the method proposed in a deep learning architecture for viral classification, achieving an accuracy of 98.35%, 99.08%, and 99.69% for the 64, 128, and 256 sizes of the viral signatures, respectively, and obtaining 99.95% precision for the vectors with size 256.
Conclusions
The classification results obtained, in comparison to the results produced using other state-of-the-art representation techniques, demonstrate that the proposed mapping can provide a satisfactory performance result with low computational memory and processing time costs."
396,FDC-SP as a diagnostic and prognostic biomarker and modulates immune infiltrates in renal cell carcinoma,"Background
Renal cell carcinoma (RCC), one of the top 10 causes of cancer death, is responsible for more than 90% of all cases of primary renal cancer worldwide. Follicular dendritic cell-secreted protein (FDC-SP) specifically binds to activated B cells and regulates the generation of antibodies. It is also thought to promote cancer cell invasion and migration, which could help with tumor metastases. This study aimed to assess the efficacy of FDC-SP in the diagnosis and prognosis of RCC and to investigate the relationship between immune infiltration in RCC and these outcomes.
Results
RCC tissues had significantly higher levels of FDC-SP protein and mRNA than normal tissues. The high level of FDC-SP expression was linked to the T stage, histological grade, pathological stage, N stage, M stage, and OS event. Functional enrichment analysis identified the major pathways that were enriched as immune response regulation, complement, and coagulation. Immunological checkpoints and immune cell infiltration were observed to substantially correlate with the levels of FDC-SP expression. FDC-SP expression levels showed the ability to precisely distinguish high-grade or high-stage renal cancer (area under the curve (AUC) = 0.830, 0.722), and RCC patients with higher FDC-SP expression levels had worse prognoses. The AUC values for one-, two-, and five-year survival rates were all greater than 0.600. Moreover, the FDC-SP expression is an independent predictive biomarker of OS in RCC patients.
Conclusion
FDC-SP may be a prospective therapeutic target in RCC as well as a possible diagnostic and prognostic biomarker associated with immune infiltration."
397,Improved computations for relationship inference using low-coverage sequencing data,"Pedigree inference, for example determining whether two persons are second cousins or unrelated, can be done by comparing their genotypes at a selection of genetic markers. When the data for one or more of the persons is from low-coverage next generation sequencing (lcNGS), currently available computational methods either ignore genetic linkage or do not take advantage of the probabilistic nature of lcNGS data, relying instead on first estimating the genotype. We provide a method and software (see familias.name/lcNGS) bridging the above gap. Simulations indicate how our results are considerably more accurate compared to some previously available alternatives. Our method, utilizing a version of the Lander-Green algorithm, uses a group of symmetries to speed up calculations. This group may be of further interest in other calculations involving linked loci."
398,Development and validation of a coagulation-related genes prognostic model for hepatocellular carcinoma,"Background
Hepatocellular carcinoma (HCC) has a high incidence and mortality worldwide, which seriously threatens people's physical and mental health. Coagulation is closely related to the occurrence and development of HCC. Whether coagulation-related genes (CRGs) can be used as prognostic markers for HCC remains to be investigated.
Methods

Firstly, we identified differentially expressed coagulation-related genes of HCC and control samples in the datasets GSE54236, GSE102079, TCGA-LIHC, and Genecards database. Then, univariate Cox regression analysis, LASSO regression analysis, and multivariate Cox regression analysis were used to determine the key CRGs and establish the coagulation-related risk score (CRRS) prognostic model in the TCGA-LIHC dataset. The predictive capability of the CRRS model was evaluated by Kaplan–Meier survival analysis and ROC analysis. External validation was performed in the ICGC-LIRI-JP dataset. Besides, combining risk score and age, gender, grade, and stage, a nomogram was constructed to quantify the survival probability. We further analyzed the correlation between risk score and functional enrichment, pathway, and tumor immune microenvironment.
Results
We identified 5 key CRGs (FLVCR1, CENPE, LCAT, CYP2C9, and NQO1) and constructed the CRRS prognostic model. The overall survival (OS) of the high-risk group was shorter than that of the low-risk group. The AUC values for 1 -, 3 -, and 5-year OS in the TCGA dataset were 0.769, 0.691, and 0.674, respectively. The Cox analysis showed that CRRS was an independent prognostic factor for HCC. A nomogram established with risk score, age, gender, grade, and stage, has a better prognostic value for HCC patients. In the high-risk group, CD4
+
T cells memory resting, NK cells activated, and B cells naive were significantly lower. The expression levels of immune checkpoint genes in the high-risk group were generally higher than that in the low-risk group.
Conclusions
The CRRS model has reliable predictive value for the prognosis of HCC patients."
399,The personalized cancer network explorer (PeCaX) as a visual analytics tool to support molecular tumor boards,"Background
Personalized oncology represents a shift in cancer treatment from conventional methods to target specific therapies where the decisions are made based on the patient specific tumor profile. Selection of the optimal therapy relies on a complex interdisciplinary analysis and interpretation of these variants by experts in molecular tumor boards. With up to hundreds of somatic variants identified in a tumor, this process requires visual analytics tools to guide and accelerate the annotation process.
Results
The Personal Cancer Network Explorer (PeCaX) is a visual analytics tool supporting the efficient annotation, navigation, and interpretation of somatic genomic variants through functional annotation, drug target annotation, and visual interpretation within the context of biological networks. Starting with somatic variants in a VCF file, PeCaX enables users to explore these variants through a web-based graphical user interface. The most protruding feature of PeCaX is the combination of clinical variant annotation and gene-drug networks with an interactive visualization. This reduces the time and effort the user needs to invest to get to a treatment suggestion and helps to generate new hypotheses. PeCaX is being provided as a platform-independent containerized software package for local or institution-wide deployment. PeCaX is available for download at 
https://github.com/KohlbacherLab/PeCaX-docker
."
400,Acute stress reduces population-level metabolic and proteomic variation,"Background
Variation in omics data due to intrinsic biological stochasticity is often viewed as a challenging and undesirable feature of complex systems analyses. In fact, numerous statistical methods are utilized to minimize the variation among biological replicates.
Results
We demonstrate that the common statistics relative standard deviation (RSD) and coefficient of variation (CV), which are often used for quality control or part of a larger pipeline in omics analyses, can also be used as a metric of a physiological stress response. Using an approach we term Replicate Variation Analysis (RVA), we demonstrate that acute physiological stress leads to feature-wide canalization of CV profiles of metabolomes and proteomes across biological replicates. Canalization is the repression of variation between replicates, which increases phenotypic similarity. Multiple in-house mass spectrometry omics datasets in addition to publicly available data were analyzed to assess changes in CV profiles in plants, animals, and microorganisms. In addition, proteomics data sets were evaluated utilizing RVA to identify functionality of reduced CV proteins.
Conclusions
RVA provides a foundation for understanding omics level shifts that occur in response to cellular stress. This approach to data analysis helps characterize stress response and recovery, and could be deployed to detect populations under stress, monitor health status, and conduct environmental monitoring."
401,Propensity scores as a novel method to guide sample allocation and minimize batch effects during the design of high throughput experiments,"Background
We developed a novel approach to minimize batch effects when assigning samples to batches. Our algorithm selects a batch allocation, among all possible ways of assigning samples to batches, that minimizes differences in average propensity score between batches. This strategy was compared to randomization and stratified randomization in a case–control study (30 per group) with a covariate (case vs control, represented as β1, set to be null) and two biologically relevant confounding variables (age, represented as β2, and hemoglobin  A1c (HbA1c), represented as β3). Gene expression values were obtained from a publicly available dataset of expression data obtained from pancreas islet cells. Batch effects were simulated as twice the median biological variation across the gene expression dataset and were added to the publicly available dataset to simulate a batch effect condition. Bias was calculated as the absolute difference between observed betas under the batch allocation strategies and the true beta (no batch effects). Bias was also evaluated after adjustment for batch effects using ComBat as well as a linear regression model. In order to understand performance of our optimal allocation strategy under the alternative hypothesis, we also evaluated bias at a single gene associated with both age and HbA1c levels in the ‘true’ dataset (CAPN13 gene).

Results
Pre-batch correction, under the null hypothesis (β1), maximum absolute bias and root mean square (RMS) of maximum absolute bias, were minimized using the optimal allocation strategy. Under the alternative hypothesis (β2 and β3 for the CAPN13 gene), maximum absolute bias and RMS of maximum absolute bias were also consistently lower using the optimal allocation strategy. ComBat and the regression batch adjustment methods performed well as the bias estimates moved towards the true values in all conditions under both the null and alternative hypotheses. Although the differences between methods were less pronounced following batch correction, estimates of bias (average and RMS) were consistently lower using the optimal allocation strategy under both the null and alternative hypotheses.

Conclusions
Our algorithm provides an extremely flexible and effective method for assigning samples to batches by exploiting knowledge of covariates prior to sample allocation."
402,EG-TransUNet: a transformer-based U-Net with enhanced and guided models for biomedical image segmentation,"Although various methods based on convolutional neural networks have improved the performance of biomedical image segmentation to meet the precision requirements of medical imaging segmentation task, medical image segmentation methods based on deep learning still need to solve the following problems: (1) Difficulty in extracting the discriminative feature of the lesion region in medical images during the encoding process due to variable sizes and shapes; (2) difficulty in fusing spatial and semantic information of the lesion region effectively during the decoding process due to redundant information and the semantic gap. In this paper, we used the attention-based Transformer during the encoder and decoder stages to improve feature discrimination at the level of spatial detail and semantic location by its multihead-based self-attention. In conclusion, we propose an architecture called EG-TransUNet, including three modules improved by a transformer: progressive enhancement module, channel spatial attention, and semantic guidance attention. The proposed EG-TransUNet architecture allowed us to capture object variabilities with improved results on different biomedical datasets. EG-TransUNet outperformed other methods on two popular colonoscopy datasets (Kvasir-SEG and CVC-ClinicDB) by achieving 93.44% and 95.26% on mDice. Extensive experiments and visualization results demonstrate that our method advances the performance on five medical segmentation datasets with better generalization ability."
403,Gene regulation network inference using k-nearest neighbor-based mutual information estimation: revisiting an old DREAM,"Background
A cell exhibits a variety of responses to internal and external cues. These responses are possible, in part, due to the presence of an elaborate gene regulatory network (GRN) in every single cell. In the past 20 years, many groups worked on reconstructing the topological structure of GRNs from large-scale gene expression data using a variety of inference algorithms. Insights gained about participating players in GRNs may ultimately lead to therapeutic benefits. Mutual information (MI) is a widely used metric within this inference/reconstruction pipeline as it can detect any correlation (linear and non-linear) between any number of variables (
n
-dimensions). However, the use of MI with continuous data (for example, normalized fluorescence intensity measurement of gene expression levels) is sensitive to data size, correlation strength and underlying distributions, and often requires laborious and, at times, ad hoc optimization.
Results
In this work, we first show that estimating MI of a bi- and tri-variate Gaussian distribution using 
k
-nearest neighbor (kNN) MI estimation results in significant error reduction as compared to commonly used methods based on fixed binning. Second, we demonstrate that implementing the MI-based kNN Kraskov–Stoögbauer–Grassberger (KSG) algorithm leads to a significant improvement in GRN reconstruction for popular inference algorithms, such as Context Likelihood of Relatedness (CLR). Finally, through extensive in-silico benchmarking we show that a new inference algorithm CMIA (Conditional Mutual Information Augmentation), inspired by CLR, in combination with the KSG-MI estimator, outperforms commonly used methods.
Conclusions
Using three canonical datasets containing 15 synthetic networks, the newly developed method for GRN reconstruction—which combines CMIA, and the KSG-MI estimator—achieves an improvement of 20–35% in precision-recall measures over the current gold standard in the field. This new method will enable researchers to discover new gene interactions or better choose gene candidates for experimental validations."
404,scEvoNet: a gradient boosting-based method for prediction of cell state evolution,"Background
Exploring the function or the developmental history of cells in various organisms provides insights into a given cell type's core molecular characteristics and putative evolutionary mechanisms. Numerous computational methods now exist for analyzing single-cell data and identifying cell states. These methods mostly rely on the expression of genes considered as markers for a given cell state. Yet, there is a lack of scRNA-seq computational tools to study the evolution of cell states, particularly how cell states change their molecular profiles. This can include novel gene activation or the novel deployment of programs already existing in other cell types, known as co-option.
Results
Here we present scEvoNet, a Python tool for predicting cell type evolution in cross-species or cancer-related scRNA-seq datasets. ScEvoNet builds the confusion matrix of cell states and a bipartite network connecting genes and cell states. It allows a user to obtain a set of genes shared by the characteristic signature of two cell states even between distantly-related datasets. These genes can be used as indicators of either evolutionary divergence or co-option occurring during organism or tumor evolution. Our results on cancer and developmental datasets indicate that scEvoNet is a helpful tool for the initial screening of such genes as well as for measuring cell state similarities.
Conclusion
The scEvoNet package is implemented in Python and is freely available from 
https://github.com/monsoro/scEvoNet
. Utilizing this framework and exploring the continuum of transcriptome states between developmental stages and species will help explain cell state dynamics."
405,coda4microbiome: compositional data analysis for microbiome cross-sectional and longitudinal studies,"Background
One of the main challenges of microbiome analysis is its compositional nature that if ignored can lead to spurious results. Addressing the compositional structure of microbiome data is particularly critical in longitudinal studies where abundances measured at different times can correspond to different sub-compositions.
Results
We developed 
coda4microbiome
, a new R package for analyzing microbiome data within the Compositional Data Analysis (CoDA) framework in both, cross-sectional and longitudinal studies. The aim of 
coda4microbiome
 is prediction, more specifically, the method is designed to identify a model (microbial signature) containing the minimum number of features with the maximum predictive power. The algorithm relies on the analysis of log-ratios between pairs of components and variable selection is addressed through penalized regression on the “all-pairs log-ratio model”, the model containing all possible pairwise log-ratios. For longitudinal data, the algorithm infers dynamic microbial signatures by performing penalized regression over the summary of the log-ratio trajectories (the area under these trajectories). In both, cross-sectional and longitudinal studies, the inferred microbial signature is expressed as the (weighted) balance between two groups of taxa, those that contribute positively to the microbial signature and those that contribute negatively. The package provides several graphical representations that facilitate the interpretation of the analysis and the identified microbial signatures. We illustrate the new method with data from a Crohn's disease study (cross-sectional data) and on the developing microbiome of infants (longitudinal data).
Conclusions
coda4microbiome
 is a new algorithm for identification of microbial signatures in both, cross-sectional and longitudinal studies. The algorithm is implemented as an R package that is available at CRAN (
https://cran.r-project.org/web/packages/coda4microbiome/
) and is accompanied with a vignette with a detailed description of the functions. The website of the project contains several tutorials: 
https://malucalle.github.io/coda4microbiome/"
406,Establishment of a prognostic signature for lung adenocarcinoma using cuproptosis-related lncRNAs,"Objective
To establish a prognostic signature for lung adenocarcinoma (LUAD) based on cuproptosis-related long non-coding RNAs (lncRNAs), and to study the immune-related functions of LUAD.
Methods
First, transcriptome data and clinical data related to LUAD were downloaded from the Cancer Genome Atlas (TCGA), and cuproptosis-related genes were analyzed to identify cuproptosis-related lncRNAs. Univariate COX analysis, least absolute shrinkage and selection operator (LASSO) analysis, and multivariate COX analysis were performed to analyze the cuproptosis-related lncRNAs, and a prognostic signature was established. Second, univariate COX analysis and multivariate COX analysis were performed for independent prognostic analyses. Receiver operating characteristic (ROC) curves, C index, survival curve, nomogram, and principal component analysis (PCA) were performed to evaluate the results of the independent prognostic analyses. Finally, gene enrichment analyses and immune-related function analyses were also carried out.
Results
(1) A total of 1,297 cuproptosis-related lncRNAs were screened. (2) A LUAD prognostic signature containing 13 cuproptosis-related lncRNAs was constructed (NIFK-AS1, AC026355.2, SEPSECS-AS1, AL360270.1, AC010999.2, ABCA9-AS1, AC032011.1, AL162632.3, LINC02518, LINC0059, AL031600.2, AP000346.1, AC012409.4). (3) The area under the multi-indicator ROC curves at 1, 3, and 5 years were AUC1 = 0.742, AUC2 = 0.708, and AUC3 = 0.762, respectively. The risk score of the prognostic signature could be used as an independent prognostic factor that was independent of other clinical indicators. (4) The results of gene enrichment analyses showed that 13 biomarkers were primarily related to amoebiasis, the wnt signaling pathway, hematopoietic cell lineage. The ssGSEA volcano map showed significant differences between high- and low-risk groups in immune-related functions, such as human leukocyte antigen (HLA), Type_II_IFN_Reponse, MHC_class_I, and Parainflammation (
P
 < 0.001).
Conclusions
Thirteen cuproptosis-related lncRNAs may be clinical molecular biomarkers for the prognosis of LUAD."
407,INSnet: a method for detecting insertions based on deep learning network,"Background
Many studies have shown that structural variations (SVs) strongly impact human disease. As a common type of SV, insertions are usually associated with genetic diseases. Therefore, accurately detecting insertions is of great significance. Although many methods for detecting insertions have been proposed, these methods often generate some errors and miss some variants. Hence, accurately detecting insertions remains a challenging task.
Results
In this paper, we propose a method named INSnet to detect insertions using a deep learning network. First, INSnet divides the reference genome into continuous sub-regions and takes five features for each locus through alignments between long reads and the reference genome. Next, INSnet uses a depthwise separable convolutional network. The convolution operation extracts informative features through spatial information and channel information. INSnet uses two attention mechanisms, the convolutional block attention module (CBAM) and efficient channel attention (ECA) to extract key alignment features in each sub-region. In order to capture the relationship between adjacent subregions, INSnet uses a gated recurrent unit (GRU) network to further extract more important SV signatures. After predicting whether a sub-region contains an insertion through the previous steps, INSnet determines the precise site and length of the insertion. The source code is available from GitHub at 
https://github.com/eioyuou/INSnet
.
Conclusion
Experimental results show that INSnet can achieve better performance than other methods in terms of F1 score on real datasets."
408,RGT: a toolbox for the integrative analysis of high throughput regulatory genomics data,"Background
Massive amounts of data are produced by combining next-generation sequencing with complex biochemistry techniques to characterize regulatory genomics profiles, such as protein–DNA interaction and chromatin accessibility. Interpretation of such high-throughput data typically requires different computation methods. However, existing tools are usually developed for a specific task, which makes it challenging to analyze the data in an integrative manner.
Results
We here describe the Regulatory Genomics Toolbox (RGT), a computational library for the integrative analysis of regulatory genomics data. RGT provides different functionalities to handle genomic signals and regions. Based on that, we developed several tools to perform distinct downstream analyses, including the prediction of transcription factor binding sites using ATAC-seq data, identification of differential peaks from ChIP-seq data, and detection of triple helix mediated RNA and DNA interactions, visualization, and finding an association between distinct regulatory factors.
Conclusion
We present here RGT; a framework to facilitate the customization of computational methods to analyze genomic data for specific regulatory genomics problems. RGT is a comprehensive and flexible Python package for analyzing high throughput regulatory genomics data and is available at: 
https://github.com/CostaLab/reg-gen
. The documentation is available at: 
https://reg-gen.readthedocs.io"
409,kegg_pull: a software package for the RESTful access and pulling from the Kyoto Encyclopedia of Gene and Genomes,"Background
The Kyoto Encyclopedia of Genes and Genomes (KEGG) provides organized genomic, biomolecular, and metabolic information and knowledge that is reasonably current and highly useful for a wide range of analyses and modeling. KEGG follows the principles of data stewardship to be findable, accessible, interoperable, and reusable (FAIR) by providing RESTful access to their database entries via their web-accessible KEGG API. However, the overall FAIRness of KEGG is often limited by the library and software package support available in a given programming language. While R library support for KEGG is fairly strong, Python library support has been lacking. Moreover, there is no software that provides extensive command line level support for KEGG access and utilization.
Results
We present kegg_pull, a package implemented in the Python programming language that provides better KEGG access and utilization functionality than previous libraries and software packages. Not only does kegg_pull include an application programming interface (API) for Python programming, it also provides a command line interface (CLI) that enables utilization of KEGG for a wide range of shell scripting and data analysis pipeline use-cases. As kegg_pull’s name implies, both the API and CLI provide versatile options for pulling (downloading and saving) an arbitrary (user defined) number of database entries from the KEGG API. Moreover, this functionality is implemented to efficiently utilize multiple central processing unit cores as demonstrated in several performance tests. Many options are provided to optimize fault-tolerant performance across a single or multiple processes, with recommendations provided based on extensive testing and practical network considerations.
Conclusions
The new kegg_pull package enables new flexible KEGG retrieval use cases not available in previous software packages. The most notable new feature that kegg_pull provides is its ability to robustly pull an arbitrary number of KEGG entries with a single API method or CLI command, including pulling an entire KEGG database. We provide recommendations to users for the most effective use of kegg_pull according to their network and computational circumstances."
410,dbGaPCheckup: pre-submission checks of dbGaP-formatted subject phenotype files,"Background
Data archiving and distribution are essential to scientific rigor and reproducibility of research. The National Center for Biotechnology Information’s Database of Genotypes and Phenotypes (dbGaP) is a public repository for scientific data sharing. To support curation of thousands of complex data sets, dbGaP has detailed submission instructions that investigators must follow when archiving their data.
Results
We developed dbGaPCheckup, an R package which implements a series of check, awareness, reporting, and utility functions to support data integrity and proper formatting of the subject phenotype data set and data dictionary prior to dbGaP submission. For example, as a tool, dbGaPCheckup ensures that the data dictionary contains all fields required by dbGaP, and additional fields required by dbGaPCheckup; the number and names of variables match between the data set and data dictionary; there are no duplicated variable names or descriptions; observed data values are not more extreme than the logical minimum and maximum values stated in the data dictionary; and more. The package also includes functions that implement a series of minor/scalable fixes when errors are detected (e.g., a function to reorder the variables in the data dictionary to match the order listed in the data set). Finally, we also include reporting functions that produce graphical and textual descriptives of the data to further reduce the likelihood of data integrity issues. The dbGaPCheckup R package is available on CRAN (
https://CRAN.R-project.org/package=dbGaPCheckup
) and developed on GitHub (
https://github.com/lwheinsberg/dbGaPCheckup
).
Conclusion
dbGaPCheckup is an innovative assistive and timesaving tool that fills an important gap for researchers by making dbGaP submission of large and complex data sets less error prone."
411,Integrated analysis identifies oxidative stress-related lncRNAs associated with progression and prognosis in colorectal cancer,"Background
Colorectal cancer (CRC) is one of the most common cancers in the world. Oxidative stress reactions have been reportedly associated with oncogenesis and tumor progression. By analyzing mRNA expression data and clinical information from The Cancer Genome Atlas (TCGA), we aimed to construct an oxidative stress-related long noncoding RNA (lncRNA) risk model and identify oxidative stress-related biomarkers to improve the prognosis and treatment of CRC.
Results
Differentially expressed oxidative stress-related genes (DEOSGs) and oxidative stress-related lncRNAs were identified by using bioinformatics tools. An oxidative stress-related lncRNA risk model was constructed based on 9 lncRNAs (
AC034213.1, AC008124.1, LINC01836, USP30-AS1, AP003555.1, AC083906.3, AC008494.3, AC009549.1,
 and 
AP006621.3
) by least absolute shrinkage and selection operator (LASSO) analysis. The patients were then divided into high- and low-risk groups based on the median risk score. The high-risk group had a significantly worse overall survival (OS) (
p
 < 0.001). Receiver operating characteristic (ROC) and calibration curves displayed the favorable predictive performance of the risk model. The nomogram successfully quantified the contribution of each metric to survival, and the concordance index and calibration plots demonstrated its excellent predictive capacity. Notably, different risk subgroups showed significant differences in terms of their metabolic activity, mutation landscape, immune microenvironment and drug sensitivity. Specifically, differences in the immune microenvironment implied that CRC patients in certain subgroups might be more responsive to immune checkpoint inhibitors.
Conclusions
Oxidative stress-related lncRNAs can predict the prognosis of CRC patients, which provides new insight for future immunotherapies based on potential oxidative stress targets."
412,Which data subset should be augmented for deep learning? a simulation study using urothelial cell carcinoma histopathology images,"Background
Applying deep learning to digital histopathology is hindered by the scarcity of manually annotated datasets. While data augmentation can ameliorate this obstacle, its methods are far from standardized. Our aim was to systematically explore the effects of skipping data augmentation; applying data augmentation to different subsets of the whole dataset (training set, validation set, test set, two of them, or all of them); and applying data augmentation at different time points (before, during, or after dividing the dataset into three subsets). Different combinations of the above possibilities resulted in 11 ways to apply augmentation. The literature contains no such comprehensive systematic comparison of these augmentation ways.
Results
Non-overlapping photographs of all tissues on 90 hematoxylin-and-eosin-stained urinary bladder slides were obtained. Then, they were manually classified as either inflammation (5948 images), urothelial cell carcinoma (5811 images), or invalid (3132 images; excluded). If done, augmentation was eight-fold by flipping and rotation. Four convolutional neural networks (Inception-v3, ResNet-101, GoogLeNet, and SqueezeNet), pre-trained on the ImageNet dataset, were fine-tuned to binary classify images of our dataset. This task was the benchmark for our experiments. Model testing performance was evaluated using accuracy, sensitivity, specificity, and area under the receiver operating characteristic curve. Model validation accuracy was also estimated. The best testing performance was achieved when augmentation was done to the remaining data after test-set separation, but before division into training and validation sets. This leaked information between the training and the validation sets, as evidenced by the optimistic validation accuracy. However, this leakage did not cause the validation set to malfunction. Augmentation before test-set separation led to optimistic results. Test-set augmentation yielded more accurate evaluation metrics with less uncertainty. Inception-v3 had the best overall testing performance.
Conclusions
In digital histopathology, augmentation should include both the test set (after its allocation), and the remaining combined training/validation set (before being split into separate training and validation sets). Future research should try to generalize our results."
413,Reverse engineering environmental metatranscriptomes clarifies best practices for eukaryotic assembly,"Background
Diverse communities of microbial eukaryotes in the global ocean provide a variety of essential ecosystem services, from primary production and carbon flow through trophic transfer to cooperation via symbioses. Increasingly, these communities are being understood through the lens of omics tools, which enable high-throughput processing of diverse communities. Metatranscriptomics offers an understanding of near real-time gene expression in microbial eukaryotic communities, providing a window into community metabolic activity.
Results
Here we present a workflow for eukaryotic metatranscriptome assembly, and validate the ability of the pipeline to recapitulate real and manufactured eukaryotic community-level expression data. We also include an open-source tool for simulating environmental metatranscriptomes for testing and validation purposes. We reanalyze previously published metatranscriptomic datasets using our metatranscriptome analysis approach.
Conclusion
We determined that a multi-assembler approach improves eukaryotic metatranscriptome assembly based on recapitulated taxonomic and functional annotations from an in-silico mock community. The systematic validation of metatranscriptome assembly and annotation methods provided here is a necessary step to assess the fidelity of our community composition measurements and functional content assignments from eukaryotic metatranscriptomes."
414,DeltaMSI: artificial intelligence-based modeling of microsatellite instability scoring on next-generation sequencing data,"Background
DNA mismatch repair deficiency (dMMR) testing is crucial for detection of microsatellite unstable (MSI) tumors. MSI is detected by aberrant indel length distributions of microsatellite markers, either by visual inspection of PCR-fragment length profiles or by automated bioinformatic scoring on next-generation sequencing (NGS) data. The former is time-consuming and low-throughput while the latter typically relies on simplified binary scoring of a single parameter of the indel distribution. The purpose of this study was to use machine learning to process the full complexity of indel distributions and integrate it into a robust script for screening of dMMR on small gene panel-based NGS data of clinical tumor samples without paired normal tissue.
Methods
Scikit-learn was used to train 7 models on normalized read depth data of 36 microsatellite loci in a cohort of 133 MMR proficient (pMMR) and 46 dMMR tumor samples, taking loss of MLH1/MSH2/PMS2/MSH6 protein expression as reference method. After selection of the optimal model and microsatellite panel the two top-performing models per locus (logistic regression and support vector machine) were integrated into a novel script (DeltaMSI) for combined prediction of MSI status on 28 marker loci at sample level. Diagnostic performance of DeltaMSI was compared to that of mSINGS, a widely used script for MSI detection on unpaired tumor samples. The robustness of DeltaMSI was evaluated on 1072 unselected, consecutive solid tumor samples in a real-world setting sequenced using capture chemistry, and 116 solid tumor samples sequenced by amplicon chemistry. Likelihood ratios were used to select result intervals with clinical validity.
Results
DeltaMSI achieved higher robustness at equal diagnostic power (AUC = 0.950; 95% CI 0.910–0.975) as compared to mSINGS (AUC = 0.876; 95% CI 0.823–0.918). Its sensitivity of 90% at 100% specificity indicated its clinical potential for high-throughput MSI screening in all tumor types.
Clinical Trial Number/IRB
 B1172020000040, Ethical Committee, AZ Delta General Hospital."
415,Gene expression variability across cells and species shapes the relationship between renal resident macrophages and infiltrated macrophages,"Background
Two main subclasses of macrophages are found in almost all solid tissues: embryo-derived resident tissue macrophages and bone marrow-derived infiltrated macrophages. These macrophage subtypes show transcriptional and functional divergence, and the programs that have shaped the evolution of renal macrophages and related signaling pathways remain poorly understood. To clarify these processes, we performed data analysis based on single-cell transcriptional profiling of renal tissue-resident and infiltrated macrophages in human, mouse and rat.
Results
In this study, we (i) characterized the transcriptional divergence among species and (ii) illustrated variability in expression among cells of each subtype and (iii) compared the gene regulation network and (iv) ligand-receptor pairs in human and mouse. Using single-cell transcriptomics, we mapped the promoter architecture during homeostasis.
Conclusions
Transcriptionally divergent genes, such as the differentially TF-encoding genes expressed in resident and infiltrated macrophages across the three species, vary among cells and include distinct promoter structures. The gene regulatory network in infiltrated macrophages shows comparatively better species-wide consistency than resident macrophages. The conserved transcriptional gene regulatory network in infiltrated macrophages among species is uniquely enriched in pathways related to kinases, and TFs associated with largely conserved regulons among species are uniquely enriched in kinase-related pathways."
416,Sensbio: an online server for biosensor design,"Allosteric transcription factor (aTF) based biosensors can be used to engineer genetic circuits for a wide range of applications. The literature and online databases contain hundreds of experimentally validated molecule-TF pairs; however, the knowledge is scattered and often incomplete. Additionally, compared to the number of compounds that can be produced in living systems, those with known associated TF-compound interactions are low. For these reasons, new tools that help researchers find new possible TF-ligand pairs are called for. In this work, we present Sensbio, a computational tool that through similarity comparison against a TF-ligand reference database, is able to identify putative transcription factors that can be activated by a given input molecule. In addition to the collection of algorithms, an online application has also been developed, together with a predictive model created to find new possible matches based on machine learning."
417,Reconstructing B cell lineage trees with minimum spanning tree and genotype abundances,"B cell receptor (BCR) genes exposed to an antigen undergo somatic hypermutations and Darwinian antigen selection, generating a large BCR-antibody diversity. This process, known as B cell affinity maturation, increases antibody affinity, forming a specific B cell lineage that includes the unmutated ancestor and mutated variants. In a B cell lineage, cells with a higher antigen affinity will undergo clonal expansion, while those with a lower affinity will not proliferate and probably be eliminated. Therefore, cellular (genotype) abundance provides a valuable perspective on the ongoing evolutionary process. Phylogenetic tree inference is often used to reconstruct B cell lineage trees and represents the evolutionary dynamic of BCR affinity maturation. However, such methods should process B-cell population data derived from experimental sampling that might contain different cellular abundances. There are a few phylogenetic methods for tracing the evolutionary events occurring in B cell lineages; best-performing solutions are time-demanding and restricted to analysing a reduced number of sequences, while time-efficient methods do not consider cellular abundances. We propose ClonalTree, a low-complexity and accurate approach to construct B-cell lineage trees that incorporates genotype abundances into minimum spanning tree (MST) algorithms. Using both simulated and experimental data, we demonstrate that ClonalTree outperforms MST-based algorithms and achieves a comparable performance to a method that explores tree-generating space exhaustively. Furthermore, ClonalTree has a lower running time, being more convenient for building B-cell lineage trees from high-throughput BCR sequencing data, mainly in biomedical applications, where a lower computational time is appreciable. It is hundreds to thousands of times faster than exhaustive approaches, enabling the analysis of a large set of sequences within minutes or seconds and without loss of accuracy. The source code is freely available at github.com/julibinho/ClonalTree."
418,SALON ontology for the formal description of sequence alignments,"Background
Information provided by high-throughput sequencing platforms allows the collection of content-rich data about biological sequences and their context. Sequence alignment is a bioinformatics approach to identifying regions of similarity in DNA, RNA, or protein sequences. However, there is no consensus about the specific common terminology and representation for sequence alignments. Thus, automatically linking the wide existing knowledge about the sequences with the alignments is challenging.
Results
The Sequence Alignment Ontology (SALON) defines a helpful vocabulary for representing and semantically annotating pairwise and multiple sequence alignments. SALON is an OWL 2 ontology that supports automated reasoning for alignments validation and retrieving complementary information from public databases under the Open Linked Data approach. This will reduce the effort needed by scientists to interpret the sequence alignment results.
Conclusions
SALON defines a full range of controlled terminology in the domain of sequence alignments. It can be used as a mediated schema to integrate data from different sources and validate acquired knowledge."
419,ncDENSE: a novel computational method based on a deep learning framework for non-coding RNAs family prediction,"Background
Although research on non-coding RNAs (ncRNAs) is a hot topic in life sciences, the functions of numerous ncRNAs remain unclear. In recent years, researchers have found that ncRNAs of the same family have similar functions, therefore, it is important to accurately predict ncRNAs families to identify their functions. There are several methods available to solve the prediction problem of ncRNAs family, whose main ideas can be divided into two categories, including prediction based on the secondary structure features of ncRNAs, and prediction according to sequence features of ncRNAs. The first type of prediction method requires a complicated process and has a low accuracy in obtaining the secondary structure of ncRNAs, while the second type of method has a simple prediction process and a high accuracy, but there is still room for improvement. The existing methods for ncRNAs family prediction are associated with problems such as complicated prediction processes and low accuracy, in this regard, it is necessary to propose a new method to predict the ncRNAs family more perfectly.
Results
A deep learning model-based method, ncDENSE, was proposed in this study, which predicted ncRNAs families by extracting ncRNAs sequence features. The bases in ncRNAs sequences were encoded by one-hot coding and later fed into an ensemble deep learning model, which contained the dynamic bi-directional gated recurrent unit (Bi-GRU), the dense convolutional network (DenseNet), and the Attention Mechanism (AM). To be specific, dynamic Bi-GRU was used to extract contextual feature information and capture long-term dependencies of ncRNAs sequences. AM was employed to assign different weights to features extracted by Bi-GRU and focused the attention on information with greater weights. Whereas DenseNet was adopted to extract local feature information of ncRNAs sequences and classify them by the full connection layer. According to our results, the ncDENSE method improved the Accuracy, Sensitivity, Precision, F-score, and MCC by 2.08
\(\%\)
, 2.33
\(\%\)
, 2.14
\(\%\)
, 2.16
\(\%\)
, and 2.39
\(\%\)
, respectively, compared with the suboptimal method.
Conclusions
Overall, the ncDENSE method proposed in this paper extracts sequence features of ncRNAs by dynamic Bi-GRU and DenseNet and improves the accuracy in predicting ncRNAs family and other data."
420,"Immunoinformatics-aided design of a new multi-epitope vaccine adjuvanted with domain 4 of pneumolysin against 
Streptococcus pneumoniae
 strains","Background
Streptococcus pneumoniae
 (Pneumococcus) has remained a leading cause of fatal infections such as pneumonia, meningitis, and sepsis. Moreover, this pathogen plays a major role in bacterial co-infection in patients with life-threatening respiratory virus diseases such as influenza and COVID-19. High morbidity and mortality in over one million cases, especially in very young children and the elderly, are the main motivations for pneumococcal vaccine development. Due to the limitations of the currently marketed polysaccharide-based vaccines, non-serotype-specific protein-based vaccines have received wide research interest in recent years. One step further is to identify high antigenic regions within multiple highly-conserved proteins in order to develop peptide vaccines that can affect various stages of pneumococcal infection, providing broader serotype coverage and more effective protection. In this study, immunoinformatics tools were used to design an effective multi-epitope vaccine in order to elicit neutralizing antibodies against multiple strains of pneumococcus.

Results
The B- and T-cell epitopes from highly protective antigens PspA (clades 1–5) and PhtD were predicted and immunodominant peptides were linked to each other with proper linkers. The domain 4 of Ply, as a potential TLR4 agonist adjuvant candidate, was attached to the end of the construct to enhance the immunogenicity of the epitope vaccine. The evaluation of the physicochemical and immunological properties showed that the final construct was stable, soluble, antigenic, and non-allergenic. Furthermore, the protein was found to be acidic and hydrophilic in nature. The protein 3D-structure was built and refined, and the Ramachandran plot, ProSA–web, ERRAT, and Verify3D validated the quality of the final model. Molecular docking analysis showed that the designed construct via Ply domain 4 had a strong interaction with TLR4. The structural stability of the docked complex was confirmed by molecular dynamics. Finally, codon optimization was performed for gene expression in 
E. coli
, followed by in silico cloning in the pET28a(+) vector.

Conclusion
The computational analysis of the construct showed acceptable results, however, the suggested vaccine needs to be experimentally verified in laboratory to ensure its safety and immunogenicity."
421,PredAOT: a computational framework for prediction of acute oral toxicity based on multiple random forest models,"Background
Acute oral toxicity of drug candidates can lead to drug development failure; thus, predicting the acute oral toxicity of small compounds is important for successful drug development. However, evaluation of the acute oral toxicity of small compounds considered in the early stages of drug discovery is limited because of cost and time. Here, we developed a computational framework, PredAOT, that predicts the acute oral toxicity of small compounds in mice and rats.
Methods
PredAOT is based on multiple random forest models for the accurate prediction of acute oral toxicity. A total of 6226 and 6238 compounds evaluated in mice and rats, respectively, were used to train the models.

Results
PredAOT has the advantage of predicting acute oral toxicity in mice and rats simultaneously, and its prediction performance is similar to or better than that of existing tools.
Conclusion
PredAOT will be a useful tool for the quick and accurate prediction of the acute oral toxicity of small compounds in mice and rats during drug development."
422,Designing multi-epitope vaccine against important colorectal cancer (CRC) associated pathogens based on immunoinformatics approach,"Background
It seems that several members of intestinal gut microbiota like 
Streptococcus bovis, Bacteroides fragilis, Helicobacter pylori, Fusobacterium nucleatum, Enterococcus faecalis, Escherichia coli, Peptostreptococcus anaerobius
 may be considered as the causative agents of Colorectal Cancer (CRC). The present study used bioinformatics and immunoinformatics approaches to design a potential epitope-based multi-epitope vaccine to prevent CRC with optimal population coverage.
Methods
In this study, ten amino acid sequences of CRC-related pathogens were retrieved from the NCBI database. Three ABCpred, BCPREDS and LBtope online servers were considered for B cells prediction and the IEDB server for T cells (CD4
+
 and CD8
+
) prediction. Then, validation, allergenicity, toxicity and physicochemical analysis of all sequences were performed using web servers. A total of three linkers, AAY, GPGPG, and KK were used to bind CTL, HTL and BCL epitopes, respectively. In addition, the final construct was subjected to disulfide engineering, molecular docking, immune simulation and codon adaptation to design an effective vaccine production strategy.

Results
A total of 19 sequences of different lengths for linear B-cell epitopes, 19 and 18 sequences were considered as epitopes of CD4
+
 T and CD8
+
 cells, respectively. The predicted epitopes were joined by appropriate linkers because they play an important role in producing an extended conformation and protein folding. The final multi-epitope construct and Toll-like receptor 4 (TLR4) were evaluated by molecular docking, which revealed stable and strong binding interactions. Immunity simulation of the vaccine showed significantly high levels of immunoglobulins, helper T cells, cytotoxic T cells and INF-γ.
Conclusion
Finally, the results showed that the designed multi-epitope vaccine could serve as an excellent prophylactic candidate against CRC-associated pathogens, but in vitro and animal studies are needed to justify our findings for its use as a possible preventive measure."
423,"Does multi-way, long-range chromatin contact data advance 3D genome reconstruction?","Background
Methods for inferring the three-dimensional (3D) configuration of chromatin from conformation capture assays that provide strictly pairwise interactions, notably Hi-C, utilize the attendant contact matrix as input. More recent assays, in particular split-pool recognition of interactions by tag extension (SPRITE), capture multi-way interactions instead of solely pairwise contacts. These assays yield contacts that straddle appreciably greater genomic distances than Hi-C, in addition to instances of exceptionally high-order chromatin interaction. Such attributes are anticipated to be consequential with respect to 3D genome reconstruction, a task yet to be undertaken with multi-way contact data. However, performing such 3D reconstruction using distance-based reconstruction techniques requires framing multi-way contacts as (pairwise) distances. Comparing approaches for so doing, and assessing the resultant impact of long-range and multi-way contacts, are the objectives of this study.
Results
We obtained 3D reconstructions via multi-dimensional scaling under a variety of weighting schemes for mapping SPRITE multi-way contacts to pairwise distances. Resultant configurations were compared following Procrustes alignment and relationships were assessed between associated Procrustes root mean square errors and key features such as the extent of multi-way and/or long-range contacts. We found that these features had surprisingly limited influence on 3D reconstruction, a finding we attribute to their influence being diminished by the preponderance of pairwise contacts.
Conclusion
Distance-based 3D genome reconstruction using SPRITE multi-way contact data is not appreciably affected by the weighting scheme used to convert multi-way interactions to pairwise distances."
424,"Immunoinformatics design of multi-epitope vaccine using OmpA, OmpD and enterotoxin against non-typhoidal salmonellosis","Background
Non-typhoidal 
Salmonella
 (NTS) is one of the important bacteria that cause foodborne diseases and invasive infections in children and elderly people. Since NTS infection is difficult to control due to the emergence of antibiotic-resistant species and its adverse effect on immune response, the development of a vaccine against NTS would be necessary. This study aimed to develop a multi-epitope vaccine against the most prevalent serovars of NTS (
Salmonella
 Typhimurium, 
Salmonella
 Enteritidis) using an immunoinformatics approach and targeting OmpA, OmpD, and enterotoxin (Stn).

Results
Initially, the B cell and T cell epitopes were predicted. Then, epitopes and suitable adjuvant were assembled by molecular linkers to construct a multi-epitope vaccine. The computational tools predicted the tertiary structure, refined the tertiary structure and validated the final vaccine construct. The effectiveness of the vaccine was evaluated via molecular docking, molecular dynamics simulation, and in silico immune simulation. The vaccine model had good binding affinity and stability with MHC-I, MHC-II, and toll-like receptors (TLR-1, 2, 4) as well as activation of T cells, IgM, IgG, IFN-
\(\gamma\)
 and IL-2 responses. Furthermore, after codon optimization of the vaccine sequence, this sequence was cloned in 
E. coli
 plasmid vector pET-30a (+) within restriction sites of HindIII and BamHI.
Conclusions
This study, for the first time, introduced a multi-epitope vaccine based on OmpA, OmpD and enterotoxin (Stn) of NTS that could stimulate T and B cell immune responses and produced in the prokaryotic system. This vaccine was validated in-silico phase which is an essential study to reduce challenges before in vitro and in vivo studies.
Graphical abstract"
425,ITDetect: a method to detect internal tandem duplication of FMS-like tyrosine kinase (FLT3) from next-generation sequencing data with high sensitivity and clinical application,"Internal tandem duplication (ITD) of the FMS-like tyrosine kinase (FLT3) gene is associated with poor clinical outcomes in patients with acute myeloid leukemia. Although recent methods for detecting FLT3-ITD from next-generation sequencing (NGS) data have replaced traditional ITD detection approaches such as conventional PCR or fragment analysis, their use in the clinical field is still limited and requires further information. Here, we introduce ITDetect, an efficient FLT3-ITD detection approach that uses NGS data. Our proposed method allows for more precise detection and provides more detailed information than existing in silico methods. Further, it enables FLT3-ITD detection from exome sequencing or targeted panel sequencing data, thereby improving its clinical application. We validated the performance of ITDetect using NGS-based and experimental ITD detection methods and successfully demonstrated that ITDetect provides the highest concordance with the experimental methods. The program and data underlying this study are available in a public repository."
426,Latent dirichlet allocation for double clustering (LDA-DC): discovering patients phenotypes and cell populations within a single Bayesian framework,"Background
Current clinical routines rely more and more on “omics” data such as flow cytometry data from host and microbiota. Cohorts variability in addition to patients’ heterogeneity and huge dimensions make it difficult to understand underlying structure of the data and decipher pathologies. Patients stratification and diagnostics from such complex data are extremely challenging. There is an acute need to develop novel statistical machine learning methods that are robust with respect to the data heterogeneity, efficient from the computational viewpoint, and can be understood by human experts.
Results
We propose a novel approach to stratify cell-based observations within a single probabilistic framework, i.e., to extract meaningful phenotypes from both patients and cells data simultaneously. We define this problem as a double clustering problem that we tackle with the proposed approach. Our method is a practical extension of the Latent Dirichlet Allocation and is used for the Double Clustering task (LDA-DC). We first validate the method on artificial datasets, then we apply our method to two real problems of patients stratification based on cytometry and microbiota data. We observe that the LDA-DC returns clusters of patients and also clusters of cells related to patients’ conditions. We also construct a graphical representation of the results that can be easily understood by humans and are, therefore, of a big help for experts involved in pre-clinical research."
427,"PriPath: identifying dysregulated pathways from differential gene expression via grouping, scoring, and modeling with an embedded feature selection approach","Background
Cell homeostasis relies on the concerted actions of genes, and dysregulated genes can lead to diseases. In living organisms, genes or their products do not act alone but within networks. Subsets of these networks can be viewed as modules that provide specific functionality to an organism. The Kyoto encyclopedia of genes and genomes (KEGG) systematically analyzes gene functions, proteins, and molecules and combines them into pathways. Measurements of gene expression (e.g., RNA-seq data) can be mapped to KEGG pathways to determine which modules are affected or dysregulated in the disease. However, genes acting in multiple pathways and other inherent issues complicate such analyses. Many current approaches may only employ gene expression data and need to pay more attention to some of the existing knowledge stored in KEGG pathways for detecting dysregulated pathways. New methods that consider more precompiled information are required for a more holistic association between gene expression and diseases.

Results
PriPath is a novel approach that transfers the generic process of grouping and scoring, followed by modeling to analyze gene expression with KEGG pathways. In PriPath, KEGG pathways are utilized as the grouping function as part of a machine learning algorithm for selecting the most significant KEGG pathways. A machine learning model is trained to differentiate between diseases and controls using those groups. We have tested PriPath on 13 gene expression datasets of various cancers and other diseases. Our proposed approach successfully assigned biologically and clinically relevant KEGG terms to the samples based on the differentially expressed genes. We have comparatively evaluated the performance of PriPath against other tools, which are similar in their merit. For each dataset, we manually confirmed the top results of PriPath in the literature and found that most predictions can be supported by previous experimental research.
Conclusions
PriPath can thus aid in determining dysregulated pathways, which applies to medical diagnostics. In the future, we aim to advance this approach so that it can perform patient stratification based on gene expression and identify druggable targets. Thereby, we cover two aspects of precision medicine."
428,Normalized L3-based link prediction in protein–protein interaction networks,"Background
Protein–protein interaction (PPI) data is an important type of data used in functional genomics. However, high-throughput experiments are often insufficient to complete the PPI interactome of different organisms. Computational techniques are thus used to infer missing data, with link prediction being one such approach that uses the structure of the network of PPIs known so far to identify non-edges whose addition to the network would make it more sound, according to some underlying assumptions. Recently, a new idea called the 
L3 principle
 introduced biological motivation into PPI link predictions, yielding predictors that are superior to general-purpose link predictors for complex networks. Interestingly, the L3 principle can be interpreted in another way, so that other signatures of PPI networks can also be characterized for PPI predictions. This alternative interpretation uncovers candidate PPIs that the current L3-based link predictors may not be able to fully capture, underutilizing the L3 principle.
Results
In this article, we propose a formulation of link predictors that we call 
NormalizedL3
 (
L3N
) which addresses certain missing elements within L3 predictors in the perspective of network modeling. Our computational validations show that the L3N predictors are able to find missing PPIs more accurately (in terms of true positives among the predicted PPIs) than the previously proposed methods on several datasets from the literature, including BioGRID, STRING, MINT, and HuRI, at the cost of using more computation time in some of the cases. In addition, we found that L3-based link predictors (including L3N) ranked a different pool of PPIs higher than the general-purpose link predictors did. This suggests that different types of PPIs can be predicted based on different topological assumptions, and that even better PPI link predictors may be obtained in the future by improved network modeling."
429,LuxHMM: DNA methylation analysis with genome segmentation via hidden Markov model,"Background
DNA methylation plays an important role in studying the epigenetics of various biological processes including many diseases. Although differential methylation of individual cytosines can be informative, given that methylation of neighboring CpGs are typically correlated, analysis of differentially methylated regions is often of more interest.
Results
We have developed a probabilistic method and software, LuxHMM, that uses hidden Markov model (HMM) to segment the genome into regions and a Bayesian regression model, which allows handling of multiple covariates, to infer differential methylation of regions. Moreover, our model includes experimental parameters that describe the underlying biochemistry in bisulfite sequencing and model inference is done using either variational inference for efficient genome-scale analysis or Hamiltonian Monte Carlo (HMC).
Conclusions
Analyses of real and simulated bisulfite sequencing data demonstrate the competitive performance of LuxHMM compared with other published differential methylation analysis methods."
430,A generalized covariate-adjusted top-scoring pair algorithm with applications to diabetic kidney disease stage classification in the Chronic Renal Insufficiency Cohort (CRIC) Study,"Background
The growing amount of high dimensional biomolecular data has spawned new statistical and computational models for risk prediction and disease classification. Yet, many of these methods do not yield biologically interpretable models, despite offering high classification accuracy. An exception, the top-scoring pair (TSP) algorithm derives parameter-free, biologically interpretable single pair decision rules that are accurate and robust in disease classification. However, standard TSP methods do not accommodate covariates that could heavily influence feature selection for the top-scoring pair. Herein, we propose a covariate-adjusted TSP method, which uses residuals from a regression of features on the covariates for identifying top scoring pairs. We conduct simulations and a data application to investigate our method, and compare it to existing classifiers, LASSO and random forests.
Results
Our simulations found that features that were highly correlated with clinical variables had high likelihood of being selected as top scoring pairs in the standard TSP setting. However, through residualization, our covariate-adjusted TSP was able to identify new top scoring pairs, that were largely uncorrelated with clinical variables. In the data application, using patients with diabetes (n = 977) selected for metabolomic profiling in the Chronic Renal Insufficiency Cohort (CRIC) study, the standard TSP algorithm identified (valine-betaine, dimethyl-arg) as the top-scoring metabolite pair for classifying diabetic kidney disease (DKD) severity, whereas the covariate-adjusted TSP method identified the pair (pipazethate, octaethylene glycol) as top-scoring. Valine-betaine and dimethyl-arg had, respectively, ≥ 0.4 absolute correlation with urine albumin and serum creatinine, known prognosticators of DKD. Thus without covariate-adjustment the top-scoring pair largely reflected known markers of disease severity, whereas covariate-adjusted TSP uncovered features liberated from confounding, and identified independent prognostic markers of DKD severity. Furthermore, TSP-based methods achieved competitive classification accuracy in DKD to LASSO and random forests, while providing more parsimonious models.
Conclusions
We extended TSP-based methods to account for covariates, via a simple, easy to implement residualizing process. Our covariate-adjusted TSP method identified metabolite features, uncorrelated from clinical covariates, that discriminate DKD severity stage based on the relative ordering between two features, and thus provide insights into future studies on the order reversals in early vs advanced disease states."
431,Deafness gene screening based on a multilevel cascaded BPNN model,"Sudden sensorineural hearing loss is a common and frequently occurring condition in otolaryngology. Existing studies have shown that sudden sensorineural hearing loss is closely associated with mutations in genes for inherited deafness. To identify these genes associated with deafness, researchers have mostly used biological experiments, which are accurate but time-consuming and laborious. In this paper, we proposed a computational method based on machine learning to predict deafness-associated genes. The model is based on several basic backpropagation neural networks (BPNNs), which were cascaded as multiple-level BPNN models. The cascaded BPNN model showed a stronger ability for screening deafness-associated genes than the conventional BPNN. A total of 211 of 214 deafness-associated genes from the deafness variant database (DVD v9.0) were used as positive data, and 2110 genes extracted from chromosomes were used as negative data to train our model. The test achieved a mean AUC higher than 0.98. Furthermore, to illustrate the predictive performance of the model for suspected deafness-associated genes, we analyzed the remaining 17,711 genes in the human genome and screened the 20 genes with the highest scores as highly suspected deafness-associated genes. Among these 20 predicted genes, three genes were mentioned as deafness-associated genes in the literature. The analysis showed that our approach has the potential to screen out highly suspected deafness-associated genes from a large number of genes, and our predictions could be valuable for future research and discovery of deafness-associated genes."
432,A robust and accurate single-cell data trajectory inference method using ensemble pseudotime,"Background
The advance in single-cell RNA sequencing technology has enhanced the analysis of cell development by profiling heterogeneous cells in individual cell resolution. In recent years, many trajectory inference methods have been developed. They have focused on using the graph method to infer the trajectory using single-cell data, and then calculate the geodesic distance as the pseudotime. However, these methods are vulnerable to errors caused by the inferred trajectory. Therefore, the calculated pseudotime suffers from such errors.
Results
 We proposed a novel framework for trajectory inference called the 
s
ingle-
c
ell data 
T
rajectory inference method using 
E
nsemble 
P
seudotime inference (scTEP). scTEP utilizes multiple clustering results to infer robust pseudotime and then uses the pseudotime to fine-tune the learned trajectory. We evaluated the scTEP using 41 real scRNA-seq data sets, all of which had the ground truth development trajectory. We compared the scTEP with state-of-the-art methods using the aforementioned data sets. Experiments on real linear and non-linear data sets demonstrate that our scTEP performed superior on more data sets than any other method. The scTEP also achieved a higher average and lower variance on most metrics than other state-of-the-art methods. In terms of trajectory inference capacity, the scTEP outperforms those methods. In addition, the scTEP is more robust to the unavoidable errors resulting from clustering and dimension reduction.
Conclusion
The scTEP demonstrates that utilizing multiple clustering results for the pseudotime inference procedure enhances its robustness. Furthermore, robust pseudotime strengthens the accuracy of trajectory inference, which is the most crucial component in the pipeline. scTEP is available at 
https://cran.r-project.org/package=scTEP
."
433,LincRNA ZNF529-AS1 inhibits hepatocellular carcinoma via FBXO31 and predicts the prognosis of hepatocellular carcinoma patients,"Background
Invasion and metastasis of hepatocellular carcinoma (HCC) is still an important reason for poor prognosis. LincRNA ZNF529-AS1 is a recently identified tumour-associated molecule that is differentially expressed in a variety of tumours, but its role in HCC is still unclear. This study investigated the expression and function of ZNF529-AS1 in HCC and explored the prognostic significance of ZNF529-AS1 in HCC.
Methods
Based on HCC information in TCGA and other databases, the relationship between the expression of ZNF529-AS1 and clinicopathological characteristics of HCC was analysed by the Wilcoxon signed-rank test and logistic regression. The relationship between ZNF529-AS1 and HCC prognosis was evaluated by Kaplan‒Meier and Cox regression analyses. The cellular function and signalling pathways involved in ZNF529-AS1 were analysed by GO and KEGG enrichment analysis. The relationship between ZNF529-AS1 and immunological signatures in the HCC tumour microenvironment was analysed by the ssGSEA algorithm and CIBERSORT algorithm. HCC cell invasion and migration were investigated by the Transwell assay. Gene and protein expression were detected by PCR and western blot analysis, respectively.
Results
ZNF529-AS1 was differentially expressed in various types of tumours and was highly expressed in HCC. The expression of ZNF529-AS1 was closely correlated with the age, sex, T stage, M stage and pathological grade of HCC patients. Univariate and multivariate analyses showed that ZNF529-AS1 was significantly associated with poor prognosis of HCC patients and could be an independent prognostic indicator of HCC. Immunological analysis showed that the expression of ZNF529-AS1 was correlated with the abundance and immune function of various immune cells. Knockdown of ZNF529-AS1 in HCC cells inhibited cell invasion and migration and inhibited the expression of FBXO31.
Conclusion
ZNF529-AS1 could be a new prognostic marker for HCC. FBXO31 may be the downstream target of ZNF529-AS1 in HCC."
434,Pathogen detection in RNA-seq data with Pathonoia,"Background
Bacterial and viral infections may cause or exacerbate various human diseases and to detect microbes in tissue, one method of choice is RNA sequencing. The detection of specific microbes using RNA sequencing offers good sensitivity and specificity, but untargeted approaches suffer from high false positive rates and a lack of sensitivity for lowly abundant organisms.
Results
We introduce Pathonoia, an algorithm that detects viruses and bacteria in RNA sequencing data with high precision and recall. Pathonoia first applies an established k-mer based method for species identification and then aggregates this evidence over all reads in a sample. In addition, we provide an easy-to-use analysis framework that highlights potential microbe-host interactions by correlating the microbial to the host gene expression. Pathonoia outperforms state-of-the-art methods in microbial detection specificity, both on in silico and real datasets.
Conclusion
Two case studies in human liver and brain show how Pathonoia can support novel hypotheses on microbial infection exacerbating disease. The Python package for Pathonoia sample analysis and a guided analysis Jupyter notebook for bulk RNAseq datasets are available on GitHub."
435,DRaW: prediction of COVID-19 antivirals by deep learning—an objection on using matrix factorization,"Background
Due to the high resource consumption of introducing a new drug, drug repurposing plays an essential role in drug discovery. To do this, researchers examine the current drug-target interaction (DTI) to predict new interactions for the approved drugs. Matrix factorization methods have much attention and utilization in DTIs. However, they suffer from some drawbacks.

Methods
We explain why matrix factorization is not the best for DTI prediction. Then, we propose a deep learning model (DRaW) to predict DTIs without having input data leakage. We compare our model with several matrix factorization methods and a deep model on three COVID-19 datasets. In addition, to ensure the validation of DRaW, we evaluate it on benchmark datasets. Furthermore, as an external validation, we conduct a docking study on the COVID-19 recommended drugs.
Results
In all cases, the results confirm that DRaW outperforms matrix factorization and deep models. The docking results approve the top-ranked recommended drugs for COVID-19.
Conclusions
In this paper, we show that it may not be the best choice to use matrix factorization in the DTI prediction. Matrix factorization methods suffer from some intrinsic issues, e.g., sparsity in the domain of bioinformatics applications and fixed-unchanged size of the matrix-related paradigm. Therefore, we propose an alternative method (DRaW) that uses feature vectors rather than matrix factorization and demonstrates better performance than other famous methods on three COVID-19 and four benchmark datasets."
436,Integrated analysis of the voltage-gated potassium channel-associated gene KCNH2 across cancers,"KCNH2 encodes the human ether-a-go-go-related gene (hERG) potassium channel and is an important repolarization reserve for regulating cardiac electrical activity. Increasing evidence suggests that it is involved in the development of various tumours, yet a thorough analysis of the underlying process has not been performed. Here, we have comprehensively examined the role of KCNH2 in multiple cancers by assessing KCNH2 gene expression, diagnostic and prognostic value, genetic alterations, immune infiltration correlations, RNA modifications, mutations, clinical correlations, interacting proteins, and associated signalling pathways. KCNH2 is differentially expressed in over 30 cancers and has a high diagnostic value for 10 tumours. Survival analysis showed that high expression of KCNH2 was associated with a poor prognosis in glioblastoma multiforme (GBM) and hepatocellular carcinoma (LIHC). Mutations and RNA methylation modifications (especially m6A) of KCNH2 are associated with its expression in multiple tumours. KCNH2 expression is correlated with tumour mutation burden, microsatellite instability, neoantigen load, and mutant-allele tumour heterogeneity. In addition, KCNH2 expression is associated with the tumour immune microenvironment and its immunosuppressive phenotype. KEGG signalling pathway enrichment analysis revealed that KCNH2 and its interacting molecules are involved in a variety of pathways related to carcinogenesis and signal regulation, such as the PI3K/Akt and focal adhesion pathways. Overall, we found that KCNH2 and its interaction molecular are expected to be immune-related biomarkers for cancer diagnosis and prognosis evaluation, and are potential regulatory targets of singalling pathways for tumour development due to their significant role in cancers."
437,petiteFinder: an automated computer vision tool to compute Petite colony frequencies in baker’s yeast,"Background
Mitochondrial respiration is central to cellular and organismal health in eukaryotes. In baker’s yeast, however, respiration is dispensable under fermentation conditions. Because yeast are tolerant of this mitochondrial dysfunction, yeast are widely used by biologists as a model organism to ask a variety of questions about the integrity of mitochondrial respiration. Fortunately, baker’s yeast also display a visually identifiable Petite colony phenotype that indicates when cells are incapable of respiration. Petite colonies are smaller than their Grande (wild-type) counterparts, and their frequency can be used to infer the integrity of mitochondrial respiration in populations of cells. Unfortunately, the computation of Petite colony frequencies currently relies on laborious manual colony counting methods which limit both experimental throughput and reproducibility.
Results
To address these problems, we introduce a deep learning enabled tool, 
petiteFinder
, that increases the throughput of the Petite frequency assay. This automated computer vision tool detects Grande and Petite colonies and computes Petite colony frequencies from scanned images of Petri dishes. It achieves accuracy comparable to human annotation but at up to 100 times the speed and outperforms semi-supervised Grande/Petite colony classification approaches. Combined with the detailed experimental protocols we provide, we believe this study can serve as a foundation to standardize this assay. Finally, we comment on how Petite colony detection as a computer vision problem highlights ongoing difficulties with small object detection in existing object detection architectures.
Conclusion
Colony detection with 
petiteFinder
 results in high accuracy Petite and Grande detection in images in a completely automated fashion. It addresses issues in scalability and reproducibility of the Petite colony assay which currently relies on manual colony counting. By constructing this tool and providing details of experimental conditions, we hope this study will enable larger-scale experiments that rely on Petite colony frequencies to infer mitochondrial function in yeast."
438,ConanVarvar: a versatile tool for the detection of large syndromic copy number variation from whole-genome sequencing data,"Background
A wide range of tools are available for the detection of copy number variants (CNVs) from whole-genome sequencing (WGS) data. However, none of them focus on clinically-relevant CNVs, such as those that are associated with known genetic syndromes. Such variants are often large in size, typically 1–5 Mb, but currently available CNV callers have been developed and benchmarked for the discovery of smaller variants. Thus, the ability of these programs to detect tens of real syndromic CNVs remains largely unknown.
Results
Here we present ConanVarvar, a tool which implements a complete workflow for the targeted analysis of large germline CNVs from WGS data. ConanVarvar comes with an intuitive R Shiny graphical user interface and annotates identified variants with information about 56 associated syndromic conditions. We benchmarked ConanVarvar and four other programs on a dataset containing real and simulated syndromic CNVs larger than 1 Mb. In comparison to other tools, ConanVarvar reports 10–30 times less false-positive variants without compromising sensitivity and is quicker to run, especially on large batches of samples.
Conclusions
ConanVarvar is a useful instrument for primary analysis in disease sequencing studies, where large CNVs could be the cause of disease."
439,Evaluation of a decided sample size in machine learning applications,"Background
An appropriate sample size is essential for obtaining a precise and reliable outcome of a study. In machine learning (ML), studies with inadequate samples suffer from overfitting of data and have a lower probability of producing true effects, while the increment in sample size increases the accuracy of prediction but may not cause a significant change after a certain sample size. Existing statistical approaches using standardized mean difference, effect size, and statistical power for determining sample size are potentially biased due to miscalculations or lack of experimental details. This study aims to design criteria for evaluating sample size in ML studies. We examined the average and grand effect sizes and the performance of five ML methods using simulated datasets and three real datasets to derive the criteria for sample size. We systematically increase the sample size, starting from 16, by randomly sampling and examine the impact of sample size on classifiers’ performance and both effect sizes. Tenfold cross-validation was used to quantify the accuracy.
Results
The results demonstrate that the effect sizes and the classification accuracies increase while the variances in effect sizes shrink with the increment of samples when the datasets have a good discriminative power between two classes. By contrast, indeterminate datasets had poor effect sizes and classification accuracies, which did not improve by increasing sample size in both simulated and real datasets. A good dataset exhibited a significant difference in average and grand effect sizes. We derived two criteria based on the above findings to assess a decided sample size by combining the effect size and the ML accuracy. The sample size is considered suitable when it has appropriate effect sizes (≥ 0.5) and ML accuracy (≥ 80%). After an appropriate sample size, the increment in samples will not benefit as it will not significantly change the effect size and accuracy, thereby resulting in a good cost-benefit ratio.
Conclusion
We believe that these practical criteria can be used as a reference for both the authors and editors to evaluate whether the selected sample size is adequate for a study."
440,A multimodal deep learning model to infer cell-type-specific functional gene networks,"Background
Functional gene networks (FGNs) capture functional relationships among genes that vary across tissues and cell types. Construction of cell-type-specific FGNs enables the understanding of cell-type-specific functional gene relationships and insights into genetic mechanisms of human diseases in disease-relevant cell types. However, most existing FGNs were developed without consideration of specific cell types within tissues.
Results
In this study, we created a multimodal deep learning model (MDLCN) to predict cell-type-specific FGNs in the human brain by integrating single-nuclei gene expression data with global protein interaction networks. We systematically evaluated the prediction performance of the MDLCN and showed its superior performance compared to two baseline models (boosting tree and convolutional neural network). Based on the predicted cell-type-specific FGNs, we observed that cell-type marker genes had a higher level of hubness than non-marker genes in their corresponding cell type. Furthermore, we showed that risk genes underlying autism and Alzheimer’s disease were more strongly connected in disease-relevant cell types, supporting the cellular context of predicted cell-type-specific FGNs.
Conclusions
Our study proposes a powerful deep learning approach (MDLCN) to predict FGNs underlying a diverse set of cell types in human brain. The MDLCN model enhances prediction accuracy of cell-type-specific FGNs compared to single modality convolutional neural network (CNN) and boosting tree models, as shown by higher areas under both receiver operating characteristic (ROC) and precision-recall curves for different levels of independent test datasets. The predicted FGNs also show evidence for the cellular context and distinct topological features (i.e. higher hubness and topological score) of cell-type marker genes. Moreover, we observed stronger modularity among disease-associated risk genes in FGNs of disease-relevant cell types. For example, the strength of connectivity among autism risk genes was stronger in neurons, but risk genes underlying Alzheimer’s disease were more connected in microglia."
441,CausNet: generational orderings based search for optimal Bayesian networks via dynamic programming with parent set constraints,"Background
Finding a globally optimal Bayesian Network using exhaustive search is a problem with super-exponential complexity, which severely restricts the number of variables that can feasibly be included. We implement a dynamic programming based algorithm with built-in dimensionality reduction and parent set identification. This reduces the search space substantially and can be applied to large-dimensional data. We use what we call ‘generational orderings’ based search for optimal networks, which is a novel way to efficiently search the space of possible networks given the possible parent sets. The algorithm supports both continuous and categorical data, as well as continuous, binary and survival outcomes.
Results
We demonstrate the efficacy of our algorithm on both synthetic and real data. In simulations, our algorithm performs better than three state-of-art algorithms that are currently used extensively. We then apply it to an Ovarian Cancer gene expression dataset with 513 genes and a survival outcome. Our algorithm is able to find an optimal network describing the disease pathway consisting of 6 genes leading to the outcome node in just 3.4 min on a personal computer with a 2.3 GHz Intel Core i9 processor with 16 GB RAM.
Conclusions
Our generational orderings based search for optimal networks is both an efficient and highly scalable approach for finding optimal Bayesian Networks and can be applied to 1000 s of variables. Using specifiable parameters—correlation, FDR cutoffs, and in-degree—one can increase or decrease the number of nodes and density of the networks. Availability of two scoring option—BIC and Bge—and implementation for survival outcomes and mixed data types makes our algorithm very suitable for many types of high dimensional data in a variety of fields."
442,A fair experimental comparison of neural network architectures for latent representations of multi-omics for drug response prediction,"Background
Recent years have seen a surge of novel neural network architectures for the integration of multi-omics data for prediction. Most of the architectures include either encoders alone or encoders and decoders, i.e., autoencoders of various sorts, to transform multi-omics data into latent representations. One important parameter is the depth of integration: the point at which the latent representations are computed or merged, which can be either early, intermediate, or late. The literature on integration methods is growing steadily, however, close to nothing is known about the relative performance of these methods under fair experimental conditions and under consideration of different use cases.
Results
We developed a comparison framework that trains and optimizes multi-omics integration methods under equal conditions. We incorporated early integration, PCA and four recently published deep learning methods: MOLI, Super.FELT, OmiEmbed, and MOMA. Further, we devised a novel method, Omics Stacking, that combines the advantages of intermediate and late integration. Experiments were conducted on a public drug response data set with multiple omics data (somatic point mutations, somatic copy number profiles and gene expression profiles) that was obtained from cell lines, patient-derived xenografts, and patient samples. Our experiments confirmed that early integration has the lowest predictive performance. Overall, architectures that integrate triplet loss achieved the best results. Statistical differences can, overall, rarely be observed, however, in terms of the average ranks of methods, Super.FELT is consistently performing best in a cross-validation setting and Omics Stacking best in an external test set setting.
Conclusions
We recommend researchers to follow fair comparison protocols, as suggested in the paper. When faced with a new data set, Super.FELT is a good option in the cross-validation setting as well as Omics Stacking in the external test set setting. Statistical significances are hardly observable, despite trends in the algorithms’ rankings. Future work on refined methods for transfer learning tailored for this domain may improve the situation for external test sets. The source code of all experiments is available under 
https://github.com/kramerlab/Multi-Omics_analysis"
443,Systematic analysis identifies XRCC4 as a potential immunological and prognostic biomarker associated with pan-cancer,"Background
XRCC4 is a NHEJ factor identified recently that plays a vital role in repairing DNA double-stranded breaks. Studies have reported the associations between abnormal expression of XRCC4 and tumor susceptibility and radiosensitivity, but the potential biological mechanisms by which XRCC4 exerts effects on tumorigenesis are not fully understood. This study aimed to systematically investigate the role of XRCC4 across cancer types.
Methods
The TIMER, GTEX and Xiantao Academic database were used to interpret the expression of XRCC4. Genomic alterations and protein expression in human organic and tumor tissues were applied in cBioPortal and the Human Protein Atlas databases. Correlations between XRCC4 expression and immune and molecular subtypes were analyzed by using the TISIDB database. Protein–protein interactions, GO and KEGG enrichment were also applied for XRCC4-related genes. The TIMER and the Tumor Immune Single Cell Hub (TISCH) online databases were used to explore the relationship between XRCC4 and tumor immune microenvironment. Drug sensitivity information was acquired from the CellMiner database to analyze the effect of XRCC4 on sensitivity analysis.
Results
The XRCC4 expression was significantly upregulated in 15 tumor types and downregulated in two tumor types compared with the normal tissues, most of which were validated by the results of Xiantao academic platform. XRCC4 was expressed at intermediate level in malignant cells. The XRCC4 expression was related to the molecular and immune subtypes of human cancers, and the survival outcome of 11 types of cancers, including KIRC, STAD and LIHC. The main type of frequent genetic alteration is amplification. Strong correlations were also found between XRCC4 and immune checkpoint genes in 33 human cancers. Furthermore, the abnormal expression of XRCC4 was related to immune cell infiltration and drug sensitivity. Enrichment analysis showed that XRCC4 was significantly correlated with DNA damage response.
Conclusions
This comprehensive pan-cancer analysis suggested that XRCC4 may play a vital role in the prognosis and immunotherapy response in cancer patients, and it is a promising therapy target in the future."
444,Model performance and interpretability of semi-supervised generative adversarial networks to predict oncogenic variants with unlabeled data,"Background
It remains an important challenge to predict the functional consequences or clinical impacts of genetic variants in human diseases, such as cancer. An increasing number of genetic variants in cancer have been discovered and documented in public databases such as COSMIC, but the vast majority of them have no functional or clinical annotations. Some databases, such as CiVIC are available with manual annotation of functional mutations, but the size of the database is small due to the use of human annotation. Since the unlabeled data (millions of variants) typically outnumber labeled data (thousands of variants), computational tools that take advantage of unlabeled data may improve prediction accuracy.
Result
To leverage unlabeled data to predict functional importance of genetic variants, we introduced a method using semi-supervised generative adversarial networks (SGAN), incorporating features from both labeled and unlabeled data. Our SGAN model incorporated features from clinical guidelines and predictive scores from other computational tools. We also performed comparative analysis to study factors that influence prediction accuracy, such as using different algorithms, types of features, and training sample size, to provide more insights into variant prioritization. We found that SGAN can achieve competitive performances with small labeled training samples by incorporating unlabeled samples, which is a unique advantage compared to traditional machine learning methods. We also found that manually curated samples can achieve a more stable predictive performance than publicly available datasets.
Conclusions
By incorporating much larger samples of unlabeled data, the SGAN method can improve the ability to detect novel oncogenic variants, compared to other machine-learning algorithms that use only labeled datasets. SGAN can be potentially used to predict the pathogenicity of more complex variants such as structural variants or non-coding variants, with the availability of more training samples and informative features."
445,A prefix and attention map discrimination fusion guided attention for biomedical named entity recognition,"Background
The biomedical literature is growing rapidly, and it is increasingly important to extract meaningful information from the vast amount of literature. Biomedical named entity recognition (BioNER) is one of the key and fundamental tasks in biomedical text mining. It also acts as a primitive step for many downstream applications such as relation extraction and knowledge base completion. Therefore, the accurate identification of entities in biomedical literature has certain research value. However, this task is challenging due to the insufficiency of sequence labeling and the lack of large-scale labeled training data and domain knowledge.
Results
In this paper, we use a novel word-pair classification method, design a simple attention mechanism and propose a novel architecture to solve the research difficulties of BioNER more efficiently without leveraging any external knowledge. Specifically, we break down the limitations of sequence labeling-based approaches by predicting the relationship between word pairs. Based on this, we enhance the pre-trained model BioBERT, through the proposed  prefix and attention map dscrimination fusion guided attention and propose the E-BioBERT. Our proposed attention differentiates the distribution of different heads in different layers in the BioBERT, which enriches the diversity of self-attention. Our model is superior to state-of-the-art compared models on five available datasets: BC4CHEMD, BC2GM, BC5CDR-Disease, BC5CDR-Chem, and NCBI-Disease, achieving F1-score of 92.55%, 85.45%, 87.53%, 94.16% and 90.55%, respectively.
Conclusion
Compared with many previous various models, our method does not require additional training datasets, external knowledge, and complex training process. The experimental results on five BioNER benchmark datasets demonstrate that our model is better at mining semantic information, alleviating the problem of label inconsistency, and has higher entity recognition ability. More importantly, we analyze and demonstrate the effectiveness of our proposed attention."
446,pLMSNOSite: an ensemble-based approach for predicting protein S-nitrosylation sites by integrating supervised word embedding and embedding from pre-trained protein language model,"Background
Protein S-nitrosylation (SNO) plays a key role in transferring nitric oxide-mediated signals in both animals and plants and has emerged as an important mechanism for regulating protein functions and cell signaling of all main classes of protein. It is involved in several biological processes including immune response, protein stability, transcription regulation, post translational regulation, DNA damage repair, redox regulation, and is an emerging paradigm of redox signaling for protection against oxidative stress. The development of robust computational tools to predict protein SNO sites would contribute to further interpretation of the pathological and physiological mechanisms of SNO.
Results
Using an intermediate fusion-based stacked generalization approach, we integrated embeddings from supervised embedding layer and contextualized protein language model (ProtT5) and developed a tool called pLMSNOSite (protein language model-based SNO site predictor). On an independent test set of experimentally identified SNO sites, pLMSNOSite achieved values of 0.340, 0.735 and 0.773 for MCC, sensitivity and specificity respectively. These results show that pLMSNOSite performs better than the compared approaches for the prediction of S-nitrosylation sites.
Conclusion
Together, the experimental results suggest that pLMSNOSite achieves significant improvement in the prediction performance of S-nitrosylation sites and represents a robust computational approach for predicting protein S-nitrosylation sites. pLMSNOSite could be a useful resource for further elucidation of SNO and is publicly available at 
https://github.com/KCLabMTU/pLMSNOSite
."
447,On triangle inequalities of correlation-based distances for gene expression profiles,"Background
Distance functions are fundamental for evaluating the differences between gene expression profiles. Such a function would output a low value if the profiles are strongly correlated—either negatively or positively—and vice versa. One popular distance function is the absolute correlation distance, 
\(d_a=1-|\rho |\)
, where 
\(\rho\)
 is similarity measure, such as Pearson or Spearman correlation. However, the absolute correlation distance fails to fulfill the triangle inequality, which would have guaranteed better performance at vector quantization, allowed fast data localization, as well as accelerated data clustering.
Results
In this work, we propose 
\(d_r=\sqrt{1-|\rho |}\)
 as an alternative. We prove that 
\(d_r\)
 satisfies the triangle inequality when 
\(\rho\)
 represents Pearson correlation, Spearman correlation, or Cosine similarity. We show 
\(d_r\)
 to be better than 
\(d_s=\sqrt{1-\rho ^2}\)
, another variant of 
\(d_a\)
 that satisfies the triangle inequality, both analytically as well as experimentally. We empirically compared 
\(d_r\)
 with 
\(d_a\)
 in gene clustering and sample clustering experiment by real-world biological data. The two distances performed similarly in both gene clustering and sample clustering in hierarchical clustering and PAM (partitioning around medoids) clustering. However, 
\(d_r\)
 demonstrated more robust clustering. According to the bootstrap experiment, 
\(d_r\)
 generated more robust sample pair partition more frequently (
P
-value 
\(<0.05\)
). The statistics on the time a class “dissolved” also support the advantage of 
\(d_r\)
 in robustness.
Conclusion
\(d_r\)
, as a variant of absolute correlation distance, satisfies the triangle inequality and is capable for more robust clustering."
448,Deep learning model integrating positron emission tomography and clinical data for prognosis prediction in non-small cell lung cancer patients,"Background
Lung cancer is the leading cause of cancer-related deaths worldwide. The majority of lung cancers are non-small cell lung cancer (NSCLC), accounting for approximately 85% of all lung cancer types. The Cox proportional hazards model (CPH), which is the standard method for survival analysis, has several limitations. The purpose of our study was to improve survival prediction in patients with NSCLC by incorporating prognostic information from F-18 fluorodeoxyglucose positron emission tomography (FDG PET) images into a traditional survival prediction model using clinical data.
Results
The multimodal deep learning model showed the best performance, with a C-index and mean absolute error of 0.756 and 399 days under a five-fold cross-validation, respectively, followed by ResNet3D for PET (0.749 and 405 days) and CPH for clinical data (0.747 and 583 days).
Conclusion
The proposed deep learning-based integrative model combining the two modalities improved the survival prediction in patients with NSCLC."
449,SSELM-neg: spherical search-based extreme learning machine for drug–target interaction prediction,"Background
The experimental verification of a drug discovery process is expensive and time-consuming. Therefore, efficiently and effectively identifying drug–target interactions (DTIs) has been the focus of research. At present, many machine learning algorithms are used for predicting DTIs. The key idea is to train the classifier using an existing DTI to predict a new or unknown DTI. However, there are various challenges, such as class imbalance and the parameter optimization of many classifiers, that need to be solved before an optimal DTI model is developed.
Methods
In this study, we propose a framework called SSELM-neg for DTI prediction, in which we use a screening approach to choose high-quality negative samples and a spherical search approach to optimize the parameters of the extreme learning machine.
Results
The results demonstrated that the proposed technique outperformed other state-of-the-art methods in 10-fold cross-validation experiments in terms of the area under the receiver operating characteristic curve (0.986, 0.993, 0.988, and 0.969) and AUPR (0.982, 0.991, 0.982, and 0.946) for the enzyme dataset, G-protein coupled receptor dataset, ion channel dataset, and nuclear receptor dataset, respectively.
Conclusion
The screening approach produced high-quality negative samples with the same number of positive samples, which solved the class imbalance problem. We optimized an extreme learning machine using a spherical search approach to identify DTIs. Therefore, our models performed better than other state-of-the-art methods."
450,Cuproptosis-related lncRNA signature for prognostic prediction in patients with acute myeloid leukemia,"Background
Long non-coding RNAs (lncRNAs) have been reported to have a crucial impact on the pathogenesis of acute myeloid leukemia (AML). Cuproptosis, a copper-triggered modality of mitochondrial cell death, might serve as a promising therapeutic target for cancer treatment and clinical outcome prediction. Nevertheless, the role of cuproptosis-related lncRNAs in AML is not fully understood.

Methods
The RNA sequencing data and demographic characteristics of AML patients were downloaded from The Cancer Genome Atlas database. Pearson correlation analysis, the least absolute shrinkage and selection operator algorithm, and univariable and multivariable Cox regression analyses were applied to identify the cuproptosis-related lncRNA signature and determine its feasibility for AML prognosis prediction. The performance of the proposed signature was evaluated via Kaplan–Meier survival analysis, receiver operating characteristic curves, and principal component analysis. Functional analysis was implemented to uncover the potential prognostic mechanisms. Additionally, quantitative real-time PCR (qRT-PCR) was employed to validate the expression of the prognostic lncRNAs in AML samples.
Results
A signature consisting of seven cuproptosis-related lncRNAs (namely NFE4, LINC00989, LINC02062, AC006460.2, AL353796.1, PSMB8-AS1, and AC000120.1) was proposed. Multivariable cox regression analysis revealed that the proposed signature was an independent prognostic factor for AML. Notably, the nomogram based on this signature showed excellent accuracy in predicting the 1-, 3-, and 5-year survival (area under curve = 0.846, 0.801, and 0.895, respectively). Functional analysis results suggested the existence of a significant association between the prognostic signature and immune-related pathways. The expression pattern of the lncRNAs was validated in AML samples.
Conclusion
Collectively, we constructed a prediction model based on seven cuproptosis-related lncRNAs for AML prognosis. The obtained risk score may reveal the immunotherapy response in patients with this disease."
451,ShrinkCRISPR: a flexible method for differential fitness analysis of CRISPR-Cas9 screen data,"Background
CRISPR screens provide large-scale assessment of cellular gene functions. Pooled libraries typically consist of several single guide RNAs (sgRNAs) per gene, for a large number of genes, which are transduced in such a way that every cell receives at most one sgRNA, resulting in the disruption of a single gene in that cell. This approach is often used to investigate effects on cellular fitness, by measuring sgRNA abundance at different time points. Comparing gene knockout effects between different cell populations is challenging due to variable cell-type specific parameters and between replicates variation. Failure to take those into account can lead to inflated or false discoveries.
Results
We propose a new, flexible approach called ShrinkCRISPR that can take into account multiple sources of variation. Impact on cellular fitness between conditions is inferred by using a mixed-effects model, which allows to test for gene-knockout effects while taking into account sgRNA-specific variation. Estimates are obtained using an empirical Bayesian approach. ShrinkCRISPR can be applied to a variety of experimental designs, including multiple factors. In simulation studies, we compared ShrinkCRISPR results with those of drugZ and MAGeCK, common methods used to detect differential effect on cell fitness. ShrinkCRISPR yielded as many true discoveries as drugZ using a paired screen design, and outperformed both drugZ and MAGeCK for an independent screen design. Although conservative, ShrinkCRISPR was the only approach that kept false discoveries under control at the desired level, for both designs. Using data from several publicly available screens, we showed that ShrinkCRISPR can take data for several time points into account simultaneously, helping to detect early and late differential effects.
Conclusions
ShrinkCRISPR is a robust and flexible approach, able to incorporate different sources of variations and to test for differential effect on cell fitness at the gene level. These improve power to find effects on cell fitness, while keeping multiple testing under the correct control level and helping to improve reproducibility. ShrinkCrispr can be applied to different study designs and incorporate multiple time points, making it a complete and reliable tool to analyze CRISPR screen data."
452,GACNNMDA: a computational model for predicting potential human microbe-drug associations based on graph attention network and CNN-based classifier,"As new drug targets, human microbes are proven to be closely related to human health. Effective computational methods for inferring potential microbe-drug associations can provide a useful complement to conventional experimental methods and will facilitate drug research and development. However, it is still a challenging work to predict potential interactions for new microbes or new drugs, since the number of known microbe-drug associations is very limited at present. In this manuscript, we first constructed two heterogeneous microbe-drug networks based on multiple measures of similarity of microbes and drugs, and known microbe-drug associations or known microbe-disease-drug associations, respectively. And then, we established two feature matrices for microbes and drugs through concatenating various attributes of microbes and drugs. Thereafter, after taking these two feature matrices and two heterogeneous microbe-drug networks as inputs of a two-layer graph attention network, we obtained low dimensional feature representations for microbes and drugs separately. Finally, through integrating low dimensional feature representations with two feature matrices to form the inputs of a convolutional neural network respectively, a novel computational model named GACNNMDA was designed to predict possible scores of microbe-drug pairs. Experimental results show that the predictive performance of GACNNMDA is superior to existing advanced methods. Furthermore, case studies on well-known microbes and drugs demonstrate the effectiveness of GACNNMDA as well. Source codes and supplementary materials are available at: 
https://github.com/tyqGitHub/TYQ/tree/master/GACNNMDA"
453,PP-DDP: a privacy-preserving outsourcing framework for solving the double digest problem,"Background
As one of the fundamental problems in bioinformatics, the double digest problem (DDP) focuses on reordering genetic fragments in a proper sequence. Although many algorithms for dealing with the DDP problem were proposed during the past decades, it is believed that solving DDP is still very time-consuming work due to the strongly NP-completeness of DDP. However, none of these algorithms consider the privacy issue of the DDP data that contains critical business interests and is collected with days or even months of gel-electrophoresis experiments. Thus, the DDP data owners are reluctant to deploy the task of solving DDP over cloud.
Results
Our main motivation in this paper is to design a secure outsourcing computation framework for solving the DDP problem. We at first propose a privacy-preserving outsourcing framework for handling the DDP problem by using a cloud server; Then, to enable the cloud server to solve the DDP instances over ciphertexts, an order-preserving homomorphic index scheme (OPHI) is tailored from an order-preserving encryption scheme published at CCS 2012; And finally, our previous work on solving DDP problem, a quantum inspired genetic algorithm (QIGA), is merged into our outsourcing framework, with the supporting of the proposed OPHI scheme. Moreover, after the execution of QIGA at the cloud server side, the optimal solution, i.e. two mapping sequences, would be transferred 
publicly
 to the data owner. Security analysis shows that from these sequences, none can learn any information about the original DDP data. Performance analysis shows that the communication cost and the computational workload for both the client side and the server side are reasonable. In particular, our experiments show that PP-DDP can find optional solutions with a high success rate towards typical test DDP instances and random DDP instances, and PP-DDP takes less running time than DDmap, SK05 and GM12, while keeping the privacy of the original DDP data.
Conclusion
The proposed outsourcing framework, PP-DDP, is secure and effective for solving the DDP problem."
454,Systematic and benchmarking studies of pipelines for mammal WGBS data in the novel NGS platform,"Background
Whole genome bisulfite sequencing (WGBS), possesses the aptitude to dissect methylation status at the nucleotide-level resolution of 5-methylcytosine (5-mC) on a genome-wide scale. It is a powerful technique for epigenome in various cell types, and tissues. As a recently established next-generation sequencing (NGS) platform, GenoLab M is a promising alternative platform. However, its comprehensive evaluation for WGBS has not been reported. We sequenced two bisulfite-converted mammal DNA in this research using our GenoLab M and NovaSeq 6000, respectively. Then, we systematically compared those data via four widely used WGBS tools (BSMAP, Bismark, BatMeth2, BS-Seeker2) and a new bisulfite-seq tool (BSBolt). We interrogated their computational time, genome depth and coverage, and evaluated their percentage of methylated Cs.
Result
Here, benchmarking a combination of pre- and post-processing methods, we found that trimming improved the performance of mapping efficiency in eight datasets. The data from two platforms uncovered ~ 80% of CpG sites genome-wide in the human cell line. Those data sequenced by GenoLab M achieved a far lower proportion of duplicates (~ 5.5%). Among pipelines, BSMAP provided an intriguing representation of 5-mC distribution at CpG sites with 5-mC levels > ~ 78% in datasets from human cell lines, especially in the GenoLab M. BSMAP performed more advantages in running time, uniquely mapped reads percentages, genomic coverage, and quantitative accuracy. Finally, compared with the previous methylation pattern of human cell line and mouse tissue, we confirmed that the data from GenoLab M performed similar consistency and accuracy in methylation levels of CpG sites with that from NovaSeq 6000.
Conclusion
Together we confirmed that GenoLab M was a qualified NGS platform for WGBS with high performance. Our results showed that BSMAP was the suitable pipeline that allowed for WGBS studies on the GenoLab M platform."
455,GENTLE: a novel bioinformatics tool for generating features and building classifiers from T cell repertoire cancer data,"Background
In the global effort to discover biomarkers for cancer prognosis, prediction tools have become essential resources. TCR (T cell receptor) repertoires contain important features that differentiate healthy controls from cancer patients or differentiate outcomes for patients being treated with different drugs. Considering, tools that can easily and quickly generate and identify important features out of TCR repertoire data and build accurate classifiers to predict future outcomes are essential.

Results
This paper introduces GENTLE (GENerator of T cell receptor repertoire features for machine LEarning): an open-source, user-friendly web-application tool that allows TCR repertoire researchers to discover important features; to create classifier models and evaluate them with metrics; and to quickly generate visualizations for data interpretations. We performed a case study with repertoires of TRegs (regulatory T cells) and TConvs (conventional T cells) from healthy controls versus patients with breast cancer. We showed that diversity features were able to distinguish between the groups. Moreover, the classifiers built with these features could correctly classify samples (‘Healthy’ or ‘Breast Cancer’)from the TRegs repertoire when trained with the TConvs repertoire, and from the TConvs repertoire when trained with the TRegs repertoire.
Conclusion
The paper walks through installing and using GENTLE and presents a case study and results to demonstrate the application’s utility. GENTLE is geared towards any researcher working with TCR repertoire data and aims to discover predictive features from these data and build accurate classifiers. GENTLE is available on 
https://github.com/dhiego22/gentle
 and 
https://share.streamlit.io/dhiego22/gentle/main/gentle.py
."
456,DeepSelectNet: deep neural network based selective sequencing for oxford nanopore sequencing,"Background
Nanopore sequencing allows selective sequencing, the ability to programmatically reject unwanted reads in a sample. Selective sequencing has many present and future applications in genomics research and the classification of species from a pool of species is an example. Existing methods for selective sequencing for species classification are still immature and the accuracy highly varies depending on the datasets. For the five datasets we tested, the accuracy of existing methods varied in the range of 
\(\sim\)
 77 to 97% (average accuracy < 89%). Here we present DeepSelectNet, an accurate deep-learning-based method that can directly classify nanopore current signals belonging to a particular species. DeepSelectNet utilizes novel data preprocessing techniques and improved neural network architecture for regularization.
Results
For the five datasets tested, DeepSelectNet’s accuracy varied between 
\(\sim\)
 91 and 99% (average accuracy 
\(\sim\)
 95%). At its best performance, DeepSelectNet achieved a nearly 12% accuracy increase compared to its deep learning-based predecessor SquiggleNet. Furthermore, precision and recall evaluated for DeepSelectNet on average were always > 89% (average 
\(\sim\)
 95%). In terms of execution performance, DeepSelectNet outperformed SquiggleNet by 
\(\sim\)
 13% on average. Thus, DeepSelectNet is a practically viable method to improve the effectiveness of selective sequencing.
Conclusions
Compared to base alignment and deep learning predecessors, DeepSelectNet can significantly improve the accuracy to enable real-time species classification using selective sequencing. The source code of DeepSelectNet is available at 
https://github.com/AnjanaSenanayake/DeepSelectNet
."
457,Single-cell spatial explorer: easy exploration of spatial and multimodal transcriptomics,"Background:
The development of single-cell technologies yields large datasets of information as diverse and multimodal as transcriptomes, immunophenotypes, and spatial position from tissue sections in the so-called ’spatial transcriptomics’. Currently however, user-friendly, powerful, and free algorithmic tools for straightforward analysis of spatial transcriptomic datasets are scarce.
Results:
Here, we introduce Single-Cell Spatial Explorer, an open-source software for multimodal exploration of spatial transcriptomics, examplified with 9 human and murine tissues datasets from 4 different technologies.
Conclusions:
Single-Cell Spatial Explorer is a very powerful, versatile, and interoperable tool for spatial transcriptomics analysis."
458,Ion-pumping microbial rhodopsin protein classification by machine learning approach,"Background
Rhodopsin is a seven-transmembrane protein covalently linked with retinal chromophore that absorbs photons for energy conversion and intracellular signaling in eukaryotes, bacteria, and archaea. Haloarchaeal rhodopsins are Type-I microbial rhodopsin that elicits various light-driven functions like proton pumping, chloride pumping and Phototaxis behaviour. The industrial application of Ion-pumping Haloarchaeal rhodopsins is limited by the lack of full-length rhodopsin sequence-based classifications, which play an important role in Ion-pumping activity. The well-studied 
Haloarchaeal
 rhodopsin is a proton-pumping bacteriorhodopsin that shows promising applications in optogenetics, biosensitized solar cells, security ink, data storage, artificial retinal implant and biohydrogen generation. As a result, a low-cost computational approach is required to identify Ion-pumping 
Haloarchaeal
 rhodopsin sequences and its subtype.

Results
This study uses a support vector machine (SVM) technique to identify these ion-pumping 
Haloarchaeal
 rhodopsin proteins. The haloarchaeal ion pumping rhodopsins viz., bacteriorhodopsin, halorhodopsin, xanthorhodopsin, sensoryrhodopsin and marine prokaryotic Ion-pumping rhodopsins like actinorhodopsin, proteorhodopsin have been utilized to develop the methods that accurately identified the ion pumping haloarchaeal and other type I microbial rhodopsins. We achieved overall maximum accuracy of 97.78%, 97.84% and 97.60%, respectively, for amino acid composition, dipeptide composition and hybrid approach on tenfold cross validation using SVM. Predictive models for each class of rhodopsin performed equally well on an independent data set. In addition to this, similar results were achieved using another machine learning technique namely random forest. Simultaneously predictive models performed equally well during five-fold cross validation. Apart from this study, we also tested the own, blank, BLAST dataset and annotated whole-genome rhodopsin sequences of PWS haloarchaeal isolates in the developed methods. The developed web server (
https://bioinfo.imtech.res.in/servers/rhodopred
) can identify the Ion Pumping Haloarchaeal rhodopsin proteins and their subtypes. We expect this web tool would be useful for rhodopsin researchers.
Conclusion
The overall performance of the developed method results show that it accurately identifies the Ionpumping 
Haloarchaeal
 rhodopsin and their subtypes using known and unknown microbial rhodopsin sequences. We expect that this study would be useful for optogenetics, molecular biologists and rhodopsin researchers."
459,"Gdaphen, R pipeline to identify the most important qualitative and quantitative predictor variables from phenotypic data","Background
In individuals or animals suffering from genetic or acquired diseases, it is important to identify which clinical or phenotypic variables can be used to discriminate between disease and non-disease states, the response to treatments or sexual dimorphism. However, the data often suffers from low number of samples, high number of variables or unbalanced experimental designs. Moreover, several parameters can be recorded in the same test. Thus, correlations should be assessed, and a more complex statistical framework is necessary for the analysis. Packages already exist that provide analysis tools, but they are not found together, rendering the decision method and implementation difficult for non-statisticians.
Result
We present Gdaphen, a fast joint-pipeline allowing the identification of most important qualitative and quantitative predictor variables to discriminate between genotypes, treatments, or sex. Gdaphen takes as input behavioral/clinical data and uses a Multiple Factor Analysis (MFA) to deal with groups of variables recorded from the same individuals or anonymize genotype-based recordings. Gdaphen uses as optimized input the non-correlated variables with 30% correlation or higher on the MFA-Principal Component Analysis (PCA), increasing the discriminative power and the classifier’s predictive model efficiency. Gdaphen can determine the strongest variables that predict gene dosage effects thanks to the General Linear Model (GLM)-based classifiers or determine the most discriminative not linear distributed variables thanks to Random Forest (RF) implementation. Moreover, Gdaphen provides the efficacy of each classifier and several visualization options to fully understand and support the results as easily readable plots ready to be included in publications. We demonstrate Gdaphen capabilities on several datasets and provide easily followable vignettes.
Conclusions
Gdaphen makes the analysis of phenotypic data much easier for medical or preclinical behavioral researchers, providing an integrated framework to perform: (1) pre-processing steps as data imputation or anonymization; (2) a full statistical assessment to identify which variables are the most important discriminators; and (3) state of the art visualizations ready for publication to support the conclusions of the analyses. Gdaphen is open-source and freely available at 
https://github.com/munizmom/gdaphen
, together with vignettes, documentation for the functions and examples to guide you in each own implementation."
460,"nf-core/circrna: a portable workflow for the quantification, miRNA target prediction and differential expression analysis of circular RNAs","Background
Circular RNAs (circRNAs) are a class of covalenty closed non-coding RNAs that have garnered increased attention from the research community due to their stability, tissue-specific expression and role as transcriptional modulators via sequestration of miRNAs. Currently, multiple quantification tools capable of detecting circRNAs exist, yet none delineate circRNA–miRNA interactions, and only one employs differential expression analysis. Efforts have been made to bridge this gap by way of circRNA workflows, however these workflows are limited by both the types of analyses available and computational skills required to run them.
Results
We present nf-core/circrna, a multi-functional, automated high-throughput pipeline implemented in nextflow that allows users to characterise the role of circRNAs in RNA Sequencing datasets via three analysis modules: (1) circRNA quantification, robust filtering and annotation (2) miRNA target prediction of the mature spliced sequence and (3) differential expression analysis. nf-core/circrna has been developed within the nf-core framework, ensuring robust portability across computing environments via containerisation, parallel deployment on cluster/cloud-based infrastructures, comprehensive documentation and maintenance support.
Conclusion
nf-core/circrna reduces the barrier to entry for researchers by providing an easy-to-use, platform-independent and scalable workflow for circRNA analyses. Source code, documentation and installation instructions are freely available at 
https://nf-co.re/circrna
 and 
https://github.com/nf-core/circrna
."
461,Effective matrix designs for COVID-19 group testing,"Background
Grouping samples with low prevalence of positives into pools and testing these pools can achieve considerable savings in testing resources compared with individual testing in the context of COVID-19. We review published pooling matrices, which encode the assignment of samples into pools and describe decoding algorithms, which decode individual samples from pools. Based on the findings we propose new one-round pooling designs with high compression that can efficiently be decoded by combinatorial algorithms. This expands the admissible parameter space for the construction of pooling matrices compared to current methods.
Results
By arranging samples in a grid and using polynomials to construct pools, we develop direct formulas for an Algorithm (Polynomial Pools (PP)) to generate assignments of samples into pools. Designs from PP guarantee to correctly decode all samples with up to a specified number of positive samples. PP includes recent combinatorial methods for COVID-19, and enables new constructions that can result in more effective designs.
Conclusion
For low prevalences of COVID-19, group tests can save resources when compared to individual testing. Constructions from the recent literature on combinatorial methods have gaps with respect to the designs that are available. We develop a method (PP), which generalizes previous constructions and enables new designs that can be advantageous in various situations."
462,Identification of prognostic and predictive biomarkers in high-dimensional data with PPLasso,"In clinical trials, identification of prognostic and predictive biomarkers has became essential to precision medicine. Prognostic biomarkers can be useful for the prevention of the occurrence of the disease, and predictive biomarkers can be used to identify patients with potential benefit from the treatment. Previous researches were mainly focused on clinical characteristics, and the use of genomic data in such an area is hardly studied. A new method is required to simultaneously select prognostic and predictive biomarkers in high dimensional genomic data where biomarkers are highly correlated. We propose a novel approach called PPLasso, that integrates prognostic and predictive effects into one statistical model. PPLasso also takes into account the correlations between biomarkers that can alter the biomarker selection accuracy. Our method consists in transforming the design matrix to remove the correlations between the biomarkers before applying the generalized Lasso. In a comprehensive numerical evaluation, we show that PPLasso outperforms the traditional Lasso and other extensions on both prognostic and predictive biomarker identification in various scenarios. Finally, our method is applied to publicly available transcriptomic and proteomic data."
463,BADASS: BActeriocin-Diversity ASsessment Software,"Background
Bacteriocins are defined as thermolabile peptides produced by bacteria with biological activity against taxonomically related species. These antimicrobial peptides have a wide application including disease treatment, food conservation, and probiotics. However, even with a large industrial and biotechnological application potential, these peptides are still poorly studied and explored. BADASS is software with a user-friendly graphical interface applied to the search and analysis of bacteriocin diversity in whole-metagenome shotgun sequencing data.
Results
The search for bacteriocin sequences is performed with tools such as BLAST or DIAMOND using the BAGEL4 database as a reference. The putative bacteriocin sequences identified are used to determine the abundance and richness of the three classes of bacteriocins. Abundance is calculated by comparing the reads identified as bacteriocins to the reads identified as 16S rRNA gene using SILVA database as a reference. BADASS has a complete pipeline that starts with the quality assessment of the raw data. At the end of the analysis, BADASS generates several plots of richness and abundance automatically as well as tabular files containing information about the main bacteriocins detected. The user is able to change the main parameters of the analysis in the graphical interface. To demonstrate how the software works, we used four datasets from WMS studies using default parameters. Lantibiotics were the most abundant bacteriocins in the four datasets. This class of bacteriocin is commonly produced by 
Streptomyces
 sp.

Conclusions
With a user-friendly graphical interface and a complete pipeline, BADASS proved to be a powerful tool for prospecting bacteriocin sequences in Whole-Metagenome Shotgun Sequencing (WMS) data. This tool is publicly available at 
https://sourceforge.net/projects/badass/
."
464,SVhound: detection of regions that harbor yet undetected structural variation,"Background
Recent population studies are ever growing in number of samples to investigate the diversity of a population or species. These studies reveal new polymorphism that lead to important insights into the mechanisms of evolution, but are also important for the interpretation of these variations. Nevertheless, while the full catalog of variations across entire species remains unknown, we can predict which regions harbor additional not yet detected variations and investigate their properties, thereby enhancing the analysis for potentially missed variants.
Results
To achieve this we developed SVhound (
https://github.com/lfpaulin/SVhound
), which based on a population level SVs dataset can predict regions that harbor unseen SV alleles. We tested SVhound using subsets of the 1000 genomes project data and showed that its correlation (average correlation of 2800 tests r = 0.7136) is high to the full data set. Next, we utilized SVhound to investigate potentially missed or understudied regions across 1KGP and CCDG. Lastly we also apply SVhound on a small and novel SV call set for rhesus macaque (
Macaca mulatta
) and discuss the impact and choice of parameters for SVhound.
Conclusions
SVhound is a unique method to identify potential regions that harbor hidden diversity in model and non model organisms and can also be potentially used to ensure high quality of SV call sets."
465,Accommodating multiple potential normalizations in microbiome associations studies,"Background
Microbial communities are known to be closely related to many diseases, such as obesity and HIV, and it is of interest to identify differentially abundant microbial species between two or more environments. Since the abundances or counts of microbial species usually have different scales and suffer from zero-inflation or over-dispersion, normalization is a critical step before conducting differential abundance analysis. Several normalization approaches have been proposed, but it is difficult to optimize the characterization of the true relationship between taxa and interesting outcomes. 
Results
To avoid the challenge of picking an optimal normalization and accommodate the advantages of several normalization strategies, we propose an omnibus approach. Our approach is based on a Cauchy combination test, which is flexible and powerful by aggregating individual 
p
 values. We also consider a truncated test statistic to prevent substantial power loss. We experiment with a basic linear regression model as well as recently proposed powerful association tests for microbiome data and compare the performance of the omnibus approach with individual normalization approaches. Experimental results show that, regardless of simulation settings, the new approach exhibits power that is close to the best normalization strategy, while controling the type I error well. 
Conclusions
The proposed omnibus test releases researchers from choosing among various normalization methods and it is an aggregated method that provides the powerful result to the underlying optimal normalization, which requires tedious trial and error. While the power may not exceed the best normalization, it is always much better than using a poor choice of normalization."
466,MultiScale-CNN-4mCPred: a multi-scale CNN and adaptive embedding-based method for mouse genome DNA N4-methylcytosine prediction,"N4-methylcytosine (4mC) is an important epigenetic mechanism, which regulates many cellular processes such as cell differentiation and gene expression. The knowledge about the 4mC sites is a key foundation to exploring its roles. Due to the limitation of techniques, precise detection of 4mC is still a challenging task. In this paper, we presented a multi-scale convolution neural network (CNN) and adaptive embedding-based computational method for predicting 4mC sites in mouse genome, which was referred to as MultiScale-CNN-4mCPred. The MultiScale-CNN-4mCPred used adaptive embedding to encode nucleotides, and then utilized multi-scale CNNs as well as long short-term memory to extract more in-depth local properties and contextual semantics in the sequences. The MultiScale-CNN-4mCPred is an end-to-end learning method, which requires no sophisticated feature design. The MultiScale-CNN-4mCPred reached an accuracy of 81.66% in the 10-fold cross-validation, and an accuracy of 84.69% in the independent test, outperforming state-of-the-art methods. We implemented the proposed method into a user-friendly web application which is freely available at: 
http://www.biolscience.cn/MultiScale-CNN-4mCPred/
."
467,A signature of immune-related genes correlating with clinical prognosis and immune microenvironment in sepsis,"Background
Immune-related genes (IRGs) remain poorly understood in their function in the onset and progression of sepsis.

Methods
GSE65682 was obtained from the Gene Expression Omnibus database. The IRGs associated with survival were screened for subsequent modeling using univariate Cox regression analysis and least absolute shrinkage and selection operator in the training cohort. Then, we assessed the reliability of the 7 IRGs signature's independent predictive value in the training and validation cohorts following the creation of a signature applying multivariable Cox regression analysis. After that, we utilized the E-MTAB-4451 external dataset in order to do an independent validation of the prognostic signature. Finally, the CIBERSORT algorithm and single-sample gene set enrichment analysis was utilized to investigate and characterize the properties of the immune microenvironment.
Results
Based on 7 IRGs signature, patients could be separated into low-risk and high-risk groups. Patients in the low-risk group had a remarkably increased 28-day survival compared to those in the high-risk group (
P
 < 0.001). In multivariable Cox regression analyses, the risk score calculated by this signature was an independent predictor of 28-day survival (
P
 < 0.001). The signature's predictive ability was confirmed by receiver operating characteristic curve analysis with the area under the curve reaching 0.876 (95% confidence interval 0.793–0.946). Moreover, both the validation set and the external dataset demonstrated that the signature had strong clinical prediction performance. In addition, patients in the high-risk group were characterized by a decreased neutrophil count and by reduced inflammation-promoting function.
Conclusion
We developed a 7 IRGs signature as a novel prognostic marker for predicting sepsis patients’ 28-day survival, indicating possibilities for individualized reasonable resource distribution of intensive care unit."
468,Genomic and immunogenomic analysis of three prognostic signature genes in LUAD,"Background
Searching for immunotherapy-related markers is an important research content to screen for target populations suitable for immunotherapy. Prognosis-related genes in early stage lung cancer may also affect the tumor immune microenvironment, which in turn affects immunotherapy.
Results
We analyzed the differential genes affecting lung cancer patients receiving immunotherapy through the Cancer Treatment Response gene signature DataBase (CTR-DB), and set a threshold to obtain a total of 176 differential genes between response and non-response to immunotherapy. Functional enrichment analysis found that these differential genes were mainly involved in immune regulation-related pathways. The early-stage lung adenocarcinoma (LUAD) prognostic model was constructed through the cancer genome atlas (TCGA) database, and three target genes (
MMP12
, 
NFE2
, HOXC8) were screened to calculate the risk score of early-stage LUAD. The receiver operating characteristic (ROC) curve indicated that the model had good prognostic value, and the validation set (GSE50081, GSE11969 and GSE42127) from the gene expression omnibus (GEO) analysis indicated that the model had good stability, and the risk score was correlated with immune infiltrations to varying degrees. Multi-type survival analysis and immune infiltration analysis revealed that the transcriptome, methylation and the copy number variation (CNV) levels of the three genes were correlated with patient prognosis and some tumor microenvironment (TME) components. Drug sensitivity analysis found that the three genes may affect some anti-tumor drugs. The mRNA expression of immune checkpoint-related genes showed significant differences between the high and low group of the three genes, and there may be a mutual regulatory network between immune checkpoint-related genes and target genes. Tumor immune dysfunction and exclusion (TIDE) analysis found that three genes were associated with immunotherapy response and maybe the potential predictors to immunotherapy, consistent with the CTR-DB database analysis.
Conclusions
From the perspective of data mining, this study suggests that 
MMP12
, 
NFE2
, and HOXC8 may be involved in tumor immune regulation and affect immunotherapy. They are expected to become markers of immunotherapy and are worthy of further experimental research."
469,PDA-PRGCN: identification of Piwi-interacting RNA-disease associations through subgraph projection and residual scaling-based feature augmentation,"Background
Emerging evidences show that Piwi-interacting RNAs (piRNAs) play a pivotal role in numerous complex human diseases. Identifying potential piRNA-disease associations (PDAs) is crucial for understanding disease pathogenesis at molecular level. Compared to the biological wet experiments, the computational methods provide a cost-effective strategy. However, few computational methods have been developed so far.
Results
Here, we proposed an end-to-end model, referred to as PDA-PRGCN (PDA prediction using subgraph Projection and Residual scaling-based feature augmentation through Graph Convolutional Network). Specifically, starting with the known piRNA-disease associations represented as a graph, we applied subgraph projection to construct piRNA-piRNA and disease-disease subgraphs for the first time, followed by a residual scaling-based feature augmentation algorithm for node initial representation. Then, we adopted graph convolutional network (GCN) to learn and identify potential PDAs as a link prediction task on the constructed heterogeneous graph. Comprehensive experiments, including the performance comparison of individual components in PDA-PRGCN, indicated the significant improvement of integrating subgraph projection, node feature augmentation and dual-loss mechanism into GCN for PDA prediction. Compared with state-of-the-art approaches, PDA-PRGCN gave more accurate and robust predictions. Finally, the case studies further corroborated that PDA-PRGCN can reliably detect PDAs.
Conclusion
PDA-PRGCN provides a powerful method for PDA prediction, which can also serve as a screening tool for studies of complex diseases."
470,Identification of biomarkers predictive of metastasis development in early-stage colorectal cancer using network-based regularization,"Colorectal cancer (CRC) is the third most common cancer and the second most deathly worldwide. It is a very heterogeneous disease that can develop via distinct pathways where metastasis is the primary cause of death. Therefore, it is crucial to understand the molecular mechanisms underlying metastasis. RNA-sequencing is an essential tool used for studying the transcriptional landscape. However, the high-dimensionality of gene expression data makes selecting novel metastatic biomarkers problematic. To distinguish early-stage CRC patients at risk of developing metastasis from those that are not, three types of binary classification approaches were used: (1) classification methods (decision trees, linear and radial kernel support vector machines, logistic regression, and random forest) using differentially expressed genes (DEGs) as input features; (2) regularized logistic regression based on the Elastic Net penalty and the proposed iTwiner—a network-based regularizer accounting for gene correlation information; and (3) classification methods based on the genes pre-selected using regularized logistic regression. Classifiers using the DEGs as features showed similar results, with random forest showing the highest accuracy. Using regularized logistic regression on the full dataset yielded no improvement in the methods’ accuracy. Further classification using the pre-selected genes found by different penalty factors, instead of the DEGs, significantly improved the accuracy of the binary classifiers. Moreover, the use of network-based correlation information (iTwiner) for gene selection produced the best classification results and the identification of more stable and robust gene sets. Some are known to be tumor suppressor genes (
OPCML-IT2
), to be related to resistance to cancer therapies (
RAC1P3
), or to be involved in several cancer processes such as genome stability (
XRCC6P2
), tumor growth and metastasis (
MIR602
) and regulation of gene transcription (
NME2P2
). We show that the classification of CRC patients based on pre-selected features by regularized logistic regression is a valuable alternative to using DEGs, significantly increasing the models’ predictive performance. Moreover, the use of correlation-based penalization for biomarker selection stands as a promising strategy for predicting patients’ groups based on RNA-seq data."
471,Identifying cancer driver genes based on multi-view heterogeneous graph convolutional network and self-attention mechanism,"Background
Correctly identifying the driver genes that promote cell growth can significantly assist drug design, cancer diagnosis and treatment. The recent large-scale cancer genomics projects have revealed multi-omics data from thousands of cancer patients, which requires to design effective models to unlock the hidden knowledge within the valuable data and discover cancer drivers contributing to tumorigenesis.
Results
In this work, we propose a graph convolution network-based method called MRNGCN that integrates multiple gene relationship networks to identify cancer driver genes. First, we constructed three gene relationship networks, including the gene–gene, gene–outlying gene and gene–miRNA networks. Then, genes learnt feature presentations from the three networks through three sharing-parameter heterogeneous graph convolution network (HGCN) models with the self-attention mechanism. After that, these gene features pass a convolution layer to generate fused features. Finally, we utilized the fused features and the original feature to optimize the model by minimizing the node and link prediction losses. Meanwhile, we combined the fused features, the original features and the three features learned from every network through a logistic regression model to predict cancer driver genes.
Conclusions
We applied the MRNGCN to predict pan-cancer and cancer type-specific driver genes. Experimental results show that our model performs well in terms of the area under the ROC curve (AUC) and the area under the precision–recall curve (AUPRC) compared to state-of-the-art methods. Ablation experimental results show that our model successfully improved the cancer driver identification by integrating multiple gene relationship networks."
472,"xcore
: an R package for inference of gene expression regulators","Background
Elucidating the Transcription Factors (TFs) that drive the gene expression changes in a given experiment is a common question asked by researchers. The existing methods rely on the predicted Transcription Factor Binding Site (TFBS) to model the changes in the motif activity. Such methods only work for TFs that have a motif and assume the TF binding profile is the same in all cell types.
Results
Given the wealth of the ChIP-seq data available for a wide range of the TFs in various cell types, we propose that gene expression modeling can be done using ChIP-seq “signatures” directly, effectively skipping the motif finding and TFBS prediction steps. We present 
xcore
, an R package that allows TF activity modeling based on ChIP-seq signatures and the user's gene expression data. We also provide 
xcoredata
 a companion data package that provides a collection of preprocessed ChIP-seq signatures. We demonstrate that 
xcore
 leads to biologically relevant predictions using transforming growth factor beta induced epithelial-mesenchymal transition time-courses, rinderpest infection time-courses, and embryonic stem cells differentiated to cardiomyocytes time-course profiled with Cap Analysis Gene Expression.
Conclusions
xcore
 provides a simple analytical framework for gene expression modeling using linear models that can be easily incorporated into differential expression analysis pipelines. Taking advantage of public ChIP-seq databases, 
xcore
 can identify meaningful molecular signatures and relevant ChIP-seq experiments."
473,DM-MOGA: a multi-objective optimization genetic algorithm for identifying disease modules of non-small cell lung cancer,"Background
Constructing molecular interaction networks from microarray data and then identifying disease module biomarkers can provide insight into the underlying pathogenic mechanisms of non-small cell lung cancer. A promising approach for identifying disease modules in the network is community detection.
Results
In order to identify disease modules from gene co-expression networks, a community detection method is proposed based on multi-objective optimization genetic algorithm with decomposition. The method is named DM-MOGA and possesses two highlights. First, the boundary correction strategy is designed for the modules obtained in the process of local module detection and pre-simplification. Second, during the evolution, we introduce Davies–Bouldin index and clustering coefficient as fitness functions which are improved and migrated to weighted networks. In order to identify modules that are more relevant to diseases, the above strategies are designed to consider the network topology of genes and the strength of connections with other genes at the same time. Experimental results of different gene expression datasets of non-small cell lung cancer demonstrate that the core modules obtained by DM-MOGA are more effective than those obtained by several other advanced module identification methods.
Conclusions
The proposed method identifies disease-relevant modules by optimizing two novel fitness functions to simultaneously consider the local topology of each gene and its connection strength with other genes. The association of the identified core modules with lung cancer has been confirmed by pathway and gene ontology enrichment analysis."
474,GBCdb: RNA expression landscapes and ncRNA–mRNA interactions in gallbladder carcinoma,"Gallbladder carcinoma (GBC), an aggressive malignant tumor of the biliary system, is characterized by high cellular heterogeneity and poor prognosis. Fewer data have been reported in GBC than other common cancer types. Multi-omics data will contribute to the understanding of the molecular mechanisms of cancer, cancer diagnosis and prognosis. Herein, to provide better understanding of the molecular events in GBC pathogenesis, we developed GBCdb (
http://tmliang.cn/gbc/
), a user-friendly interface for the query and browsing of GBC-associated genes and RNA interaction networks using published multi-omics data, which also included experimentally supported data from different molecular levels. GBCdb will help to elucidate the potential biological roles of different RNAs and allow for the exploration of RNA interactions in GBC. These resources will provide an opportunity for unraveling the potential molecular features of Gallbladder carcinoma."
475,Using transfer learning and dimensionality reduction techniques to improve generalisability of machine-learning predictions of mosquito ages from mid-infrared spectra,"Background
Old mosquitoes are more likely to transmit malaria than young ones. Therefore, accurate prediction of mosquito population age can drastically improve the evaluation of mosquito-targeted interventions. However, standard methods for age-grading mosquitoes are laborious and costly. We have shown that Mid-infrared spectroscopy (MIRS) can be used to detect age-specific patterns in mosquito cuticles and thus can be used to train age-grading machine learning models. However, these models tend to transfer poorly across populations. Here, we investigate whether applying dimensionality reduction and transfer learning to MIRS data can improve the transferability of MIRS-based predictions for mosquito ages.
Methods
We reared adults of the malaria vector 
Anopheles arabiensis
 in two insectaries. The heads and thoraces of female mosquitoes were scanned using an attenuated total reflection-Fourier transform infrared spectrometer, which were grouped into two different age classes. The dimensionality of the spectra data was reduced using unsupervised principal component analysis or t-distributed stochastic neighbour embedding, and then used to train deep learning and standard machine learning classifiers. Transfer learning was also evaluated to improve transferability of the models when predicting mosquito age classes from new populations.
Results
Model accuracies for predicting the age of mosquitoes from the same population as the training samples reached 99% for deep learning and 92% for standard machine learning. However, these models did not generalise to a different population, achieving only 46% and 48% accuracy for deep learning and standard machine learning, respectively. Dimensionality reduction did not improve model generalizability but reduced computational time. Transfer learning by updating pre-trained models with 2% of mosquitoes from the alternate population improved performance to ~ 98% accuracy for predicting mosquito age classes in the alternative population.
Conclusion
Combining dimensionality reduction and transfer learning can reduce computational costs and improve the transferability of both deep learning and standard machine learning models for predicting the age of mosquitoes. Future studies should investigate the optimal quantities and diversity of training data necessary for transfer learning and the implications for broader generalisability to unseen datasets."
476,Negation detection in Dutch clinical texts: an evaluation of rule-based and machine learning methods,"When developing models for clinical information retrieval and decision support systems, the discrete outcomes required for training are often missing. These labels need to be extracted from free text in electronic health records. For this extraction process one of the most important contextual properties in clinical text is negation, which indicates the absence of findings. We aimed to improve large scale extraction of labels by comparing three methods for negation detection in Dutch clinical notes. We used the Erasmus Medical Center Dutch Clinical Corpus to compare a rule-based method based on ContextD, a biLSTM model using MedCAT and (finetuned) RoBERTa-based models. We found that both the biLSTM and RoBERTa models consistently outperform the rule-based model in terms of F1 score, precision and recall. In addition, we systematically categorized the classification errors for each model, which can be used to further improve model performance in particular applications. Combining the three models naively was not beneficial in terms of performance. We conclude that the biLSTM and RoBERTa-based models in particular are highly accurate accurate in detecting clinical negations, but that ultimately all three approaches can be viable depending on the use case at hand."
477,Ensemble feature selection with data-driven thresholding for Alzheimer's disease biomarker discovery,"Background
Feature selection is often used to identify the important features in a dataset but can produce unstable results when applied to high-dimensional data. The stability of feature selection can be improved with the use of feature selection ensembles, which aggregate the results of multiple base feature selectors. However, a threshold must be applied to the final aggregated feature set to separate the relevant features from the redundant ones. A fixed threshold, which is typically used, offers no guarantee that the final set of selected features contains only relevant features. This work examines a selection of data-driven thresholds to automatically identify the relevant features in an ensemble feature selector and evaluates their predictive accuracy and stability. Ensemble feature selection with data-driven thresholding is applied to two real-world studies of Alzheimer's disease. Alzheimer's disease is a progressive neurodegenerative disease with no known cure, that begins at least 2–3 decades before overt symptoms appear, presenting an opportunity for researchers to identify early biomarkers that might identify patients at risk of developing Alzheimer's disease.
Results
The ensemble feature selectors, combined with data-driven thresholds, produced more stable results, on the whole, than the equivalent individual feature selectors, showing an improvement in stability of up to 34%. The most successful data-driven thresholds were the robust rank aggregation threshold and the threshold algorithm threshold from the field of information retrieval. The features identified by applying these methods to datasets from Alzheimer's disease studies reflect current findings in the AD literature.
Conclusions
Data-driven thresholds applied to ensemble feature selectors provide more stable, and therefore more reproducible, selections of features than individual feature selectors, without loss of performance. The use of a data-driven threshold eliminates the need to choose a fixed threshold a-priori and can select a more meaningful set of features. A reliable and compact set of features can produce more interpretable models by identifying the factors that are important in understanding a disease."
478,A pseudo-value regression approach for differential network analysis of co-expression data,"Background


The differential network (DN) analysis identifies changes in measures of association among genes under two or more experimental conditions. In this article, we introduce a pseudo-value regression approach for network analysis (PRANA). This is a novel method of differential network analysis that also adjusts for additional clinical covariates. We start from mutual information criteria, followed by pseudo-value calculations, which are then entered into a robust regression model.


Results


This article assesses the model performances of PRANA in a multivariable setting, followed by a comparison to 
dnapath
 and DINGO in both univariable and multivariable settings through variety of simulations. Performance in terms of precision, recall, and F1 score of differentially connected (DC) genes is assessed. By and large, PRANA outperformed 
dnapath
 and DINGO, neither of which is equipped to adjust for available covariates such as patient-age. Lastly, we employ PRANA in a real data application from the Gene Expression Omnibus database to identify DC genes that are associated with chronic obstructive pulmonary disease to demonstrate its utility.


Conclusion


To the best of our knowledge, this is the first attempt of utilizing a regression modeling for DN analysis by collective gene expression levels between two or more groups with the inclusion of additional clinical covariates. By and large, adjusting for available covariates improves accuracy of a DN analysis."
479,Machine learning to analyse omic-data for COVID-19 diagnosis and prognosis,"Background


With the global spread of COVID-19, the world has seen many patients, including many severe cases. The rapid development of machine learning (ML) has made significant disease diagnosis and prediction achievements. Current studies have confirmed that omics data at the host level can reflect the development process and prognosis of the disease. Since early diagnosis and effective treatment of severe COVID-19 patients remains challenging, this research aims to use omics data in different ML models for COVID-19 diagnosis and prognosis. We used several ML models on omics data of a large number of individuals to first predict whether patients are COVID-19 positive or negative, followed by the severity of the disease.


Results


On the COVID-19 diagnosis task, we got the best AUC of 0.99 with our multilayer perceptron model and the highest F1-score of 0.95 with our logistic regression (LR) model. For the severity prediction task, we achieved the highest accuracy of 0.76 with an LR model. Beyond classification and predictive modeling, our study founds ML models performed better on integrated multi-omics data, rather than single omics. By comparing top features from different omics dataset, we also found the robustness of our model, with a wider range of applicability in diverse dataset related to COVID-19. Additionally, we have found that omics-based models performed better than image or physiological feature-based models, proving the importance of the omics-based dataset for future model development.


Conclusions


This study diagnoses COVID-19 positive cases and predicts accurate severity levels. It lowers the dependence on clinical data and professional judgment, by leveraging the utilization of state-of-the-art models. our model showed wider applicability across different omics dataset, which is highly transferable in other respiratory or similar diseases. Hospital and public health care mechanisms can optimize the distribution of medical resources and improve the robustness of the medical system."
480,ASV portal: an interface to DNA-based biodiversity data in the Living Atlas,"Background
The Living Atlas is an open source platform used to collect, visualise and analyse biodiversity data from multiple sources, and serves as the national biodiversity data hub in many countries. Although powerful, the Living Atlas has had limited functionality for species occurrence data derived from DNA sequences. As a step toward integrating this fast-growing data source into the platform, we developed the Amplicon Sequence Variant (ASV) portal: a web interface to sequence-based biodiversity observations in the Living Atlas.
Results
The ASV portal allows data providers to submit denoised metabarcoding output to the Living Atlas platform via an intermediary ASV database. It also enables users to search for existing ASVs and associated Living Atlas records using the Basic Local Alignment Search Tool, or via filters on taxonomy and sequencing details. The ASV portal is a Python-Flask/jQuery web interface, implemented as a multi-container docker service, and is an integral part of the Swedish Biodiversity Data Infrastructure.
Conclusion
The ASV portal is a web interface that effectively integrates biodiversity data derived from DNA sequences into the Living Atlas platform."
481,Single-cell multi-omics integration for unpaired data by a siamese network with graph-based contrastive loss,"Background
Single-cell omics technology is rapidly developing to measure the epigenome, genome, and transcriptome across a range of cell types. However, it is still challenging to integrate omics data from different modalities. Here, we propose a variation of the Siamese neural network framework called MinNet, which is trained to integrate multi-omics data on the single-cell resolution by using graph-based contrastive loss.
Results
By training the model and testing it on several benchmark datasets, we showed its accuracy and generalizability in integrating scRNA-seq with scATAC-seq, and scRNA-seq with epitope data. Further evaluation demonstrated our model's unique ability to remove the batch effect, a common problem in actual practice. To show how the integration impacts downstream analysis, we established model-based smoothing and cis-regulatory element-inferring method and validated it with external pcHi-C evidence. Finally, we applied the framework to a COVID-19 dataset to bolster the original work with integration-based analysis, showing its necessity in single-cell multi-omics research.
Conclusions
MinNet is a novel deep-learning framework for single-cell multi-omics sequencing data integration. It ranked top among other methods in benchmarking and is especially suitable for integrating datasets with batch and biological variances. With the single-cell resolution integration results, analysis of the interplay between genome and transcriptome can be done to help researchers understand their data and question."
482,Comprehensive analysis of cuproptosis-related lncRNAs in immune infiltration and prognosis in hepatocellular carcinoma,"Background
Being among the most common malignancies worldwide, hepatocellular carcinoma (HCC) accounting for the third cause of cancer mortality. The regulation of cell death is the most crucial step in tumor progression and has become a crucial target for nearly all therapeutic options. Cuproptosis, a copper-induced cell death, was recently reported in Science. However, its primary function in carcinogenesis is still unclear.
Methods
Cuproptosis-related lncRNAs significantly associated with overall survival (OS) were screened by stepwise univariate Cox regression. The signature of cuproptosis-related lncRNAs for HCC prognosis was constructed by the LASSO algorithm and multivariate Cox regression. Further Kaplan–Meier analysis, proportional hazards model, and ROC analysis were performed. Functional annotation was performed using gene set enrichment analysis (GSEA). The relationship between prognostic cuproptosis-related lncRNAs and HCC prognosis was further explored by GEPIA(
http://gepia.cancer-pku.cn/
) online analysis tool. Finally, we used the ESTIMATE and XCELL algorithms to estimate stromal and immune cells in tumor tissue and cast each sample to infer the underlying mechanism of cuproptosis-related lncRNAs in the tumor immune microenvironment (TIME) of HCC patients.
Results
Four cuproptosis-related lncRNAs were used to construct a prognostic lncRNA signature, which was an independent factor in predicting OS in HCC patients. Kaplan–Meier curves showed significant differences in survival rates between risk subgroups (
p
 = 0.002). At the same time, we found that the expression levels of most immune checkpoint genes increased with increasing risk scores. Tumorigenesis and immunological-related pathways were primarily enhanced in the high-risk group, as determined by GSEA. The results of drug sensitivity analysis showed that compared with patients in the high-risk group, the IC50 values of erlotinib and lapatinib were lower in patients in the low-risk group, while the opposite was true for sunitinib, paclitaxel, gemcitabine, and imatinib. We also found that elevated 
AL133243.2
 expression was significantly associated with worse OS and disease-free survival (DFS), more advanced T stage and higher tumor grade, and reduced immune cell infiltration, suggesting that HCC patients with low 
AL133243.2
 expression in tumor tissues may have a better response to immunotherapy.
Conclusion
Collectively, the cuproptosis-associated lncRNA signature can serve as an independent predictor to guide individual treatment strategies. Furthermore, 
AL133243.2
 is a promising marker for predicting immunotherapy response in HCC patients. This data may facilitate further exploration of more effective immunotherapy strategies for HCC."
483,A hybrid algorithm for clinical decision support in precision medicine based on machine learning,"Purpose
The objective of the manuscript is to propose a hybrid algorithm combining the improved BM25 algorithm, k-means clustering, and BioBert model to better determine biomedical articles utilizing the PubMed database so, the number of retrieved biomedical articles whose content contains much similar information regarding a query of a specific disease could grow larger.

Design/methodology/approach
In the paper, a two-stage information retrieval method is proposed to conduct an improved Text-Rank algorithm. The first stage consists of employing the improved BM25 algorithm to assign scores to biomedical articles in the database and identify the 1000 publications with the highest scores. The second stage is composed of employing a method called a cluster-based abstract extraction to reduce the number of article abstracts to match the input constraints of the BioBert model, and then the BioBert-based document similarity matching method is utilized to obtain the most similar search outcomes between the document and the retrieved morphemes. To realize reproducibility, the written code is made available on 
https://github.com/zzc1991/TREC_Precision_Medicine_Track
.
Findings
The experimental study is conducted based on the data sets of TREC2017 and TREC2018 to train the proposed model and the data of TREC2019 is used as a validation set confirming the effectiveness and practicability of the proposed algorithm that would be implemented for clinical decision support in precision medicine with a generalizability feature.
Originality/value
This research integrates multiple machine learning and text processing methods to devise a hybrid method applicable to domains of specific medical literature retrieval. The proposed algorithm provides a 3% increase of P@10 than that of the state-of-the-art algorithm in TREC 2019."
484,A gene based combination test using GWAS summary data,"Background
Gene-based association tests provide a useful alternative and complement to the usual single marker association tests, especially in genome-wide association studies (GWAS). The way of weighting for variants in a gene plays an important role in boosting the power of a gene-based association test. Appropriate weights can boost statistical power, especially when detecting genetic variants with weak effects on a trait. One major limitation of existing gene-based association tests lies in using weights that are predetermined biologically or empirically. This limitation often attenuates the power of a test. On another hand, effect sizes or directions of causal genetic variants in real data are usually unknown, driving a need for a flexible yet robust methodology of gene based association tests. Furthermore, access to individual-level data is often limited, while thousands of GWAS summary data are publicly and freely available.
Results
To resolve these limitations, we propose a combination test named as OWC which is based on summary statistics from GWAS data. Several traditional methods including burden test, weighted sum of squared score test [SSU], weighted sum statistic [WSS], SNP-set Kernel Association Test [SKAT], and the score test are special cases of OWC. To evaluate the performance of OWC, we perform extensive simulation studies. Results of simulation studies demonstrate that OWC outperforms several existing popular methods. We further show that OWC outperforms comparison methods in real-world data analyses using schizophrenia GWAS summary data and a fasting glucose GWAS meta-analysis data. The proposed method is implemented in an R package available at 
https://github.com/Xuexia-Wang/OWC-R-package
Conclusions
We propose a novel gene-based association test that incorporates four different weighting schemes (two constant weights and two weights proportional to normal statistic 
Z
) and includes several popular methods as its special cases. Results of the simulation studies and real data analyses illustrate that the proposed test, OWC, outperforms comparable methods in most scenarios. These results demonstrate that OWC is a useful tool that adapts to the underlying biological model for a disease by weighting appropriately genetic variants and combination of well-known gene-based tests."
485,Evaluation of automatic discrimination between benign and malignant prostate tissue in the era of high precision digital pathology,"Background
Prostate cancer is a major health concern in aging men. Paralleling an aging society, prostate cancer prevalence increases emphasizing the need for efficient diagnostic algorithms.
Methods
Retrospectively, 106 prostate tissue samples from 48 patients (mean age, 
\(66\pm 6.6\)
 years) were included in the study. Patients suffered from prostate cancer (n = 38) or benign prostatic hyperplasia (n = 10) and were treated with radical prostatectomy or Holmium laser enucleation of the prostate, respectively. We constructed tissue microarrays (TMAs) comprising representative malignant (n = 38) and benign (n = 68) tissue cores. TMAs were processed to histological slides, stained, digitized and assessed for the applicability of machine learning strategies and open–source tools in diagnosis of prostate cancer. We applied the software QuPath to extract features for shape, stain intensity, and texture of TMA cores for three stainings, H&E, ERG, and PIN-4. Three machine learning algorithms, neural network (NN), support vector machines (SVM), and random forest (RF), were trained and cross-validated with 100 Monte Carlo random splits into 70% training set and 30% test set. We determined AUC values for single color channels, with and without optimization of hyperparameters by exhaustive grid search. We applied recursive feature elimination to feature sets of multiple color transforms.
Results
Mean AUC was above 0.80. PIN-4 stainings yielded higher AUC than H&E and ERG. For PIN-4 with the color transform saturation, NN, RF, and SVM revealed AUC of 
\(0.93\pm 0.04\)
, 
\(0.91\pm 0.06\)
, and 
\(0.92\pm 0.05\)
, respectively. Optimization of hyperparameters improved the AUC only slightly by 0.01. For H&E, feature selection resulted in no increase of AUC but to an increase of 0.02–0.06 for ERG and PIN-4.
Conclusions
Automated pipelines may be able to discriminate with high accuracy between malignant and benign tissue. We found PIN-4 staining best suited for classification. Further bioinformatic analysis of larger data sets would be crucial to evaluate the reliability of automated classification methods for clinical practice and to evaluate potential discrimination of aggressiveness of cancer to pave the way to automatic precision medicine."
